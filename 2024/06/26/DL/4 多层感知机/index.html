<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>深度学习—— 4 多层感知机 | 可爱可倾</title><meta name="author" content="可爱可倾"><meta name="copyright" content="可爱可倾"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="深度学习—— 4 多层感知机"><meta name="application-name" content="深度学习—— 4 多层感知机"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="深度学习—— 4 多层感知机"><meta property="og:url" content="https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html"><meta property="og:site_name" content="可爱可倾"><meta property="og:description" content="4 多层感知机 最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。 4.1 多层感知机 多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。 4.1.1 隐藏层"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://blog.keaikeqing.cn/papercover/DL/DL.webp"><meta property="article:author" content="可爱可倾"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.keaikeqing.cn/papercover/DL/DL.webp"><meta name="description" content="4 多层感知机 最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。 4.1 多层感知机 多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。 4.1.1 隐藏层"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="var(--anzhiyu-main)"/><link rel="mask-icon" href="/img/siteicon/apple-icon-180.png" color="#5bbad5"/><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/apple-icon-180.png"/><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/siteicon/apple-icon-180.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/16.png"/><link rel="bookmark" href="/img/siteicon/apple-icon-180.png"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2048-2732.jpg" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2732-2048.jpg" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1668-2388.jpg" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2388-1668.jpg" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1536-2048.jpg" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2048-1536.jpg" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1668-2224.jpg" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2224-1668.jpg" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1620-2160.jpg" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2160-1620.jpg" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1290-2796.jpg" media="(device-width: 430px) and (device-height: 932px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2796-1290.jpg" media="(device-width: 430px) and (device-height: 932px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1179-2556.jpg" media="(device-width: 393px) and (device-height: 852px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2556-1179.jpg" media="(device-width: 393px) and (device-height: 852px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1284-2778.jpg" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2778-1284.jpg" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1170-2532.jpg" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2532-1170.jpg" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1125-2436.jpg" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2436-1125.jpg" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1242-2688.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2688-1242.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-828-1792.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1792-828.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1242-2208.jpg" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2208-1242.jpg" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-750-1334.jpg" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1334-750.jpg" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-640-1136.jpg" media="(device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"/><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1136-640.jpg" media="(device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: undefined,
  postHeadAiDescription: undefined,
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 期待下次再见!","backTitle":"♪(^∇^*) 欢迎肥来！"},
  LA51: {"enable":true,"ck":"3KAOg7IT5KO5nuXL","LingQueMonitorID":"3KIpEgdIKLBlVguQ"},
  greetingBox: {"enable":true,"default":"每一天安好","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 宝贝","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":22,"endTime":24}]},
  twikooEnvId: 'https://twikoo.keaikeqing.cn',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"both","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: {"skills":["🤖️ 数码科技爱好者","📚 热爱学习与探索","🎮 游戏娱乐和创作","🔨 设计开发一条龙","🎨 创意与艺术设计","🌟 追随动漫二次元"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":180,"position":"top","messagePrev":"时光悄然流逝，距离上次更新已有","messageNext":"天，内容可能已经过时，请谨慎参考。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":true,"limitCount":50,"languages":{"author":"作者: 可爱可倾","link":"链接: ","source":"来源: 可爱可倾","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '可爱可倾',
  title: '深度学习—— 4 多层感知机',
  postAI: '',
  pageFillDescription: '4 多层感知机, 4.1 多层感知机, 4.1.1 隐藏层, 4.1.1.1 线性模型的局限性, 4.1.1.2 在网络中加入隐藏层, 4.1.1.3 从线性到非线性, 4.1.1.4 通用近似定理, 4.1.2 激活函数, 4.1.2.1 ReLU函数(修正线性单元), 4.1.2.2 sigmoid函数(挤压函数), 4.1.2.3 tanh函数(双曲正切函数), 4.2 多层感知机的从零开始实现, 4.3 多层感知机的简洁实现, 4.4 模型选择、欠拟合和过拟合, 4.4.1 训练误差和泛化误差, 4.4.2 模型选择, 4.4.3 欠拟合和过拟合, 4.4.3.1 模型复杂性, 4.4.3.2 数据集大小, 4.4.4 多项式回归, 4.4.4.1 使用线性函数拟合(欠拟合), 4.5 权重衰减, 4.5.1 高维线性回归, 4.5.2 从零开始实现, 4.5.2.1 不使用正则化, 4.5.2.2 使用权重衰减, 4.5.3 简洁实现, 4.5.3.1 不使用正则化, 4.5.3.2 使用权重衰减, 4.6 暂退法, 4.6.1 实践中的暂退法, 4.6.2 从零开始实现, 4.6.3 简洁实现, 4.7 前向传播、反向传播和计算图, 4.7.1 前向传播, 4.7.2 前向传播计算图, 4.7.3 反向传播, 4.7.4 内存需求, 4.8 数值稳定性和模型初始化, 4.8.2 初始化参数, 4.9 环境与分布偏移, 4.9.1 分布偏移的类型, 4.9.2 分布偏移纠正, 4.10 实战Kaggle比赛：房价预测, 附录, 附录A：激活函数绘图代码, 附录B：神经网络反向传播通俗解释, 附录C：为什么关注激活函数的导数？多层感知机最简单的深度网络称为多层感知机多层感知机由多层神经元组成每一层与它的上一层相连从中接收输入同时每一层也与它的下一层相连影响当前层的神经元多层感知机多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层并通过激活函数转换隐藏层的输出隐藏层线性模型的局限性线性模型的一个主要局限性是它们只能表示输入和输出之间的简单关系如果我们想要模拟更复杂的非线性关系就需要更复杂的模型在网络中加入隐藏层我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制使其能处理更普遍的函数关系类型要做到这一点最简单的方法是将许多全连接层堆叠在一起我们可以把前层看作表示把最后一层看作线性预测器这种架构通常称为多层感知机通常缩写为多层感知机这个多层感知机有个输入个输出其隐藏层包含个隐藏单元输入层不涉及任何计算因此使用此网络产生输出只需要实现隐藏层和输出层的计算因此这个多层感知机中的层数为注意这两个层都是全连接的每个输入都会影响隐藏层中的每个神经元而隐藏层中的每个神经元又会影响输出层中的每个神经元然而具有全连接层的多层感知机的参数开销可能会高得令人望而却步即使在不改变输入或输出大小的情况下需要在参数节约和模型有效性之间进行权衡从线性到非线性即使在添加隐藏层后按照之前的逻辑我们的输出仍然是输入的线性组合也就是说即使我们有无限多的隐藏层多层感知机仍然等同于线性模型原因如下其中表示输入表示隐藏层的输出表示输出层的输出和是权重参数和是偏置参数上面的隐藏单元由输入的仿射函数给出而输出操作前只是隐藏单元的仿射函数仿射函数的仿射函数本身就是仿射函数所以这和线性模型没有区别引入关键因素在仿射变换之后对每个隐藏单元应用非线性的激活函数通用近似定理多层感知机可以通过隐藏神经元捕捉到输入之间复杂的相互作用这些神经元依赖于每个输入的值激活函数激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活它们将输入信号转换为输出的可微运算常用的激活函数包括函数函数和函数其函数和导数如下图所示激活函数绘图代码见附录函数修正线性单元函数仅保留正元素并丢弃所有负元素给定元素函数被定义为该元素与的最大值使用的原因是它求导表现得特别好要么让参数消失要么让参数通过这使得优化表现得更好并且减轻了困扰以往神经网络的梯度消失问题注意函数有许多变体包括参数化函数该变体为添加了一个线性项因此即使参数是负的某些信息仍然可以通过函数挤压函数函数将范围中的任意输入压缩到区间中的某个值当我们想要将输出视作二元分类问题的概率时仍然被广泛用作输出单元上的激活函数可以视为的特例然而在隐藏层中已经较少使用它在大部分时候被更简单更容易训练的所取代函数双曲正切函数函数也能将其输入压缩转换到区间上多层感知机的从零开始实现激活函数模型因为忽略了空间结构所以需要使用将每个二维图像转换为一个长度为的向量这里代表矩阵乘法多层感知机的从零开始实现读取数据初始化模型参数激活函数模型损失函数表示直接返回每个样本的损失其他有训练预测多层感知机的简洁实现多层感知机的简洁实现定义模型将输入的多维张量转换为一维张量全连接层激活函数全连接层初始化模型参数读取数据并训练模型预测模型选择欠拟合和过拟合如何发现可以泛化的模式是机器学习的根本问题训练误差和泛化误差训练误差是指模型在训练数据集上计算得到的误差泛化误差是指模型应用在同样从原始样本的分布中抽取的无限多数据样本时模型误差的期望实际中只能通过测试数据集来近似估计泛化误差几个倾向于影响模型泛化的因素可调整参数的数量当可调整参数的数量有时称为自由度很大时模型往往更容易过拟合参数采用的值当权重的取值范围较大时模型可能更容易过拟合训练样本的数量即使模型很简单也很容易过拟合只包含一两个样本的数据集而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型模型选择训练集用于训练模型的数据集验证集用于调整模型超参数的数据集测试集用于评估模型泛化误差的数据集数据集用途参与训练用来调参用来评估最终性能训练集拿来训练模型是否否验证集拿来选模型调参数否是否测试集拿来最终评估否否是但现实是验证数据和测试数据之间的边界模糊得令人担忧当训练数据稀缺时我们甚至可能无法提供足够的数据来构成一个合适的验证集这个问题的一个流行的解决方案是采用折交叉验证这里原始训练数据被分成个不重叠的子集然后执行次模型训练和验证每次在个子集上进行训练并在剩余的一个子集在该轮中没有用于训练的子集上进行验证最后通过对次实验的结果取平均来估计训练和验证误差欠拟合和过拟合欠拟合训练误差和验证误差都很严重但它们之间仅有一点差距如果模型不能降低训练误差这可能意味着模型过于简单即表达能力不足无法捕获试图学习的模式此外由于我们的训练和验证误差之间的泛化误差很小我们有理由相信可以用一个更复杂的模型降低训练误差过拟合当我们的训练误差明显低于验证误差时要小心这表明严重的过拟合注意过拟合并不总是一件坏事特别是在深度学习领域众所周知最好的预测模型在训练数据上的表现往往比在保留验证数据上好得多最终我们通常更关心验证误差而不是训练误差和验证误差之间的差距是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小模型复杂性比如一个线性回归模型的拟合特征是的幂给出的模型的权重是给出的偏置是给出高阶多项式函数比低阶多项式函数复杂得多高阶多项式的参数较多模型函数的选择范围较广因此在固定训练数据集的情况下高阶多项式函数相对于低阶多项式的训练误差应该始终更低最坏也是相等事实上当数据样本包含了的不同值时函数阶数等于数据样本数量的多项式函数可以完美拟合训练集模型的选择和拟合情况如下图所示模型的选择和拟合情况数据集大小训练数据集中的样本越少我们就越有可能且更严重地过拟合随着训练数据量的增加泛化误差通常会减小多项式回归通过多项式拟合来探索这些概念评估给定数据集上模型的损失损失的总和样本数量不设置偏置因为我们已经在多项式中实现了它生成数据集多项式的最大阶数训练和测试数据集大小分配大量的空间噪声项打乱顺序的维度转换为定义训练和测试模型在外部函数中定义三阶多项式函数拟合正常从多项式特征中选择前个维度即线性函数拟合欠拟合从多项式特征中选择前个维度即和减少该模型的训练损失相对困难在最后一个迭代周期完成后训练损失仍然很高高阶多项式函数拟合过拟合从多项式特征中选取所有维度即训练误差迅速降低但测试损失仍然很高数据集的生成在优化的过程中我们通常希望避免非常大的梯度值或损失值这就是我们将特征从调整为的原因这样可以避免很大的带来的特别大的指数值使用线性函数拟合欠拟合减少该模型的训练损失相对困难在最后一个迭代周期完成后训练损失仍然很高使用线性函数拟合使用三阶多项式函数拟合合适该模型能有效降低训练损失和测试损失学习到的模型参数也接近真实值使用三阶多项式函数拟合使用高阶多项式函数拟合过拟合在这种情况下没有足够的数据用于学到高阶系数应该具有接近于零的值因此这个过于复杂的模型会轻易受到训练数据中噪声的影响虽然训练损失可以有效地降低但测试损失仍然很高使用高阶多项式函数拟合权重衰减虽然可以通过去收集更多的训练数据来缓解过拟合但是并不太可能假设我们已经拥有尽可能多的高质量数据我们便可以将重点放在正则化技术上正则化是处理过拟合的常用方法在训练集的损失函数中加入惩罚项以降低学习到的模型的复杂度权重衰减是最广泛使用的正则化的技术之一它通常也被称为正则化原来的损失函数现在的损失函数通过正则化常数来描述这种权衡这是一个非负超参数高维线性回归使用的数据集由下面公式生成为了使过拟合的效果更加明显我们可以将问题的维数增加到并使用一个只包含个样本的小训练集从零开始实现随机初始化模型参数范数惩罚定义训练增加了范数惩罚项广播机制使成为一个长度为的向量的范数是无正则化使用权重衰减不使用正则化这里训练误差有了减少但测试误差没有减少这意味着出现了严重的过拟合不使用正则化使用权重衰减在这里训练误差增大但测试误差减小这正是我们期望从正则化中得到的效果使用权重衰减简洁实现深度学习框架为了便于我们使用权重衰减将权重衰减集成到优化算法中以便与任何损失函数结合使用默认情况下同时衰减权重和偏移这里我们只为权重设置了所以偏置参数不会衰减的范数无正则化使用权重衰减不使用正则化不使用正则化使用权重衰减使用权重衰减暂退法暂退法在前向传播过程中计算每一内部层的同时注入噪声这已经成为训练神经网络的常用技术这种方法之所以被称为暂退法因为我们从表面上看是在训练过程中丢弃一些神经元在整个训练过程的每一次迭代中标准暂退法包括在计算下一层之前将当前层中的一些节点置零那么关键的挑战就是如何注入这种噪声一种想法是以一种无偏向的方式注入噪声这样在固定住其他层时每一层的期望值等于没有噪音时的值在标准暂退法正则化中通过按保留未丢弃的节点的分数进行规范化来消除每一层的偏差概率为其他情况期望值保持不变即实践中的暂退法当我们将暂退法应用到隐藏层以的概率将隐藏单元置为零时结果可以看作一个只包含原始神经元子集的网络如图删除了和因此输出的计算不再依赖于或并且它们各自的梯度在执行反向传播时也会消失这样输出层的计算不能过度依赖于的任何一个元素前后的多层感知机通常我们在测试时不用暂退法给定一个训练好的模型和一个新的样本我们不会丢弃任何节点因此不需要标准化然而也有一些例外一些研究人员在测试时使用暂退法用于估计神经网络预测的不确定性如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的那么我们可以说网络发挥更稳定从零开始实现定义一个函数在本情况中所有元素都被丢弃在本情况中所有元素都被保留生成一个按位测试函数只有在训练模型时才使用在第一个全连接层之后添加一个层在第二个全连接层之后添加一个层定义模型我们可以将暂退法应用于每个隐藏层的输出在激活函数之后并且可以为每一层分别设置暂退概率常见的技巧是在靠近输入层的地方设置较低的暂退概率下面的模型将第一个和第二个隐藏层的暂退概率分别设置为和并且暂退法只在训练期间有效训练和测试模型简洁实现在第一个全连接层之后添加一个层在第二个全连接层之后添加一个层前向传播反向传播和计算图前向传播按顺序从输入层到输出层计算和存储神经网络中每层的结果损失函数样本标签则损失项正则项正则化损失目标函数即为前向传播计算图正方形表示变量圆圈表示操作符左下角表示输入右上角表示输出前向传播计算图反向传播反向传播指的是计算神经网络参数梯度的方法简言之该方法根据微积分中的链式规则按相反的顺序从输出层到输入层遍历网络该算法存储了计算某些参数梯度时所需的任何中间变量偏导数在此例子中即通过链式法则计算和内存需求因此在训练神经网络时在初始化模型参数后我们交替使用前向传播和反向传播利用反向传播给出的梯度来更新模型参数注意反向传播重复利用前向传播中存储的中间值以避免重复计算带来的影响之一是我们需要保留中间值直到反向传播完成这也是训练比单纯的预测需要更多的内存显存的原因之一此外这些中间值的大小与网络层的数量和批量的大小大致成正比因此使用更大的批量来训练更深层次的网络更容易导致内存不足错误数值稳定性和模型初始化初始化方案的选择在神经网络学习中起着举足轻重的作用它对保持数值稳定性至关重要糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失需要用启发式的初始化方法来确保初始梯度既不太大也不太小激活函数缓解了梯度消失问题这样可以加速收敛随机初始化是保证在进行优化前打破对称性的关键梯度爆炸梯度消失打破对称性梯度消失参数更新过小在每次更新时几乎不会移动导致模型无法学习根据链式求导法则梯度的计算是由不同因子的连乘结果只要其中某个因子的数值小于那么随着网络的加深后续的梯度一定是逐渐降低的假设其他因子设置合理如果因子的数值够低后续梯度甚至会出现消失现象导致网络难以训练和收敛这就是梯度消失的现象例如当函数的输入很大或是很小时它的梯度都会消失梯度爆炸参数更新过大破坏了模型的稳定收敛同梯度消失的原理一样梯度爆炸也是因为因子的数值大于在经过网络的不断加深后续梯度出现爆炸的现象打破对称性神经网络设计中的另一个问题是其参数化所固有的对称性如果我们将隐藏层的所有参数初始化为同一个常量会发生什么在这种情况下在前向传播期间隐藏单元采用相同的输入和参数产生相同的激活该激活被送到输出单元在反向传播期间根据参数对输出单元进行微分得到一个梯度其元素都取相同的值因此在基于梯度的迭代之后隐藏单元的所有元素仍然采用相同的值这样的迭代永远不会打破对称性我们可能永远也无法实现网络的表达能力隐藏层的行为就好像只有一个单元请注意虽然小批量随机梯度下降不会打破这种对称性但暂退法正则化可以初始化参数的默认初始化初始化从均值为零方差的高斯分布中采样权重环境与分布偏移分布偏移是指模型在训练和测试数据集之间的数据分布不匹配的情况这种不匹配可能导致模型在测试集上的表现下降因为模型在训练时学习到的特征在测试时可能不再适用分布偏移的类型协变量特征偏移虽然输入的分布改变但标签没有改变例如猫狗识别训练数据主要来自家养而测试数据主要来自野外拍摄此时输入数据的分布发生了变化但标签猫或狗没有变化标签偏移虽然输入的分布保持不变但标签的分布改变假设你训练了一个模型来预测某城市的天气训练数据中晴天和雨天的比例是但测试数据中这个比例变成了输入的天气特征分布保持不变但标签的分布发生了变化概念偏移输入和输出之间的映射关系发生了变化即特征和标签之间的关系变了例如有一个垃圾邮件分类器训练数据中垃圾邮件的特征是某些关键词如免费优惠等但随着时间推移垃圾邮件发送者改变了策略使用了新的关键词如促销折扣等分布偏移纠正可以采用加权经验风险最小化等方法注真实风险是从真实分布中抽取的所有数据的总体损失的预期然而这个数据总体通常是无法获得的经验风险是训练数据的平均损失用于近似真实风险在实践中我们进行经验风险最小化实战比赛房价预测下载一个中的文件返回本地文件名不存在于初始化一个对象逐块读取文件内容每次读取可以在历史哈希值基础上用当前文件内容更新哈希值命中缓存正在从下载下载并解压文件只有文件可以被解压缩下载中的所有文件采用相对误差而不是绝对误差来衡量误差将预测值中小于的部分设置为以避免在取对数时出现负无穷大的情况这里使用的是优化算法用折交叉验证来评估模型折训练验证训练将网络应用于测试集将其重新格式化以导出到划分训练集以创建验证集删除列以及标签列数据预处理若无法获得测试数据则可根据训练数据计算均值和标准差标准化数据有两个原因方便优化不知道哪些特征是相关的避免让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大在标准化数据之后所有均值消失因此我们可以将缺失值设置为将离散数值转换为独热编码函数将分类变量转换为虚拟变量将缺失值视为有效的特征值并为其创建指示符特征通过属性得到格式的数据并转换成张量训练模型将训练集划分为份然后使用第份作为验证集其余作为训练集折验证平均训练平均验证使用所有数据对其进行训练然后预测并在提交结果附录附录激活函数绘图代码设置中文字体定义激活函数定义导数创建数据点创建图形和子图行列共个子图设置图表标题激活函数及其导数对比第一行激活函数激活函数激活函数激活函数激活函数激活函数激活函数激活函数激活函数第二行导数导数导数导数导数导数导数导数导数调整布局保存图像图像已保存至显示图像附录神经网络反向传播通俗解释神经网络使用反向传播算法进行训练核心是链式法则链式求导训练的目标是最小化损失函数我们要解决的就是怎么调整模型的参数才能让损失函数最小这就类似于的函数我们想要找到一个最优的使得最小这里的就是神经网络的参数而就是损失函数自然而然的引出了导数的概念也就是梯度它告诉我们在当前参数位置损失函数的变化率由此引出以下几个问题导数和梯度的关系是什么情况一一元函数只有一个输入它的导数是它的梯度也是在这种情况下梯度就等于导数本身只是写成了一个一维向量情况二多元函数多个输入没法用导数描述它的全部方向的变化要用梯度情况使用的概念是否等价一维函数导数梯度等价梯度是维向量多维函数梯度向量不等价导数只看一个方向梯度是全部方向为什么不直接让梯度为就可以了问题回答为什么不直接让梯度为因为求不出来无解析解而且梯度为不一定是最小值那我们怎么做使用梯度下降沿着负梯度方向一点点走逐步逼近最小值好处不需要显式解方程适合高维大规模非线性优化问题有了梯度之后怎么更新参数为什么这么更新有了梯度之后我们用梯度的反方向来更新参数因为梯度指出了损失函数增加最快的方向所以反方向是下降最快的方向参数更新的公式梯度下降法设模型参数可以是权重偏置等损失函数当前参数处的梯度学习率控制步子大小那么参数更新公式为为什么是这个公式梯度告诉我们什么梯度是一个向量表示在每一个维度上增加参数损失函数会上升多少所以梯度的方向就是损失函数上升最快的方向那我们想做什么我们想让损失函数变小而不是变大因此我们要朝损失下降最快的方向移动那就是梯度的负方向为什么还要乘一个学习率梯度只是一个方向我们还需要控制走多远步子太大可能错过最小值甚至震荡或发散步子太小学得太慢收敛极慢所以我们加一个系数就是学习率来控制更新的幅度更新量步骤说明计算梯度得到损失函数对参数的导数每个方向的变化率取反方向因为梯度是上升方向下降要走反方向乘以学习率控制更新的幅度避免步子太大或太小更新参数附录为什么关注激活函数的导数举例的导数它的导数为这意味着只有当输入为正时才会让梯度流动否则梯度为神经元不再学习了解梯度消失或爆炸的问题某些激活函数的导数可能导致梯度消失如或爆炸如的大输入分析导数可以帮助理解和选择更合适的激活函数避免这些问题例子的导数最大值是输入过大或过小时导数接近导致梯度消失设计新激活函数很多改进型激活函数如都是基于对导数行为的深入理解后提出的例如解决的神经元死亡问题即一旦输出为就无法恢复通过在负区间给一个小斜率导数非零',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-30 16:50:56',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/fonts.css" media="print" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/rss.xml" title="可爱可倾" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/img/favicon.ico"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">可爱可倾</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=8265222461&amp;server=tencent"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><span> 关于</span></a></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-weichat.webp" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-weichat.webp"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-alipay.webp" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-alipay.webp"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">7</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">18</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">11</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a></span><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">深度学习—— 4 多层感知机</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2024-06-26T04:12:56.000Z" title="发表于 2024-06-26 12:12:56">2024-06-26</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-05-30T08:50:56.000Z" title="更新于 2025-05-30 16:50:56">2025-05-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">10.9k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>43分钟</span></span><span class="post-meta-separator"></span><span id="" data-flag-title="深度学习—— 4 多层感知机"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="twikoo_visitors" title="访问量"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为天津"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>天津</span><span class="post-meta-separator"></span><span class="post-meta-commentcount"><i class="anzhiyufont anzhiyu-icon-comments post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/#post-comment" tabindex="-1"><span id="twikoo-count"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></a></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="/papercover/DL/DL.webp"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"><header><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a><h1 id="CrawlerTitle" itemprop="name headline">深度学习—— 4 多层感知机</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">可爱可倾</span><time itemprop="dateCreated datePublished" datetime="2024-06-26T04:12:56.000Z" title="发表于 2024-06-26 12:12:56">2024-06-26</time><time itemprop="dateCreated datePublished" datetime="2025-05-30T08:50:56.000Z" title="更新于 2025-05-30 16:50:56">2025-05-30</time></header><h2 id="多层感知机">4 多层感知机</h2>
<p>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，
每一层与它的上一层相连，从中接收输入；
同时每一层也与它的下一层相连，影响当前层的神经元。</p>
<h3 id="多层感知机-1">4.1 多层感知机</h3>
<p>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</p>
<h4 id="隐藏层">4.1.1 隐藏层</h4>
<h5 id="线性模型的局限性">4.1.1.1 线性模型的局限性</h5>
<p>线性模型的一个主要局限性是它们只能表示输入和输出之间的简单关系。
如果我们想要模拟更复杂的非线性关系，就需要更复杂的模型。</p>
<h5 id="在网络中加入隐藏层">4.1.1.2 在网络中加入隐藏层</h5>
<p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，
使其能处理更普遍的函数关系类型。
要做到这一点，最简单的方法是将许多全连接层堆叠在一起。我们可以把前<span
class="math inline">\(L-1\)</span>层看作表示，把最后一层看作线性预测器。
这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。</p>

      <div style="text-align: center;">
        <img src="./images/4.1_multilayer-perceptron.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.1_multilayer-perceptron.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.1：多层感知机</span>
      </div>
    
<p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。
输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。
因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。
每个输入都会影响隐藏层中的每个神经元，
而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p>
<p>然而，具有全连接层的多层感知机的参数开销可能会高得令人望而却步。
即使在不改变输入或输出大小的情况下，
需要在参数节约和模型有效性之间进行权衡。</p>
<h5 id="从线性到非线性">4.1.1.3 从线性到非线性</h5>
<p>即使在添加隐藏层后，按照之前的逻辑，我们的输出仍然是输入的线性组合。
也就是说，即使我们有无限多的隐藏层，多层感知机仍然等同于线性模型。</p>
<p>原因如下:</p>
<p><span class="math display">\[
\begin{gathered}
\mathbf{H} &amp; = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{gathered}
\tag{4.1}
\]</span></p>
<p>其中<span class="math inline">\(X\)</span>表示输入，<span
class="math inline">\(H\)</span>表示隐藏层的输出，<span
class="math inline">\(O\)</span>表示输出层的输出，<span
class="math inline">\(\mathbf{W}^{(1)}\)</span>和<span
class="math inline">\(\mathbf{W}^{(2)}\)</span>是权重参数，<span
class="math inline">\(\mathbf{b}^{(1)}\)</span>和<span
class="math inline">\(\mathbf{b}^{(2)}\)</span>是偏置参数。</p>
<p>上面的隐藏单元由输入<span
class="math inline">\(X\)</span>的仿射函数给出， 而输出<span
class="math inline">\(O\)</span>（softmax操作前）只是隐藏单元<span
class="math inline">\(H\)</span>的仿射函数。
仿射函数的仿射函数本身就是仿射函数， 所以这和线性模型没有区别。</p>
<p><strong>引入关键因素</strong>:
在仿射变换之后对每个隐藏单元应用非线性的激活函数<span
class="math inline">\(\sigma\)</span></p>
<p><span class="math display">\[
\begin{gathered}
\mathbf{H} &amp; = \sigma(\mathbf{X} \mathbf{W}^{(1)} +
\mathbf{b}^{(1)}), \\
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{gathered}
\tag{4.2}
\]</span></p>
<h5 id="通用近似定理">4.1.1.4 通用近似定理</h5>
<p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，
这些神经元依赖于每个输入的值。</p>
<h4 id="激活函数">4.1.2 激活函数</h4>
<p>激活函数通过<strong>计算加权和</strong>并加上<strong>偏置</strong>来确定神经元是否应该被激活，
它们将输入信号转换为输出的可微运算。常用的激活函数包括<strong>ReLU函数、sigmoid函数和tanh函数</strong>。</p>
<p>其函数和导数如下图所示：</p>

      <div style="text-align: center;">
        <img src="./images/4.2_activation-functions.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.2_activation-functions.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.2：激活函数</span>
      </div>
    
<p>绘图代码见<a href="#附录a激活函数绘图代码">附录A</a>。</p>
<h5 id="relu函数修正线性单元">4.1.2.1 ReLU函数(修正线性单元)</h5>
<p>ReLU函数仅保留正元素并丢弃所有负元素，给定元素<span
class="math inline">\(x\)</span>，ReLU函数被定义为该元素与<span
class="math inline">\(0\)</span>的最大值：</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\operatorname{ReLU}(x) = \max(x, 0).
\end{gathered}\end{equation}
\tag{4.3}
\]</span></p>
<p>使用ReLU的原因是，它求导表现得特别好：<strong>要么让参数消失，要么让参数通过</strong>。
这使得优化表现得更好，并且<strong>ReLU减轻了困扰以往神经网络的梯度消失问题</strong>。</p>
<p>**注意，ReLU函数有许多变体，包括参数化ReLU（pReLU） 函数。
该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：*</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).
\end{gathered}\end{equation}
\tag{4.4}
\]</span></p>
<h5 id="sigmoid函数挤压函数">4.1.2.2 sigmoid函数(挤压函数)</h5>
<p>sigmoid函数将范围 <span class="math inline">\((-inf, inf)\)</span>
中的任意输入压缩到区间 <span class="math inline">\((0, 1)\)</span>
中的某个值：</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.
\end{gathered}\end{equation}
\tag{4.5}
\]</span></p>
<p>当我们想要将输出视作<strong>二元分类</strong>问题的概率时，
sigmoid仍然被广泛用作输出单元上的激活函数
（<strong>sigmoid可以视为softmax的特例</strong>）。然而，sigmoid在<strong>隐藏层中已经较少使用</strong>，
它在大部分时候被更简单、更容易训练的<strong>ReLU所取代</strong>。</p>
<h5 id="tanh函数双曲正切函数">4.1.2.3 tanh函数(双曲正切函数)</h5>
<p>tanh函数也能将其输入压缩转换到区间<span class="math inline">\((-1,
1)\)</span>上:</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.
\end{gathered}\end{equation}
\tag{4.6}
\]</span></p>
<h3 id="多层感知机的从零开始实现">4.2 多层感知机的从零开始实现</h3>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

<span class="token comment"># 4.2.3 激活函数</span>


<span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    a <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> a<span class="token punctuation">)</span>

<span class="token comment"># 4.2.4 模型</span>


<span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 因为忽略了空间结构，所以需要使用reshape将每个二维图像转换为一个长度为num_inputs的向量</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    H <span class="token operator">=</span> relu<span class="token punctuation">(</span>X@W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span>  <span class="token comment"># 这里“@”代表矩阵乘法</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>H@W2 <span class="token operator">+</span> b2<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 4.2 多层感知机的从零开始实现</span>

    <span class="token comment"># 4.2.1 读取数据</span>
    batch_size <span class="token operator">=</span> <span class="token number">256</span>
    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>

    <span class="token comment"># 4.2.2 初始化模型参数</span>
    num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>

    W1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
        num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
    b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    W2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
        num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
    b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    params <span class="token operator">=</span> <span class="token punctuation">[</span>W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span>

    <span class="token comment"># 4.2.3 激活函数</span>

    <span class="token comment"># 4.2.4 模型</span>

    <span class="token comment"># 4.2.5 损失函数</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token comment"># reduction='none'表示直接返回每个样本的损失，其他有sum、mean</span>

    <span class="token comment"># 4.2.6 训练</span>
    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span>
    updater <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> updater<span class="token punctuation">)</span>

    <span class="token comment"># 4.2.7 预测</span>
    d2l<span class="token punctuation">.</span>predict_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span></code></pre>
<h3 id="多层感知机的简洁实现">4.3 多层感知机的简洁实现</h3>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 4.3 多层感知机的简洁实现</span>

    <span class="token comment"># 4.3.1 定义模型</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token comment"># 将输入的多维张量转换为一维张量</span>
                        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 全连接层</span>
                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token comment"># 激活函数</span>
                        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># 全连接层</span>

    <span class="token comment"># 4.3.2 初始化模型参数</span>
    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>

    <span class="token comment"># 4.3.3 读取数据并训练模型</span>
    batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">10</span>
    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>

    <span class="token comment"># 4.3.4 预测</span>
    d2l<span class="token punctuation">.</span>predict_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span></code></pre>
<h3 id="模型选择欠拟合和过拟合">4.4 模型选择、欠拟合和过拟合</h3>
<p>如何发现可以<strong>泛化</strong>的模式是机器学习的根本问题。</p>
<h4 id="训练误差和泛化误差">4.4.1 训练误差和泛化误差</h4>
<p>训练误差是指， 模型在训练数据集上计算得到的误差。 泛化误差是指，
模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。(实际中只能通过测试数据集来近似估计泛化误差)</p>
<p>几个倾向于影响模型泛化的因素:</p>
<ol type="1">
<li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li>
<li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li>
<li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li>
</ol>
<h4 id="模型选择">4.4.2 模型选择</h4>
<p>训练集: 用于训练模型的数据集 验证集: 用于调整模型超参数的数据集
测试集: 用于评估模型泛化误差的数据集</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>用途</th>
<th>参与训练</th>
<th>用来调参</th>
<th>用来评估最终性能</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>训练集</strong></td>
<td>拿来训练模型</td>
<td>✅ 是</td>
<td>❌ 否</td>
<td>❌ 否</td>
</tr>
<tr>
<td><strong>验证集</strong></td>
<td>拿来选模型/调参数</td>
<td>❌ 否</td>
<td>✅ 是</td>
<td>❌ 否</td>
</tr>
<tr>
<td><strong>测试集</strong></td>
<td>拿来最终评估</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>✅ 是</td>
</tr>
</tbody>
</table>
<p>（但现实是验证数据和测试数据之间的边界模糊得令人担忧。）</p>
<p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是<strong>采用<span
class="math inline">\(K\)</span>折交叉验证</strong>。
这里，原始训练数据被分成<span
class="math inline">\(K\)</span>个不重叠的子集。 然后执行<span
class="math inline">\(K\)</span>次模型训练和验证，每次在<span
class="math inline">\(K-1\)</span>个子集上进行训练，
并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。
最后，通过对<span
class="math inline">\(K\)</span>次实验的结果取平均来估计训练和验证误差。</p>
<h4 id="欠拟合和过拟合">4.4.3 欠拟合和过拟合</h4>
<p><strong>欠拟合</strong> 训练误差和验证误差都很严重，
但它们之间仅有一点差距。
如果<strong>模型不能降低训练误差</strong>，这可能意味着模型过于简单（即表达能力不足），
无法捕获试图学习的模式。
此外，由于我们的训练和验证误差之间的泛化误差很小，
我们有理由相信可以用一个更复杂的模型降低训练误差。</p>
<p><strong>过拟合</strong> 当我们的训练误差明显低于验证误差时要小心，
这表明严重的过拟合。 <strong>注意，过拟合并不总是一件坏事。</strong>
特别是在深度学习领域，众所周知，
最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。
<strong>最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</strong></p>
<p><strong><em>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小</em></strong></p>
<h5 id="模型复杂性">4.4.3.1 模型复杂性</h5>
<p>比如一个线性回归模型的拟合(特征是<span
class="math inline">\(x\)</span>的幂给出的， 模型的权重是<span
class="math inline">\(w_i\)</span>给出的，偏置是<span
class="math inline">\(w_0\)</span>给出):</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\hat{y}= \sum_{i=0}^d x^i w_i
\end{gathered}\end{equation}
\tag{4.7}
\]</span></p>
<p>高阶多项式函数比低阶多项式函数复杂得多。
高阶多项式的参数较多，模型函数的选择范围较广。
因此在固定训练数据集的情况下，
高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。
事实上，当数据样本包含了<span
class="math inline">\(x\)</span>的不同值时，
函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。</p>
<p>模型的选择和拟合情况如下图所示：</p>

      <div style="text-align: center;">
        <img src="./images/4.3_model-complexity-vs-dataset-size.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.3_model-complexity-vs-dataset-size.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.3：模型复杂度对欠拟合和过拟合的影响</span>
      </div>
    
<h5 id="数据集大小">4.4.3.2 数据集大小</h5>
<p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。
随着训练数据量的增加，泛化误差通常会减小。</p>
<h4 id="多项式回归">4.4.4 多项式回归</h4>
<p>通过多项式拟合来探索这些概念</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> math
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">evaluate_loss</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>
    <span class="token triple-quoted-string string">"""评估给定数据集上模型的损失"""</span>
    metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 损失的总和,样本数量</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        out <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> l<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>


<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span>
          num_epochs<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
    input_shape <span class="token operator">=</span> train_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># 不设置偏置，因为我们已经在多项式中实现了它</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    batch_size <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                batch_size<span class="token punctuation">)</span>
    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>
                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1e-3</span><span class="token punctuation">,</span> <span class="token number">1e2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        d2l<span class="token punctuation">.</span>train_epoch_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span>
        <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">20</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'weight:'</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1. 生成数据集</span>
    max_degree <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># 多项式的最大阶数</span>
    n_train<span class="token punctuation">,</span> n_test <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span>  <span class="token comment"># 训练和测试数据集大小</span>
    true_w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span>  <span class="token comment"># 分配大量的空间</span>
    true_w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">,</span> <span class="token number">5.6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    features <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_train <span class="token operator">+</span> n_test<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 噪声项</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>features<span class="token punctuation">)</span> <span class="token comment"># 打乱顺序</span>
    poly_features <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>features<span class="token punctuation">,</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">:</span>
        poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/=</span> math<span class="token punctuation">.</span>gamma<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># gamma(n)=(n-1)!</span>
    <span class="token comment"># labels的维度:(n_train+n_test,)</span>
    labels <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>poly_features<span class="token punctuation">,</span> true_w<span class="token punctuation">)</span>
    labels <span class="token operator">+=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

    <span class="token comment"># NumPy ndarray转换为tensor</span>
    true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        x<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span>true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">]</span>

    <span class="token comment"># 2. 定义、训练和测试模型</span>
    <span class="token comment"># 在外部函数中定义</span>

    <span class="token comment"># 3.1  三阶多项式函数拟合(正常)</span>
    <span class="token comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span>
    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 3.2 线性函数拟合(欠拟合)</span>
    <span class="token comment"># 从多项式特征中选择前2个维度，即1和x</span>
    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 减少该模型的训练损失相对困难，在最后一个迭代周期完成后，训练损失仍然很高。</span>

    <span class="token comment"># 3.3 高阶多项式函数拟合(过拟合)</span>
    <span class="token comment"># 从多项式特征中选取所有维度，即1,x,x^2/2!,...,x^19/19!</span>
    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">1500</span><span class="token punctuation">)</span>
    <span class="token comment"># 训练误差迅速降低，但测试损失仍然很高</span></code></pre>
<p>数据集的生成：</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{
where }
\epsilon \sim \mathcal{N}(0, 0.1^2)
\end{gathered}\end{equation}
\tag{4.8}
\]</span></p>
<p>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。
这就是我们将特征从 <span class="math inline">\(x^i\)</span> 调整为 <span
class="math inline">\(\frac{x^i}{i!}\)</span> 的原因，
这样可以避免很大的 <span class="math inline">\(i\)</span>
带来的特别大的指数值。</p>
<h5 id="使用线性函数拟合欠拟合">4.4.4.1 使用线性函数拟合(欠拟合)</h5>
<p>减少该模型的训练损失相对困难。
在最后一个迭代周期完成后，训练损失仍然很高。</p>

      <div style="text-align: center;">
        <img src="./images/4.4_linear-function-fit.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.4_linear-function-fit.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.4：使用线性函数拟合</span>
      </div>
    
<h5 id="使用三阶多项式函数拟合合适">4.4.4.2
使用三阶多项式函数拟合(合适)</h5>
<p>该模型能有效降低训练损失和测试损失。
学习到的模型参数也接近真实值。</p>

      <div style="text-align: center;">
        <img src="./images/4.5_cubic-function-fit.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.5_cubic-function-fit.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.5：使用三阶多项式函数拟合</span>
      </div>
    
<h5 id="使用高阶多项式函数拟合过拟合">4.4.4.3
使用高阶多项式函数拟合(过拟合)</h5>
<p>在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。
因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。
虽然训练损失可以有效地降低，但测试损失仍然很高。</p>

      <div style="text-align: center;">
        <img src="./images/4.6_high-order-function-fit.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.6_high-order-function-fit.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.6：使用高阶多项式函数拟合</span>
      </div>
    
<h3 id="权重衰减">4.5 权重衰减</h3>
<p>虽然可以通过去收集更多的训练数据来缓解<strong>过拟合</strong>，但是……并不太可能
假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在<strong>正则化技术</strong>上。正则化是处理<strong>过拟合</strong>的常用方法：<strong>在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度</strong>。
<strong>权重衰减（weight decay）是最广泛使用的正则化的技术之一，
它通常也被称为<span
class="math inline">\(L_2\)</span>正则化。</strong></p>
<p>原来的损失函数:</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n
\frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2
\end{gathered}\end{equation}
\tag{4.9}
\]</span></p>
<p>现在的损失函数(通过正则化常数<span
class="math inline">\(\lambda\)</span>来描述这种权衡，
这是一个非负超参数):</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2
\end{gathered}\end{equation}
\tag{4.10}
\]</span></p>
<h4 id="高维线性回归">4.5.1 高维线性回归</h4>
<p>使用的数据集由下面公式生成:</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.01^2)
\end{gathered}\end{equation}
\tag{4.11}
\]</span></p>
<p>为了使过拟合的效果更加明显，我们可以将问题的维数增加到 <span
class="math inline">\(d=200\)</span>，
并使用一个只包含20个样本的小训练集。</p>
<h4 id="从零开始实现">4.5.2 从零开始实现</h4>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch

<span class="token comment"># 随机初始化模型参数</span>
<span class="token keyword">def</span> <span class="token function">init_params</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    w <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span>

<span class="token comment"># L2范数惩罚</span>
<span class="token keyword">def</span> <span class="token function">l2_penalty</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>

<span class="token comment"># 定义训练</span>
<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>lambd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    w<span class="token punctuation">,</span> b <span class="token operator">=</span> init_params<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token keyword">lambda</span> X<span class="token punctuation">:</span> d2l<span class="token punctuation">.</span>linreg<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>squared_loss
    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0.003</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epochs'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>
                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            <span class="token comment"># 增加了L2范数惩罚项，</span>
            <span class="token comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">+</span> lambd <span class="token operator">*</span> l2_penalty<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w的L2范数是:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    n_train<span class="token punctuation">,</span> n_test<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">5</span>
    true_w<span class="token punctuation">,</span> true_b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.05</span>
    train_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_train<span class="token punctuation">)</span>
    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    test_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_test<span class="token punctuation">)</span>
    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token comment"># 无正则化</span>
    train<span class="token punctuation">(</span>lambd<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># 使用权重衰减</span>
    train<span class="token punctuation">(</span>lambd<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre>
<h5 id="不使用正则化">4.5.2.1 不使用正则化</h5>
<p>这里训练误差有了减少，但测试误差没有减少，
这意味着出现了严重的过拟合</p>

      <div style="text-align: center;">
        <img src="./images/4.7_no-weight-decay.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.7_no-weight-decay.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.7：不使用正则化</span>
      </div>
    
<h5 id="使用权重衰减">4.5.2.2 使用权重衰减</h5>
<p>在这里训练误差增大，但测试误差减小。
这正是我们期望从正则化中得到的效果。</p>

      <div style="text-align: center;">
        <img src="./images/4.8_weight-decay.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.8_weight-decay.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.8：使用权重衰减</span>
      </div>
    
<h4 id="简洁实现">4.5.3 简洁实现</h4>
<p>深度学习框架为了便于我们使用权重衰减，
将权重衰减集成到优化算法中，以便与任何损失函数结合使用。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch


<span class="token keyword">def</span> <span class="token function">train_concise</span><span class="token punctuation">(</span>wd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0.003</span>
    <span class="token comment"># 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了weight_decay，所以偏置参数b不会衰减。</span>
    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> wd<span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epochs'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>
                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span>
                         <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>
                          d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w的L2范数:'</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    n_train<span class="token punctuation">,</span> n_test<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">5</span>
    true_w<span class="token punctuation">,</span> true_b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.05</span>
    train_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_train<span class="token punctuation">)</span>
    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    test_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_test<span class="token punctuation">)</span>
    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token comment"># 无正则化</span>
    train_concise<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># 使用权重衰减</span>
    train_concise<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre>
<h5 id="不使用正则化-1">4.5.3.1 不使用正则化</h5>

      <div style="text-align: center;">
        <img src="./images/4.9_no-weight-decay.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.9_no-weight-decay.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.9：不使用正则化</span>
      </div>
    
<h5 id="使用权重衰减-1">4.5.3.2 使用权重衰减</h5>

      <div style="text-align: center;">
        <img src="./images/4.10_weight-decay.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.10_weight-decay.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.10：使用权重衰减</span>
      </div>
    
<h3 id="暂退法">4.6 暂退法</h3>
<p><strong>暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。</strong>
这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop
out）一些神经元。
<strong>在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</strong></p>
<p>那么关键的挑战就是如何注入这种噪声。
一种想法是以一种无偏向（unbiased）的方式注入噪声。
这样在固定住其他层时，每一层的期望值等于没有噪音时的值。
在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差:</p>
<p><span class="math display">\[
\begin{split}\begin{aligned}
h&#39; =
\begin{cases}
    0 &amp; \text{ 概率为 } p \\
    \frac{h}{1-p} &amp; \text{ 其他情况}
\end{cases}
\end{aligned}\end{split}
\tag{4.12}
\]</span></p>
<p>期望值保持不变，即 <span class="math inline">\(E[h&#39;] =
h\)</span>。</p>
<h4 id="实践中的暂退法">4.6.1 实践中的暂退法</h4>
<p>当我们将暂退法应用到隐藏层，以<span
class="math inline">\(p\)</span>的概率将隐藏单元置为零时，
结果可以看作一个只包含原始神经元子集的网络。 如图4.11， 删除了<span
class="math inline">\(h_2\)</span>和<span
class="math inline">\(h_5\)</span>， 因此输出的计算不再依赖于<span
class="math inline">\(h_2\)</span>或<span
class="math inline">\(h_5\)</span>，并且它们各自的梯度在执行反向传播时也会消失。
这样，输出层的计算不能过度依赖于<span
class="math inline">\(h_1,...,h_5\)</span>的任何一个元素。</p>

      <div style="text-align: center;">
        <img src="./images/4.11_dropout_MLP.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.11_dropout_MLP.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.11：dropout前后的多层感知机</span>
      </div>
    
<p>通常，我们在测试时不用暂退法。
给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。
然而也有一些例外：一些研究人员在测试时使用暂退法，
用于估计神经网络预测的“不确定性”：
如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p>
<h4 id="从零开始实现-1">4.6.2 从零开始实现</h4>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

<span class="token comment"># 定义一个dropout函数</span>
<span class="token keyword">def</span> <span class="token function">dropout_layer</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> dropout <span class="token operator">&lt;=</span> <span class="token number">1</span>
    <span class="token comment"># 在本情况中，所有元素都被丢弃</span>
    <span class="token keyword">if</span> dropout <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token comment"># 在本情况中，所有元素都被保留</span>
    <span class="token keyword">if</span> dropout <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> X
    mask <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">></span> dropout<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 生成一个mask</span>
    <span class="token keyword">return</span> mask <span class="token operator">*</span> X <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> dropout<span class="token punctuation">)</span> <span class="token comment"># 按位mask</span>

<span class="token comment"># 测试dropout函数</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">,</span>
                 is_training <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_inputs <span class="token operator">=</span> num_inputs
        self<span class="token punctuation">.</span>training <span class="token operator">=</span> is_training
        self<span class="token punctuation">.</span>lin1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lin2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lin3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        H1 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lin1<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 只有在训练模型时才使用dropout</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>
            H1 <span class="token operator">=</span> dropout_layer<span class="token punctuation">(</span>H1<span class="token punctuation">,</span> dropout1<span class="token punctuation">)</span>
        H2 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lin2<span class="token punctuation">(</span>H1<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>
            H2 <span class="token operator">=</span> dropout_layer<span class="token punctuation">(</span>H2<span class="token punctuation">,</span> dropout2<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>lin3<span class="token punctuation">(</span>H2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span>

    <span class="token comment"># 定义模型</span>
    <span class="token comment"># 我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 </span>
    <span class="token comment"># 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。</span>
    dropout1<span class="token punctuation">,</span> dropout2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span>
    net <span class="token operator">=</span> Net<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>

    <span class="token comment"># 训练和测试模型</span>
    num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">256</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></code></pre>
<h4 id="简洁实现-1">4.6.3 简洁实现</h4>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>
                    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout1<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>
                    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout2<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>


net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>

trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></code></pre>
<h3 id="前向传播反向传播和计算图">4.7 前向传播、反向传播和计算图</h3>
<h4 id="前向传播">4.7.1 前向传播</h4>
<p>按顺序（从输入层到输出层）<strong>计算</strong>和<strong>存储</strong>神经网络中每层的结果。</p>
<p><span class="math display">\[
\begin{equation}\begin{gathered}
\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}\\
\mathbf{h}= \phi(\mathbf{z})\\
\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}\\
\end{gathered}\end{equation}
\tag{4.13}
\]</span></p>
<p>损失函数<span class="math inline">\(l\)</span>，样本标签<span
class="math inline">\(y\)</span>，则损失项，正则项，正则化损失:</p>
<p><span class="math display">\[
\begin{gathered}
L = l(\mathbf{o}, y)\\
s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 +
\|\mathbf{W}^{(2)}\|_F^2\right)\\
J = L + s
\end{gathered}
\tag{4.14}
\]</span></p>
<p>目标函数即为<span class="math inline">\(J\)</span>。</p>
<h4 id="前向传播计算图">4.7.2 前向传播计算图</h4>
<p>正方形表示变量，圆圈表示操作符，左下角表示输入，右上角表示输出。</p>

      <div style="text-align: center;">
        <img src="./images/4.12_forward.svg" onerror="this.onerror=null,this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" data-lazy-src="./images/4.12_forward.svg" style="display: block; margin: 0 auto; max-width: 500px; max-height: 500px;">
        <span style="margin-top: 10px; text-decoration: underline; text-underline-offset: 2px; text-decoration-color: #d9d9d9; font-size: 20px; display: block;">图4.12：前向传播计算图</span>
      </div>
    
<h4 id="反向传播">4.7.3 反向传播</h4>
<p>反向传播指的是计算神经网络参数梯度的方法。
简言之，该方法根据微积分中的<strong>链式规则</strong>，按相反的顺序从输出层到输入层遍历网络。
<strong>该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）</strong>。</p>
<p>在此例子中，即通过链式法则计算<span class="math inline">\(\partial
J/\partial \mathbf{W}^{(1)}\)</span>和<span
class="math inline">\(\partial J/\partial
\mathbf{W}^{(2)}\)</span>。</p>
<p><span class="math display">\[
\begin{gathered}
  \frac{\partial J}{\partial \mathbf{W}^{(2)}}=
\text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial
\mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) +
\text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial
s}{\partial \mathbf{W}^{(2)}}\right)\\
  \frac{\partial J}{\partial \mathbf{W}^{(1)}}
  = \text{prod}\left(\frac{\partial J}{\partial \mathbf{z}},
\frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) +
\text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial
s}{\partial \mathbf{W}^{(1)}}\right)
\end{gathered}
\tag{4.15}
\]</span></p>
<h4 id="内存需求">4.7.4 内存需求</h4>
<p>因此，在训练神经网络时，在初始化模型参数后，
我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。
注意，<strong>反向传播重复利用前向传播中存储的中间值</strong>，以避免重复计算。
带来的影响之一是我们<strong>需要保留中间值</strong>，直到反向传播完成。
这也是<strong>训练比单纯的预测需要更多的内存（显存）</strong>的原因之一。
此外，这些<strong>中间值的大小与网络层的数量和批量的大小大致成正比</strong>。
因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of
memory）错误。</p>
<h3 id="数值稳定性和模型初始化">4.8 数值稳定性和模型初始化</h3>
<p>初始化方案的选择在神经网络学习中起着举足轻重的作用，
它对保持数值稳定性至关重要。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</p>
<ol type="1">
<li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li>
<li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
<li>随机初始化是保证在进行优化前打破对称性的关键。</li>
</ol>
<h4 id="梯度爆炸梯度消失打破对称性">4.8.1
梯度爆炸、梯度消失、打破对称性</h4>
<p><strong>梯度消失（gradient vanishing）</strong>：
<strong>参数更新过小，在每次更新时几乎不会移动</strong>，导致模型无法学习。根据链式求导法则，梯度的计算是由不同因子的连乘结果，只要其中某个因子的数值小于1那么随着网络的加深，后续的梯度一定是逐渐降低的（假设其他因子设置合理）。如果因子的数值够低，后续梯度甚至会出现消失现象，导致网络难以训练和收敛，这就是梯度消失的现象。例如:
当sigmoid函数的输入很大或是很小时，它的梯度都会消失。
<strong>梯度爆炸（gradient exploding）</strong>：
<strong>参数更新过大，破坏了模型的稳定收敛</strong>。同梯度消失的原理一样，梯度爆炸也是因为因子的数值大于1，在经过网络的不断加深，后续梯度出现爆炸的现象。
<strong>打破对称性</strong>：神经网络设计中的另一个问题是其参数化所固有的对称性。
如果我们<strong>将隐藏层的所有参数初始化为同一个常量</strong>，会发生什么？
在这种情况下，在前向传播期间，隐藏单元采用相同的输入和参数，
产生相同的激活，该激活被送到输出单元。
在反向传播期间，根据参数对输出单元进行微分，
得到一个梯度，其元素都取相同的值。 因此，在基于梯度的迭代之后，
隐藏单元的所有元素仍然采用相同的值。
这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。
<strong>隐藏层的行为就好像只有一个单元</strong>。
请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p>
<h4 id="初始化参数">4.8.2 初始化参数</h4>
<ol type="1">
<li>Pytorch的默认初始化</li>
<li>Xavier初始化: 从均值为零，方差 <span class="math inline">\(\sigma^2
= \frac{2}{n_\mathrm{in} + n_\mathrm{out}}\)</span>
的高斯分布中采样权重。</li>
</ol>
<h3 id="环境与分布偏移">4.9 环境与分布偏移</h3>
<p><strong>分布偏移（Distribution
Shift）是指模型在训练和测试数据集之间的数据分布不匹配的情况。</strong>这种不匹配可能导致模型在测试集上的表现下降，因为模型在训练时学习到的特征在测试时可能不再适用。</p>
<h4 id="分布偏移的类型">4.9.1 分布偏移的类型</h4>
<ol type="1">
<li><strong>协变量（特征）偏移</strong>：虽然输入的分布改变，但标签没有改变。例如猫狗识别，训练数据主要来自家养，而测试数据主要来自野外拍摄，此时输入数据的分布发生了变化，但标签（猫或狗）没有变化。</li>
<li><strong>标签偏移</strong>：虽然输入的分布保持不变，但标签的分布改变。假设你训练了一个模型来预测某城市的天气，训练数据中晴天和雨天的比例是9:1，但测试数据中这个比例变成了1:1。输入的天气特征分布保持不变，但标签的分布发生了变化。</li>
<li><strong>概念偏移</strong>：输入和输出之间的映射关系发生了变化，即特征和标签之间的关系变了。例如，有一个垃圾邮件分类器，训练数据中垃圾邮件的特征是某些关键词（如“免费”、“优惠”等），但随着时间推移，垃圾邮件发送者改变了策略，使用了新的关键词（如“促销”、“折扣”等）。</li>
</ol>
<h4 id="分布偏移纠正">4.9.2 分布偏移纠正</h4>
<p>可以采用<strong>加权经验风险最小化</strong>等方法。</p>
<p>注：真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。</p>
<h3 id="实战kaggle比赛房价预测">4.10 实战Kaggle比赛：房价预测</h3>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> hashlib
<span class="token keyword">import</span> os
<span class="token keyword">import</span> tarfile
<span class="token keyword">import</span> zipfile
<span class="token keyword">import</span> requests
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">download</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'..'</span><span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""下载一个DATA_HUB中的文件，返回本地文件名"""</span>
    <span class="token keyword">assert</span> name <span class="token keyword">in</span> DATA_HUB<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string"> 不存在于 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>DATA_HUB<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
    url<span class="token punctuation">,</span> sha1_hash <span class="token operator">=</span> DATA_HUB<span class="token punctuation">[</span>name<span class="token punctuation">]</span>
    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>cache_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    fname <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cache_dir<span class="token punctuation">,</span> url<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sha1 <span class="token operator">=</span> hashlib<span class="token punctuation">.</span>sha1<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 初始化一个sha1对象</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
                data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token number">1048576</span><span class="token punctuation">)</span>  <span class="token comment"># 逐块读取文件内容，每次读取 1 MB</span>
                <span class="token keyword">if</span> <span class="token keyword">not</span> data<span class="token punctuation">:</span>
                    <span class="token keyword">break</span>
                sha1<span class="token punctuation">.</span>update<span class="token punctuation">(</span>data<span class="token punctuation">)</span>  <span class="token comment"># sha1可以在历史哈希值基础上用当前文件内容更新哈希值</span>
        <span class="token keyword">if</span> sha1<span class="token punctuation">.</span>hexdigest<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> sha1_hash<span class="token punctuation">:</span>
            <span class="token keyword">return</span> fname  <span class="token comment"># 命中缓存</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'正在从</span><span class="token interpolation"><span class="token punctuation">&#123;</span>url<span class="token punctuation">&#125;</span></span><span class="token string">下载</span><span class="token interpolation"><span class="token punctuation">&#123;</span>fname<span class="token punctuation">&#125;</span></span><span class="token string">...'</span></span><span class="token punctuation">)</span>
    r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> stream<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> verify<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>r<span class="token punctuation">.</span>content<span class="token punctuation">)</span>
    <span class="token keyword">return</span> fname


<span class="token keyword">def</span> <span class="token function">download_extract</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> folder<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""下载并解压zip/tar文件"""</span>
    fname <span class="token operator">=</span> download<span class="token punctuation">(</span>name<span class="token punctuation">)</span>
    base_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>
    data_dir<span class="token punctuation">,</span> ext <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>
    <span class="token keyword">if</span> ext <span class="token operator">==</span> <span class="token string">'.zip'</span><span class="token punctuation">:</span>
        fp <span class="token operator">=</span> zipfile<span class="token punctuation">.</span>ZipFile<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> ext <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'.tar'</span><span class="token punctuation">,</span> <span class="token string">'.gz'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        fp <span class="token operator">=</span> tarfile<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'只有zip/tar文件可以被解压缩'</span>
    fp<span class="token punctuation">.</span>extractall<span class="token punctuation">(</span>base_dir<span class="token punctuation">)</span>
    <span class="token keyword">return</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>base_dir<span class="token punctuation">,</span> folder<span class="token punctuation">)</span> <span class="token keyword">if</span> folder <span class="token keyword">else</span> data_dir


<span class="token keyword">def</span> <span class="token function">download_all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""下载DATA_HUB中的所有文件"""</span>
    <span class="token keyword">for</span> name <span class="token keyword">in</span> DATA_HUB<span class="token punctuation">:</span>
        download<span class="token punctuation">(</span>name<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">get_net</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> net

<span class="token comment"># 采用相对误差而不是绝对误差来衡量误差</span>


<span class="token keyword">def</span> <span class="token function">log_rmse</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 将预测值中小于1的部分设置为1，以避免在取对数时出现负无穷大的情况</span>
    clipped_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    rmse <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>clipped_preds<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> rmse<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span>
          num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_ls<span class="token punctuation">,</span> test_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    <span class="token comment"># 这里使用的是Adam优化算法</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 lr<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span>
                                 weight_decay<span class="token operator">=</span>weight_decay<span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> test_labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_ls<span class="token punctuation">,</span> test_ls

<span class="token comment"># 用K折交叉验证来评估模型</span>


<span class="token keyword">def</span> <span class="token function">get_k_fold_data</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> i<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> k <span class="token operator">></span> <span class="token number">1</span>
    fold_size <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> k
    X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>
        idx <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>j <span class="token operator">*</span> fold_size<span class="token punctuation">,</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> fold_size<span class="token punctuation">)</span>
        X_part<span class="token punctuation">,</span> y_part <span class="token operator">=</span> X<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
        <span class="token keyword">if</span> j <span class="token operator">==</span> i<span class="token punctuation">:</span>
            X_valid<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> X_part<span class="token punctuation">,</span> y_part
        <span class="token keyword">elif</span> X_train <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> X_part<span class="token punctuation">,</span> y_part
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            X_train <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>X_train<span class="token punctuation">,</span> X_part<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            y_train <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>y_train<span class="token punctuation">,</span> y_part<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_valid


<span class="token keyword">def</span> <span class="token function">k_fold</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_l_sum<span class="token punctuation">,</span> valid_l_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>
        data <span class="token operator">=</span> get_k_fold_data<span class="token punctuation">(</span>k<span class="token punctuation">,</span> i<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
        net <span class="token operator">=</span> get_net<span class="token punctuation">(</span><span class="token punctuation">)</span>
        train_ls<span class="token punctuation">,</span> valid_ls <span class="token operator">=</span> train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token operator">*</span>data<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span>
                                   weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        train_l_sum <span class="token operator">+=</span> train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        valid_l_sum <span class="token operator">+=</span> valid_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            d2l<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>train_ls<span class="token punctuation">,</span> valid_ls<span class="token punctuation">]</span><span class="token punctuation">,</span>
                     xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'rmse'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>
                     legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'valid'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'折</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">，训练log rmse</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">, '</span></span>
              <span class="token string-interpolation"><span class="token string">f'验证log rmse</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>valid_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_l_sum <span class="token operator">/</span> k<span class="token punctuation">,</span> valid_l_sum <span class="token operator">/</span> k


<span class="token keyword">def</span> <span class="token function">train_and_pred</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_data<span class="token punctuation">,</span>
                   num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> get_net<span class="token punctuation">(</span><span class="token punctuation">)</span>
    train_ls<span class="token punctuation">,</span> _ <span class="token operator">=</span> train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                        num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    d2l<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>train_ls<span class="token punctuation">]</span><span class="token punctuation">,</span> xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span>
             ylabel<span class="token operator">=</span><span class="token string">'log rmse'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'训练log rmse：</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token comment"># 将网络应用于测试集。</span>
    preds <span class="token operator">=</span> net<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 将其重新格式化以导出到Kaggle</span>
    test_data<span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span> <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>preds<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    submission <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>test_data<span class="token punctuation">[</span><span class="token string">'Id'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    submission<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">'submission.csv'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    DATA_HUB <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    DATA_URL <span class="token operator">=</span> <span class="token string">'http://d2l-data.s3-accelerate.amazonaws.com/'</span>

    DATA_HUB<span class="token punctuation">[</span><span class="token string">'kaggle_house_train'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
        DATA_URL <span class="token operator">+</span> <span class="token string">'kaggle_house_pred_train.csv'</span><span class="token punctuation">,</span> <span class="token string">'585e9cc93e70b39160e7921475f9bcd7d31219ce'</span><span class="token punctuation">)</span>
    DATA_HUB<span class="token punctuation">[</span><span class="token string">'kaggle_house_test'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
        DATA_URL <span class="token operator">+</span> <span class="token string">'kaggle_house_pred_test.csv'</span><span class="token punctuation">,</span> <span class="token string">'fa19780a7b011d9b009e8bff8e99922a8ee2eb90'</span><span class="token punctuation">)</span>

    <span class="token comment">#  划分训练集以创建验证集</span>
    train_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>download<span class="token punctuation">(</span><span class="token string">'kaggle_house_train'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    test_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>download<span class="token punctuation">(</span><span class="token string">'kaggle_house_test'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 删除ID列以及标签列</span>
    all_features <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 数据预处理</span>
    <span class="token comment"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span>
    numeric_features <span class="token operator">=</span> all_features<span class="token punctuation">.</span>dtypes<span class="token punctuation">[</span>all_features<span class="token punctuation">.</span>dtypes <span class="token operator">!=</span>
                                           <span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index
    all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span> <span class="token operator">=</span> all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>
        <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 标准化数据有两个原因： 方便优化；不知道哪些特征是相关的，避免让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大。</span>
    <span class="token comment"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span>
    all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span> <span class="token operator">=</span> all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment"># 将离散数值转换为独热编码(get_dummies函数将分类变量转换为虚拟变量)</span>
    <span class="token comment"># “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征</span>
    all_features <span class="token operator">=</span> pd<span class="token punctuation">.</span>get_dummies<span class="token punctuation">(</span>all_features<span class="token punctuation">,</span> dummy_na<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># 通过values属性得到NumPy格式的数据，并转换成张量</span>
    n_train <span class="token operator">=</span> train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    train_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        all_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    test_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        all_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    train_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        train_data<span class="token punctuation">.</span>SalePrice<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

    <span class="token comment"># 训练模型</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    in_features <span class="token operator">=</span> train_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    k<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">64</span>
    <span class="token comment"># 将训练集划分为K份，然后使用第K份作为验证集，其余作为训练集</span>
    train_l<span class="token punctuation">,</span> valid_l <span class="token operator">=</span> k_fold<span class="token punctuation">(</span>
        k<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">-折验证: 平均训练log rmse: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_l<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">, '</span></span><span class="token string-interpolation"><span class="token string">f'平均验证log rmse: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>valid_l<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

    <span class="token comment"># 使用所有数据对其进行训练，然后预测并在Kaggle提交结果</span>
    train_and_pred<span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span>
                   test_data<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span></code></pre>
<h3 id="附录">附录</h3>
<h4 id="附录a激活函数绘图代码">附录A：激活函数绘图代码</h4>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib <span class="token keyword">as</span> mpl
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 设置中文字体</span>
mpl<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'SimHei'</span><span class="token punctuation">]</span>
mpl<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>

<span class="token comment"># 定义激活函数</span>


<span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">tanh</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">prelu</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> alpha <span class="token operator">*</span> np<span class="token punctuation">.</span>minimum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>

<span class="token comment"># 定义导数</span>


<span class="token keyword">def</span> <span class="token function">relu_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">sigmoid_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    s <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> s <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> s<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">tanh_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>


<span class="token keyword">def</span> <span class="token function">prelu_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> alpha<span class="token punctuation">)</span>


<span class="token comment"># 创建数据点</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>

<span class="token comment"># 创建图形和子图 (2行4列，共8个子图)</span>
fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 设置图表标题</span>
fig<span class="token punctuation">.</span>suptitle<span class="token punctuation">(</span><span class="token string">'激活函数及其导数对比'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 第一行：激活函数</span>
<span class="token comment"># ReLU 激活函数</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'ReLU 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'ReLU(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># Sigmoid 激活函数</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'g-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Sigmoid 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Sigmoid(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># Tanh 激活函数</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'b-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Tanh 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Tanh(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># pReLU 激活函数</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prelu<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'m-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'pReLU 激活函数 (α=0.2)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'pReLU(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># 第二行：导数</span>
<span class="token comment"># ReLU 导数</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> relu_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'ReLU 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'ReLU\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># Sigmoid 导数</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sigmoid_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'g-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Sigmoid 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Sigmoid\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># Tanh 导数</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tanh_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'b-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Tanh 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Tanh\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># pReLU 导数</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prelu_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'m-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'pReLU 导数 (α=0.2)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'pReLU\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token comment"># 调整布局</span>
plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存图像</span>
save_path <span class="token operator">=</span> <span class="token string">'./source/_posts/DL/activation_functions.svg'</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span>save_path<span class="token punctuation">,</span> <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">'svg'</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> bbox_inches<span class="token operator">=</span><span class="token string">'tight'</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"图像已保存至: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>save_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 显示图像</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<h4
id="附录b神经网络反向传播通俗解释">附录B：神经网络反向传播通俗解释</h4>
<p>神经网络使用反向传播算法进行训练，核心是链式法则（链式求导），训练的目标是最小化损失函数。我们要解决的就是怎么调整模型的参数<span
class="math inline">\(\omega\)</span>才能让损失函数最小。</p>
<p>这就类似于<span
class="math inline">\(y=f(x)\)</span>的函数，我们想要找到一个最优的<span
class="math inline">\(x\)</span>使得<span
class="math inline">\(y\)</span>最小。这里的<span
class="math inline">\(x\)</span>就是神经网络的参数<span
class="math inline">\(\omega\)</span>，而<span
class="math inline">\(y\)</span>就是损失函数<span
class="math inline">\(L(\omega)\)</span>。自然而然的引出了导数的概念，也就是梯度<span
class="math inline">\(\frac{\partial L}{\partial
\omega}\)</span>，它告诉我们在当前参数位置，损失函数的变化率。</p>
<p>由此引出以下几个问题：</p>
<p><strong>1. 导数和梯度的关系是什么？</strong></p>
<p>情况一：一元函数（只有一个输入）<span class="math inline">\(f(w) =
w^2\)</span></p>
<ul>
<li>它的导数是：<span class="math inline">\(\frac{df}{dw} =
2w\)</span></li>
<li>它的梯度也是：<span class="math inline">\(\nabla f(w) =
(2w)\)</span></li>
</ul>
<p>在这种情况下，<strong>梯度就等于导数本身</strong>，只是写成了一个一维向量。</p>
<p>情况二：多元函数（多个输入）<span class="math inline">\(f(w_1, w_2) =
w_1^2 + w_2^2\)</span></p>
<p><strong>没法用「导数」描述它的全部方向的变化</strong>。要用梯度：<span
class="math inline">\(\nabla f = \left( \frac{\partial f}{\partial w_1},
\frac{\partial f}{\partial w_2} \right) = (2w_1, 2w_2)\)</span></p>
<table>
<colgroup>
<col style="width: 46%" />
<col style="width: 12%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>情况</th>
<th>使用的概念</th>
<th>是否等价</th>
</tr>
</thead>
<tbody>
<tr>
<td>一维函数 <span class="math inline">\(f(w)\)</span></td>
<td>导数 &amp; 梯度</td>
<td>✅ 等价（梯度是 1 维向量）</td>
</tr>
<tr>
<td>多维函数 <span class="math inline">\(f(w_1, w_2,
\dots)\)</span></td>
<td>梯度（向量）</td>
<td>❌ 不等价，导数只看一个方向，梯度是全部方向</td>
</tr>
</tbody>
</table>
<p><strong>2. 为什么不直接让梯度为0就可以了？</strong></p>
<table>
<colgroup>
<col style="width: 31%" />
<col style="width: 68%" />
</colgroup>
<thead>
<tr>
<th>问题</th>
<th>回答</th>
</tr>
</thead>
<tbody>
<tr>
<td>为什么不直接让梯度为 0？</td>
<td>因为求不出来（无解析解），而且梯度为 0 不一定是最小值</td>
</tr>
<tr>
<td>那我们怎么做？</td>
<td>使用梯度下降，沿着负梯度方向一点点走，逐步逼近最小值</td>
</tr>
<tr>
<td>好处？</td>
<td>不需要显式解方程，适合高维、大规模、非线性优化问题</td>
</tr>
</tbody>
</table>
<p><strong>3. 有了梯度之后怎么更新参数？为什么这么更新？</strong></p>
<p>有了梯度之后，<strong>我们用梯度的反方向来更新参数</strong>，因为梯度指出了损失函数增加最快的方向，所以反方向是<strong>下降最快的方向</strong>。</p>
<p>参数更新的公式（梯度下降法）。设：</p>
<ul>
<li><span
class="math inline">\(\mathbf{w}\)</span>：模型参数（可以是权重、偏置等）</li>
<li><span
class="math inline">\(\mathcal{L}(\mathbf{w})\)</span>：损失函数</li>
<li><span class="math inline">\(\nabla
\mathcal{L}(\mathbf{w})\)</span>：当前参数处的梯度</li>
<li><span class="math inline">\(\eta\)</span>：学习率（learning
rate，控制步子大小）</li>
</ul>
<p>那么参数更新公式为：</p>
<p><span class="math display">\[
\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \cdot \nabla
\mathcal{L}(\mathbf{w}_{\text{old}})
\tag{B.1}
\]</span></p>
<p>为什么是这个公式？</p>
<blockquote>
<p>1️⃣ 梯度告诉我们什么？</p>
</blockquote>
<p>梯度 <span class="math inline">\(\nabla
\mathcal{L}(\mathbf{w})\)</span> 是一个向量，表示在每一个维度上：</p>
<ul>
<li>增加参数 → 损失函数会上升多少</li>
</ul>
<p>所以梯度的方向，就是“损失函数上升最快的方向”</p>
<blockquote>
<p>2️⃣ 那我们想做什么？</p>
</blockquote>
<p>我们想让损失函数 <strong>变小</strong>，而不是变大！</p>
<p>因此，我们要朝<strong>损失下降最快的方向</strong>移动，那就是梯度的<strong>负方向</strong>：<span
class="math inline">\(-\nabla \mathcal{L}(\mathbf{w})\)</span></p>
<blockquote>
<p>3️⃣ 为什么还要乘一个 <span class="math inline">\(\eta\)</span>
学习率？</p>
</blockquote>
<p>梯度只是一个方向，我们还需要控制“走多远”：</p>
<ul>
<li>步子太大 → 可能错过最小值，甚至震荡或发散</li>
<li>步子太小 → 学得太慢，收敛极慢</li>
</ul>
<p>所以我们加一个系数 <span class="math inline">\(\eta\)</span>，就是
<strong>学习率</strong>，来控制更新的幅度：<span
class="math inline">\(\text{更新量} = -\eta \cdot \nabla
\mathcal{L}(\mathbf{w})\)</span></p>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 91%" />
</colgroup>
<thead>
<tr>
<th>步骤</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 计算梯度</td>
<td>得到损失函数对参数的导数（每个方向的变化率）</td>
</tr>
<tr>
<td>2. 取反方向</td>
<td>因为梯度是上升方向，下降要走反方向</td>
</tr>
<tr>
<td>3. 乘以学习率</td>
<td>控制更新的幅度，避免步子太大或太小</td>
</tr>
<tr>
<td>4. 更新参数</td>
<td><span class="math inline">\(\mathbf{w}_{\text{new}} =
\mathbf{w}_{\text{old}} - \eta \cdot \nabla \mathcal{L}\)</span></td>
</tr>
</tbody>
</table>
<h4
id="附录c为什么关注激活函数的导数">附录C：为什么关注激活函数的导数？</h4>
<blockquote>
<p><strong>举例：ReLU 的导数</strong></p>
</blockquote>
<ul>
<li><p>ReLU(x) = max(0, x)</p></li>
<li><p>它的导数为：</p>
<p><span class="math display">\[
\text{ReLU}&#39;(x) = \begin{cases}
1, &amp; x &gt; 0 \\
0, &amp; x \leq 0
\end{cases}
\tag{C.1}
\]</span></p></li>
</ul>
<p>这意味着只有当输入为正时，ReLU 才会让梯度流动——否则梯度为
0，神经元不再学习。</p>
<blockquote>
<p><strong>了解梯度消失或爆炸的问题</strong></p>
</blockquote>
<p>某些激活函数的导数可能导致梯度消失（如 Sigmoid）或爆炸（如 Softplus
的大输入）。分析导数可以帮助理解和选择<strong>更合适的激活函数</strong>，避免这些问题。</p>
<p><strong>例子：Sigmoid 的导数</strong></p>
<ul>
<li>最大值是 0.25，输入过大或过小时导数接近 0，导致梯度消失。</li>
</ul>
<blockquote>
<p><strong>设计新激活函数</strong></p>
</blockquote>
<p>很多改进型激活函数（如 Leaky
ReLU、ELU、Swish、Mish）都是基于对导数行为的深入理解后提出的。例如：</p>
<ul>
<li><p><strong>Leaky ReLU</strong>：解决 ReLU
的“神经元死亡”问题（即一旦输出为
0，就无法恢复），通过在负区间给一个小斜率（导数非零）：</p>
<p><span class="math display">\[
\text{Leaky ReLU}(x) = \begin{cases}
x, &amp; x &gt; 0 \\
0.01x, &amp; x \leq 0
\end{cases}
\tag{C.2}
\]</span></p></li>
</ul>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/avatar.ico" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/avatar.ico" title="头像" alt="头像"></a><div class="post-copyright__author_name">可爱可倾</div><div class="post-copyright__author_desc">花开有期，云舒有意</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/')">深度学习—— 4 多层感知机</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-weichat.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-weichat.webp" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-alipay.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/qrcode-alipay.webp" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=深度学习—— 4 多层感知机&amp;url=https://blog.keaikeqing.cn/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/&amp;pic=/papercover/DL/DL.webp" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.keaikeqing.cn" target="_blank">可爱可倾</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="/papercover/Python/Python.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/25/DL/3%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/DL/DL.webp" onerror="onerror=null;src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习—— 3 线性神经网络</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/27/DL/5%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/DL/DL.webp" onerror="onerror=null;src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习—— 5 深度学习计算</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-switch"><span class="first-comment">Twikoo</span><span id="switch-btn"></span><span class="second-comment">Giscus</span></div><div class="comment-tips" id="comment-tips"><span>✅ 若未加载出评论区，请刷新页面~</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div id="giscus-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/avatar.ico" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/author_status.webp" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38; margin:0.6rem 0; text-align:center; color:rgba(255, 255, 255, 0.8);">让每一瞬间都散发可爱的光</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">可爱可倾</h1><div class="author-info__desc">花开有期，云舒有意</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/keaikeqing" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/474502279" target="_blank" title="BiliBili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">愿你的世界如花般美好，似云般自由~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">4 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-1"><span class="toc-number">1.1.</span> <span class="toc-text">4.1 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">4.1.1 隐藏层</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">4.1.1.1 线性模型的局限性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9C%A8%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%8A%A0%E5%85%A5%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">4.1.1.2 在网络中加入隐藏层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">4.1.1.3 从线性到非线性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">4.1.1.4 通用近似定理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">4.1.2 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#relu%E5%87%BD%E6%95%B0%E4%BF%AE%E6%AD%A3%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">4.1.2.1 ReLU函数(修正线性单元)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sigmoid%E5%87%BD%E6%95%B0%E6%8C%A4%E5%8E%8B%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">4.1.2.2 sigmoid函数(挤压函数)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tanh%E5%87%BD%E6%95%B0%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">4.1.2.3 tanh函数(双曲正切函数)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.</span> <span class="toc-text">4.2 多层感知机的从零开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.</span> <span class="toc-text">4.3 多层感知机的简洁实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.</span> <span class="toc-text">4.4 模型选择、欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.4.1 训练误差和泛化误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.4.2 模型选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.4.3 欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">4.4.3.1 模型复杂性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">4.4.3.2 数据集大小</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4.4 多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.4.1.</span> <span class="toc-text">4.4.4.1 使用线性函数拟合(欠拟合)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%89%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E5%90%88%E9%80%82"><span class="toc-number">1.4.4.2.</span> <span class="toc-text">4.4.4.2
使用三阶多项式函数拟合(合适)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%AB%98%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.4.3.</span> <span class="toc-text">4.4.4.3
使用高阶多项式函数拟合(过拟合)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">4.5 权重衰减</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E7%BB%B4%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.5.1 高维线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.5.2 从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8D%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">4.5.2.1 不使用正则化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">4.5.2.2 使用权重衰减</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.5.3 简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8D%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%E5%8C%96-1"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">4.5.3.1 不使用正则化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F-1"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">4.5.3.2 使用权重衰减</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">4.6 暂退法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E6%9A%82%E9%80%80%E6%B3%95"><span class="toc-number">1.6.1.</span> <span class="toc-text">4.6.1 实践中的暂退法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">1.6.2.</span> <span class="toc-text">4.6.2 从零开始实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">1.6.3.</span> <span class="toc-text">4.6.3 简洁实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.7.</span> <span class="toc-text">4.7 前向传播、反向传播和计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.7.1.</span> <span class="toc-text">4.7.1 前向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.7.2.</span> <span class="toc-text">4.7.2 前向传播计算图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.7.3.</span> <span class="toc-text">4.7.3 反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E9%9C%80%E6%B1%82"><span class="toc-number">1.7.4.</span> <span class="toc-text">4.7.4 内存需求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">4.8 数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%89%93%E7%A0%B4%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-number">1.8.1.</span> <span class="toc-text">4.8.1
梯度爆炸、梯度消失、打破对称性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-number">1.8.2.</span> <span class="toc-text">4.8.2 初始化参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-number">1.9.</span> <span class="toc-text">4.9 环境与分布偏移</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.9.1.</span> <span class="toc-text">4.9.1 分布偏移的类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB%E7%BA%A0%E6%AD%A3"><span class="toc-number">1.9.2.</span> <span class="toc-text">4.9.2 分布偏移纠正</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-number">1.10.</span> <span class="toc-text">4.10 实战Kaggle比赛：房价预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">1.11.</span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%E5%BD%95a%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%BB%98%E5%9B%BE%E4%BB%A3%E7%A0%81"><span class="toc-number">1.11.1.</span> <span class="toc-text">附录A：激活函数绘图代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%E5%BD%95b%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A"><span class="toc-number">1.11.2.</span> <span class="toc-text">附录B：神经网络反向传播通俗解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%E5%BD%95c%E4%B8%BA%E4%BB%80%E4%B9%88%E5%85%B3%E6%B3%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-number">1.11.3.</span> <span class="toc-text">附录C：为什么关注激活函数的导数？</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/18/Python/logging/" title="Python系列之logging"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/Python/Python.webp" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="Python系列之logging"/></a><div class="content"><a class="title" href="/2025/03/18/Python/logging/" title="Python系列之logging">Python系列之logging</a><time datetime="2025-03-18T11:53:35.000Z" title="发表于 2025-03-18 19:53:35">2025-03-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/03/Web/assets/" title="博客资源托管方案"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/Web/Web.webp" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="博客资源托管方案"/></a><div class="content"><a class="title" href="/2025/03/03/Web/assets/" title="博客资源托管方案">博客资源托管方案</a><time datetime="2025-03-03T02:00:00.000Z" title="发表于 2025-03-03 10:00:00">2025-03-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/26/TechnicalNotes/CDN/" title="内容分发网络 (CDN)"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/TechnicalNotes/CDN.webp" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="内容分发网络 (CDN)"/></a><div class="content"><a class="title" href="/2025/02/26/TechnicalNotes/CDN/" title="内容分发网络 (CDN)">内容分发网络 (CDN)</a><time datetime="2025-02-26T06:09:26.000Z" title="发表于 2025-02-26 14:09:26">2025-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/25/TechnicalNotes/Git/" title="git"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/TechnicalNotes/Git.webp" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="git"/></a><div class="content"><a class="title" href="/2025/02/25/TechnicalNotes/Git/" title="git">git</a><time datetime="2025-02-25T06:50:50.000Z" title="发表于 2025-02-25 14:50:50">2025-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/20/TechnicalNotes/live2d/" title="Live2D Widget使用说明"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/papercover/TechnicalNotes/Live2D.webp" onerror="this.onerror=null;this.src='https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp'" alt="Live2D Widget使用说明"/></a><div class="content"><a class="title" href="/2024/11/20/TechnicalNotes/live2d/" title="Live2D Widget使用说明">Live2D Widget使用说明</a><time datetime="2024-11-20T04:50:50.000Z" title="发表于 2024-11-20 12:50:50">2024-11-20</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:keaikeqing@gmail.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" href="/rss.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/avatar.ico" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/keaikeqing" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/474502279" title="Bilibili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2024 - 2025 By <a class="footer-bar-link" href="/" title="可爱可倾" target="_blank">可爱可倾</a></div></div><div id="footer-type-tips"></div><div class="js-pjax"><script>function subtitleType () {
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      if (true) {
        const from = '出自 ' + data.from
        const sub = []
        sub.unshift(data.hitokoto, from)
        window.typed = new Typed('#footer-type-tips', {
          strings: sub,
          startDelay: 300,
          typeSpeed: 150,
          loop: true,
          backSpeed: 50,
        })
      } else {
        document.getElementById('footer-type-tips').innerHTML = data.hitokoto
      }
    })
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.cbd.int/typed.js@2.1.0/dist/typed.umd.js').then(subtitleType)
  }
} else {
  subtitleType()
}
</script></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="冀ICP备2024090701号">冀ICP备2024090701号</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">9</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" href="https://blog.keaikeqing.cn/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/config_anzhiyu/404.webp&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=8265222461&amp;server=tencent"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><span> 关于</span></a></div></div><span class="sidebar-menu-item-title">标签</span></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8265222461" server="tencent" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.3"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8265222461&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.cbd.int/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.cbd.int/mermaid@10.2.4/dist/mermaid.min.js').then(runMermaid)
  }

  anzhiyu.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.keaikeqing.cn',
      region: 'ap-tianjin',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.44/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.keaikeqing.cn',
      region: 'ap-tianjin',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><script>(()=>{
  const getGiscusTheme = theme => {
    return theme === 'dark' ? 'dark' : 'preferred_color_scheme'
  }

  const loadGiscus = () => {
    const config = Object.assign({
      src: 'https://giscus.app/client.js',
      'data-repo': 'keaikeqing/blogdis',
      'data-repo-id': 'R_kgDONKpBew',
      'data-category-id': 'DIC_kwDONKpBe84Cj_Px',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true
    },{"data-lang":"zh-CN","data-mapping":"pathname","data-category":"Announcements","data-input-position":"top"})

    const ele = document.createElement('script')
    for (let key in config) {
      ele.setAttribute(key, config[key])
    }
    document.getElementById('giscus-wrap').appendChild(ele)
  }

  const changeGiscusTheme = theme => {
    const sendMessage = message => {
      const iframe = document.querySelector('iframe.giscus-frame')
      if (!iframe) return
      iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
    }

    sendMessage({
      setConfig: {
        theme: getGiscusTheme(theme)
      }
    });
  }

  anzhiyu.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if ('Twikoo' === 'Giscus' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo.keaikeqing.cn',
        region: 'ap-tianjin',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.44/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom && ($dom.innerHTML= result)
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "visitor@keaikeqing.cn";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://cdn.cbd.int/qrcodejs@1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script defer src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/js/mathjax_config.js"></script><script type="text/javascript" id="MathJax-script" defer src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/js/lib/MathJax/tex-chtml.js"></script><script defer src="https://gcore.jsdelivr.net/npm/@keaikeqing/blog-static@latest/assets/live2d-widget/autoload.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="#waifu"]):not([href="#waifu-toggle"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>