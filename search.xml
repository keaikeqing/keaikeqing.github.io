<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>KTransformers</title>
      <link href="/2024/08/30/NLP/KTransformers/"/>
      <url>/2024/08/30/NLP/KTransformers/</url>
      
        <content type="html"><![CDATA[<h1 id="ktransformers">KTransformers</h1><p>KTransformers KTransformers 是一个灵活的、以 Python为中心的框架，旨在通过高级内核优化和放置/并行策略增强 transformers</p><p><ahref="https://github.com/kvcache-ai/KTransformers">KTransformers的Github链接</a><a href="https://github.com/deepseek-ai/DeepSeek-V2">DeepSeekV2的Github链接</a> <a href="https://arxiv.org/abs/2405.04434">DeepSeekV2的论文链接</a></p><h2 id="介绍">1. 介绍</h2><p>KTransformers 的核心是一个用户友好的、基于模板的注入框架。</p><figure><img src="./images/InjectStruction.png#60x60"title="图1：KTransformers 架构" alt="KTransformers 架构" /><figcaption aria-hidden="true">KTransformers 架构</figcaption></figure><h2 id="优化技术">2. 优化技术</h2><p>KTransformers采用了多种优化技术，将需要两块80GB显存GPU的 DeepSeek-V2Q4_k_m 模型运行在21GB显存和136GB内存的台式计算机上。</p><h3 id="mla-注意力机制">2.1 MLA 注意力机制</h3><p>DeepSeek V2 的 MLA官方开源明确地解压了MLA的压缩表示，并缓存了解压后的键值对。针对此，KTransformers对DeepSeek V2 的MLA按照原文进行了实现，将解压缩矩阵直接吸收到 q_proj 和out_proj 权重中，减少了 KV缓存大小并增加了该算子的算力强度，从而大大优化了 GPU计算能力的利用率。</p><figure><img src="./images/DeepSeek-on-KTransformers.PNG#100x100"title="图2：实现后的MLA 架构" alt="实现后的MLA 架构" /><figcaption aria-hidden="true">实现后的MLA 架构</figcaption></figure><h4 id="deepseek-v2-的mla">2.1.1 DeepSeek V2 的MLA</h4><p>DeepSeek V2 设计了MLA，它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持有效的推理。</p><figure><img src="./images/deepseekv2.png#80x80" title="图3：DeepSeek MLA 架构"alt="DeepSeek MLA 架构" /><figcaption aria-hidden="true">DeepSeek MLA 架构</figcaption></figure><h5 id="mla的提出">2.1.1.1 MLA的提出</h5><p>MLA的核心是对键和值进行低秩联合压缩，以减少KV缓存:</p><figure><img src="./images/dsattn.png#100x100" title="图4：MLA 架构"alt="MLA 架构" /><figcaption aria-hidden="true">MLA 架构</figcaption></figure><p>Attention的计算公式为<span class="math inline">\(A =\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)</span>，其中<spanclass="math inline">\(Q,K,V\)</span>分别为查询、键、值，<spanclass="math inline">\(d_k\)</span>为键的维度。</p><p>假定输入是<span class="math inline">\(x\)</span>，对<spanclass="math inline">\(x\)</span>做一个低秩转换：</p><p><span class="math display">\[\begin{aligned}    \textcolor{red}{c} &amp;= xW^C \quad W^C \in\mathbb{R}^{d_{\text{model}} \times d_c} \quad x \in \mathbb{R}^{n\times d_{\text{model}}}\end{aligned}\]</span></p><p>接着计算<strong>每一个head</strong>的<spanclass="math inline">\(Q,K,V\)</span>：</p><p><span class="math display">\[\begin{aligned}    Q &amp;= xW^Q \quad W^Q \in \mathbb{R}^{d_{\text{model}} \timesd_k}\\    K &amp;= xW^K = \textcolor{red}{c}W^{K&#39;}  \quad W^K \in\mathbb{R}^{d_{\text{model}} \times d_k} \quad W^{K&#39;} \in\mathbb{R}^{d_{\textcolor{red}{c}} \times d_k}\\    V &amp;= xW^V = \textcolor{red}{c}W^{V&#39;} \quad W^V \in\mathbb{R}^{d_{\text{model}} \times d_v} \quad W^{V&#39;} \in\mathbb{R}^{d_{\textcolor{red}{c}} \times d_v}\\    QK^T &amp;= xW^Q(\textcolor{red}{c}W^{K&#39;})^T =x(W^QW^{K&#39;T})\textcolor{red}{c}^T\end{aligned}\]</span></p><p>此时如果有<spanclass="math inline">\(W^{Q&#39;}=W^QW^{K&#39;T}\)</span>(<strong>会不会有精度损失？</strong>)且 <span class="math inline">\(W^{Q&#39;} \in\mathbb{R}^{d_{\text{model}} \times d_c}\)</span>，则<spanclass="math inline">\(QK^T=xW^{Q&#39;}\textcolor{red}{c}^T\)</span>，在推理过程中不需要再缓存<code>K</code><code>V</code> 矩阵，只需要缓存 <spanclass="math inline">\(\textcolor{red}{c}\)</span> 矩阵，如下：</p><p><span class="math display">\[\begin{aligned}A &amp;= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\&amp;=\text{softmax}(\frac{xW^{Q&#39;}\textcolor{red}{c}^T}{\sqrt{d_k}}){\textcolor{red}{c}W^{V&#39;}}\\\end{aligned}\]</span></p><p><strong><spanclass="math inline">\(\textcolor{red}{注意}\)</span></strong>：这里的<spanclass="math inline">\(\textcolor{red}{c}\)</span>是从<spanclass="math inline">\(x\)</span>得到的，也就是说在每一个Layer，所有的Attention-Head使用的都是一个同样的<spanclass="math inline">\(\textcolor{red}{c}\)</span>，而不是每个Head都使用不同的<code>K</code><code>V</code>矩阵。参数从<span class="math inline">\(2 \timeshead\)</span> 变为了 <spanclass="math inline">\(\textcolor{red}{1}\)</span>。</p><p>从上面的公式可以看出，<strong>MLA实际上是一个MHA，但是KV_Cache占用甚至少于GQA</strong>。再次回顾图3，可以清晰看出区别。</p><p>其中<span class="math inline">\(d_{\textcolor{red}{c}}\)</span>是超参数，<span class="math inline">\(d_{\text{model}}\)</span>是模型维度，<span class="math inline">\(d_k\)</span> 是键的维度，<spanclass="math inline">\(d_v\)</span> 是值的维度。</p><h6id="问题1和gqamha对比不同之处"><strong>问题1：和GQA，MHA对比不同之处</strong></h6><p>如果考虑所有头维度加起来的话:</p><p><span class="math display">\[n_{head} \times d_{k}=\begin{cases}    d_{model} &amp; \text{MHA} \\    (1,d_{model}) &amp; \text{GQA} \\\end{cases}\]</span></p><p><span class="math inline">\(W^Q\)</span>和<spanclass="math inline">\(W^K\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>，<span class="math inline">\(W^V\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_v}\)</span>。</p><p><span class="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{model} \timesd_{\textcolor{red}{c}}}\)</span>，<spanclass="math inline">\(W^{Q&#39;}\)</span>和<spanclass="math inline">\(W^{V&#39;}\)</span>不存在了，<spanclass="math inline">\(W^{O&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\textcolor{red}{c}} \timesd_{model}}\)</span>。DeepSeek V2 选择的是<spanclass="math inline">\(d_{\textcolor{red}{c}} = 4\timesd_k\)</span>。</p><p>所以MLA当<span class="math inline">\(d_{\textcolor{red}{c}} =d_k\)</span>时就是正常的MHA。</p><h6id="问题2和正常的attention对比为什么不进一步cache-x"><strong>问题2：和正常的Attention对比，为什么不进一步cachex</strong></h6><p>正常的Attention计算如下：</p><p><span class="math display">\[\begin{aligned}A &amp;= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\&amp;= \text{softmax}(\frac{xW^QW^{KT}x^T}{\sqrt{d_k}}){xW^V}\end{aligned}\]</span></p><p>和上文介绍的对比，为什么不直接保存<spanclass="math inline">\(x\)</span>，而去保存<spanclass="math inline">\(c\)</span>?</p><ol type="1"><li>保存<spanclass="math inline">\(x\)</span>就是不做KV_Cache的情况：或者说保存<spanclass="math inline">\(x\)</span>，把<spanclass="math inline">\(W^K\)</span>吸收到<spanclass="math inline">\(W^Q\)</span>中这时候的<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_{\text{model}}}\)</span>，而保存<spanclass="math inline">\(c\)</span>的时候<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_c}\)</span>。通常<span class="math inline">\(d_{\text{model}} \ggd_c\)</span>，所以保存<spanclass="math inline">\(x\)</span>的计算量更大。</li><li>保存全量KV_Cache就是通常的KV_Cache的做法</li><li>而保存<spanclass="math inline">\(c\)</span>则是折中：<strong>降低显存和加载代价,而提升计算代价</strong></li></ol><p>这也是整个方法的核心思想。</p><h5 id="不兼容rope">2.1.1.2 不兼容ROPE</h5><p>但是当加入ROPE后就会存在问题：</p><p><span class="math display">\[f_{\{q, k\}}\left(\boldsymbol{x}_{m}, m\right)=\boldsymbol{R}_{\Theta,m}^{d} \boldsymbol{W}_{\{q, k\}} \boldsymbol{x}_{m}\]</span></p><p>其中<span class="math inline">\(\boldsymbol{R}_{\Theta,m}^{d}\)</span> 是旋转矩阵，<spanclass="math inline">\(\boldsymbol{W}_{\{q, k\}}\)</span>是权重矩阵，<span class="math inline">\(\boldsymbol{x}_{m}\)</span>是输入。</p><p><span class="math display">\[\begin{aligned}Q &amp;= xW^QR^Q\\K &amp;= xW^KR^K\\QK^T &amp;= xW^QR^Q(xW^KR^K)^T\\    &amp;= xW^QR^Q(cW^{K&#39;}R^K)^T\\    &amp;= x(W^QR^{Q-K}W^{K&#39;T})c^T\\\end{aligned}\]</span></p><p>此时的<spanclass="math inline">\((W^QR^{Q-K}W^{K&#39;T})\)</span>因为加入了位置信息，所以不再是常数，所以无法像MLA一样进行优化。</p><p>所以DeepSeek V2 采取了一种混合的方法——每个Attention Head的Q、K新增<span class="math inline">\(dr\)</span>个维度用来添加RoPE，其中K新增的维度每个Head共享(注1)：</p><p><span class="math display">\[\begin{aligned}Q &amp;= [xW^{Q&#39;}, xW^{Qr}R^Q] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr})\\K &amp;= [cW^{K&#39;}, xW^{Kr}R^K] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr)}\\\end{aligned}\]</span></p><p>此时的矩阵中<spanclass="math inline">\(xW^{Q&#39;}\)</span>就不携带位置信息，所以可以像MLA一样进行优化。而<span class="math inline">\(xW^{Qr}R^Q\)</span>和<spanclass="math inline">\(xW^{Kr}R^K\)</span>则携带位置信息。</p><p>此时在推理过程KV_Cache中，只需要多缓存一个<spanclass="math inline">\(xW^{Kr}R^K\)</span>(命名为K^r)。 即只需要缓存<spanclass="math inline">\(\textcolor{red}{c}\)</span>和<spanclass="math inline">\(\textcolor{red}{K^r}\)</span>。</p><p>论文中选取的参数如下：</p><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td><span class="math inline">\(d_{\text{model}}\)</span></td><td>5120</td></tr><tr><td><span class="math inline">\(d_k\)</span></td><td>128</td></tr><tr><td><span class="math inline">\(d_c\)</span></td><td>512</td></tr><tr><td><span class="math inline">\(dr\)</span></td><td>64</td></tr></tbody></table><p>注1： 虽然不同的注意力头有各自独立的 <spanclass="math inline">\(k_{head}\)</span>，但这些新增的 <spanclass="math inline">\(dr\)</span>个维度是相同的，所有头在这些维度上使用的是相同的位置编码信息。这与标准多头注意力中的做法一致，在标准多头注意力中，尽管每个头有不同的键向量<spanclass="math inline">\(k_{head}\)</span>,但这些键向量的位置编码信息是相同的，因为每个AttentionHead输入都是一样的。这种共享有助于在不同的头之间保持位置信息的一致性，从而让每个头能够以一致的方式利用位置信息，尽管它们在关注的内容上可能有所不同。</p><h5 id="query-低秩">2.1.1.3. Query 低秩</h5><p>DeepSeek V2 还将 <span class="math inline">\(Q\)</span>也转化为了低秩矩阵，这并不会减少KV_Cache的存储量，文中称可以减少训练期间参数量和相应的梯度。</p><p><span class="math display">\[\begin{aligned}Q &amp;= [c^{&#39;}W^{Q&#39;}, c^{&#39;}W^{Qr}R^Q] \in\mathbb{R}^{d_{model} \times (d_{\text{k}}+dr})\\K &amp;= [cW^{K&#39;}, xW^{Kr}R^K] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr)}\\\end{aligned}\]</span></p><p>其中<spanclass="math inline">\(d_{c^{&#39;}}\)</span>是一个超参数，<spanclass="math inline">\(d_{c^{&#39;}}=1536\)</span>。</p><h4 id="kv_cache和flops的权衡">2.1.2 KV_Cache和FLOPs的权衡</h4><p><strong>减少部分KVCache，增加部分计算量，在保存全量KV_Cache和不保存KV_Cache之间取得了一个折中</strong>。如下：</p><p><strong>1. 训练阶段</strong> MLA基本相当于把MHA的每个Attention Head中的<span class="math inline">\(Q,K\)</span>维度从<spanclass="math inline">\(d_k\)</span>变为了<spanclass="math inline">\(d_c+dr\)</span>。</p><p><strong>2. 推理阶段</strong></p><p>由于<span class="math inline">\(d_{\textcolor{red}{c}} = 4\timesd_k\)</span>，实际上MLA在推理阶段做的这个转换，虽然能有效减少KVCache，但其推理的计算量是增加的。</p><h5 id="kv_cache">2.1.2.1 KV_Cache</h5><table><thead><tr><th style="text-align: center;">Attention Mechanism</th><th style="text-align: center;">KV_Cache per token</th><th style="text-align: center;">Capability</th></tr></thead><tbody><tr><td style="text-align: center;">MHA</td><td style="text-align: center;"><spanclass="math inline">\(2n_hd_hl\)</span></td><td style="text-align: center;">Strong</td></tr><tr><td style="text-align: center;">GQA</td><td style="text-align: center;"><spanclass="math inline">\(2n_gd_hl\)</span></td><td style="text-align: center;">Moderate</td></tr><tr><td style="text-align: center;">MQA</td><td style="text-align: center;"><spanclass="math inline">\(2d_hl\)</span></td><td style="text-align: center;">Weak</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">MLA</td><td style="text-align: center;"><span class="math inline">\((d_c+d_r)l\approx 4.5d_hl\)</span></td><td style="text-align: center;">Strong</td></tr></tbody></table><p>其中<span class="math inline">\(d_h\)</span>是head维度，<spanclass="math inline">\(n_h\)</span>是head数，<spanclass="math inline">\(n_g\)</span>是gqa head数。</p><p>所以大概等效于<spanclass="math inline">\(n_g=2.25\)</span>的GQA的KV_Cache，相比于8个head的GQA大概降低到了原来的0.28倍。</p><h5 id="flops">2.1.2.2 FLOPs</h5><p>因为正常来讲<span class="math inline">\(W^Q\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>，<span class="math inline">\(W^O\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_v \timesd_{\text{model}}}\)</span>。而在MLA中<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_c}\)</span>，<spanclass="math inline">\(W^{O&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_c \timesd_{\text{model}}}\)</span>。所以在推理阶段，MLA的计算量是增加的。</p><p><strong>具体增加了多少</strong></p><p>假设输入的batch为<span class="math inline">\(b \times s \timesd\)</span>，其中<spanclass="math inline">\(b\)</span>为batch_size，<spanclass="math inline">\(s\)</span>为输入长度，<spanclass="math inline">\(d\)</span>为嵌入维度</p><ol type="1"><li>Embedding层：查表操作，不涉及矩阵乘法，故不计入FLOPs</li><li>预测多分类头(logits)将尺寸为 d 的隐藏向量映射为词表大小：<spanclass="math inline">\([b \times s \times d] \times [d \times V] = [b\times s \times V]\)</span> —— <spanclass="math inline">\(2bsVd\)</span></li><li>Self-Attention层： <span class="math display">\[     Q&#39;=xW^{Q&#39;}\\     Attention =\text{softmax}(\frac{Q&#39;\textcolor{red}{c&#39;}^T}{\sqrt{d_k}}){\textcolor{red}{c}}\\     x_{out}= AW^{O&#39;}+x\]</span><ul><li>每个头的注意力计算：一下需要乘<spanclass="math inline">\(n_{heads}\)</span>次，每次计算如下：<ul><li>计算 Q’ 矩阵：<span class="math inline">\([b \times s \times d]\times [d \times d_{c&#39;}] = [b \times s \times d_{c&#39;}]\)</span>—— <span class="math inline">\(2bsdd_{c&#39;}\)</span></li><li>计算注意力权重<spanclass="math inline">\(Q&#39;{c&#39;}^T\)</span>：<spanclass="math inline">\([b \times s \times d_{c&#39;}] \times [b \times s\times d_{c&#39;}]^T = [b \times s \times s]\)</span> —— <spanclass="math inline">\(2bs^2d_{c&#39;}\)</span></li><li>汇聚价值信息<span class="math inline">\(Ac\)</span>: <spanclass="math inline">\([b \times s \times s] \times [b \times s \timesd_c] = [b \times s \times d_c]\)</span> —— <spanclass="math inline">\(2bs^2d_c\)</span></li></ul></li><li>拼接多头注意力: 不涉及矩阵乘法，故不计入FLOPs</li><li>输出矩阵<span class="math inline">\(O\)</span>: <spanclass="math inline">\([b \times s \times (n_{heads}\times d_c)] \times[(n_{heads}\times d_c) \times d] = [b \times s \times d]\)</span> ——<span class="math inline">\(2bsd(n_{heads}\times d_c)\)</span></li></ul></li><li>MLP层：<span class="math inline">\(16bsd^2\)</span> <spanclass="math display">\[     h=Relu(x_{out}W^1+b_1)\\     h_{out}=hW^2+b2\\     x=h_{out}+x_{out}\]</span><ul><li>第一个线性层：<span class="math inline">\([b \times s \times d]\times [d \times 4d] = [b \times s \times 4d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li><li>第二个线性层：<span class="math inline">\([b \times s \times 4d]\times [4d \times d] = [b \times s \times d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li></ul></li></ol><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单次FLOPs</th><th style="text-align: center;">总FLOPs</th></tr></thead><tbody><tr><td style="text-align: center;">Embedding</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr><td style="text-align: center;">LM Head(logits)</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td></tr><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(2bsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2blsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(16bsd^2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(16blsd^2\)</span></td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">正常的MHA FLOPs</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;"><span class="math inline">\(2bsVd +24blsd^2 + 4bls^2d\)</span></td></tr></tbody></table><p>其中<span class="math inline">\(l\)</span>为层数，<spanclass="math inline">\(d\)</span>为嵌入维度，<spanclass="math inline">\(s\)</span>为输入长度，<spanclass="math inline">\(V\)</span>为词表大小，<spanclass="math inline">\(b\)</span>为batch_size，<spanclass="math inline">\(n_{heads}\)</span>为head数。</p><p><span class="math display">\[\begin{aligned}FLOPs(MLA/MHA)&amp;\approx\frac{16blsd^2+2blsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)}{24blsd^2 +4bls^2d}\\&amp;= \frac{33d+17s}{24d+4s}(\textcolor{red}{存疑})\\\end{aligned}\]</span></p><h4 id="deepseek-v2-的moe">2.1.3 DeepSeek V2 的MOE</h4><p>另外 DeepSeek V2 是一个改进的MOE模型，部分改动如下：</p><ol type="1"><li>细粒度专家分割：通过将每个FFN专家进一步细分，这允许模型在保持参数总数不变的情况下，激活更多的、更细粒度的专家。这种策略使得各个专家能够专注于更细致的知识领域，提高了专家的专业化程度。</li><li>共享专家隔离：设置一部分专家作为“共享专家”，这些专家总是被激活，用于捕捉和整合常见的跨上下文知识。这样可以减少路由专家之间的知识冗余，避免重复知识，每个路由专家可以更专注于独特的知识领域。</li></ol><h3 id="改进的量化内核">2.2 改进的量化内核</h3><p>Transformers 的 Torch实现必须在处理之前将这些张量反量化为支持的数据类型，这会带来不必要的计算开销并增加内存流量。为了克服这个问题，我们加入了直接对量化数据类型进行操作的高级内核，从而优化了推理性能。</p><h3 id="计算强度引导的模型加载">2.3 计算强度引导的模型加载</h3><p>遵循按算术强度排列所有算子并尽可能将最密集的算子放在 GPU中的原则，策略性地只将计算最密集的参数存储在 GPU 上。</p><p>优先将 MoE 参数和词嵌入计算放在 CPU端以利用其更大的内存容量。其余参数（包括共享专家、注意模块中的投影和MLA）存储在 GPU VRAM 中。由于每个 token都会访问这些参数，因此将它们放置在 GPU上可以最大限度地发挥高内存带宽的优势。如果使用 Q4_K_M版本，此配置将导致大约 20.7 GB 的 VRAM 使用量和 136GB 的 DRAM内存请求，即使在本地桌面上也是可行的。</p><ol type="1"><li>MLA 运算符（包含 128 个头，具有共享的压缩键值表示）的算术强度为512。这使其成为最密集的运算符，尤其是在较小的推理批次大小期间。因此，它被分配给GPU 以利用张量核心。</li><li>DeepSeek-V2 中的每个转换器块包含 160 位混合专家 (MoE)专家，占总参数的 96%。但是，MoE 路由器为每个 token 仅激活这 160位专家中的 6 位。因此，此操作主要涉及批量通用矩阵向量乘法 (GEMV)，可以由CPU 高效处理。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><ahref="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA</a></li><li>知乎：如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2<ol type="1"><li><ahref="https://www.zhihu.com/question/655172528/answer/3491439374">回答1</a></li><li><ahref="https://www.zhihu.com/question/655172528/answer/3495218670">回答2</a></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
