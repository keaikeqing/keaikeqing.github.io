<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python系列之logging</title>
      <link href="/2025/03/18/Python/logging/"/>
      <url>/2025/03/18/Python/logging/</url>
      
        <content type="html"><![CDATA[<h1 id="logging">logging</h1><p>Python的logging模块提供了灵活的日志记录功能，可用于调试和记录程序运行信息。</p><h2 id="基本配置">1 基本配置</h2><p>将以下代码添加到Python文件中，以配置日志记录：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> logging<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path<span class="token comment"># 配置日志文件路径</span>script_dir <span class="token operator">=</span> Path<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">.</span>parent  <span class="token comment"># 获取当前脚本所在目录</span>log_file_path <span class="token operator">=</span> script_dir <span class="token operator">/</span> <span class="token string">'log_file.log'</span>  <span class="token comment"># 在脚本目录下创建日志文件</span><span class="token comment"># 配置日志</span>logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>    level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span>  <span class="token comment"># 设置日志级别</span>    <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">'%(asctime)s - %(levelname)s - %(message)s'</span><span class="token punctuation">,</span>  <span class="token comment"># 设置日志格式</span>    handlers<span class="token operator">=</span><span class="token punctuation">[</span>        logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 输出到控制台</span>        logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span>log_file_path<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>  <span class="token comment"># 同时输出到文件</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>  <span class="token comment"># 获取当前模块的logger</span><span class="token comment"># 使用示例：不同级别的日志记录</span>logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">"这是调试信息"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"这是一般信息"</span><span class="token punctuation">)</span> logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">"这是警告信息"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">"这是错误信息"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>critical<span class="token punctuation">(</span><span class="token string">"这是严重错误信息"</span><span class="token punctuation">)</span></code></pre><h2 id="高级配置">2 高级配置</h2><p>以下是一个更复杂的日志配置示例，作为模块使用。该示例展示了如何创建一个通用的日志配置类，支持多模块共享日志设置、按大小轮转、压缩旧日志、不同级别日志分文件、控制台输出和自定义过滤器。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> logging<span class="token keyword">import</span> os<span class="token keyword">import</span> gzip<span class="token keyword">from</span> logging<span class="token punctuation">.</span>handlers <span class="token keyword">import</span> RotatingFileHandler<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token punctuation">,</span> Dict<span class="token punctuation">,</span> List<span class="token keyword">import</span> shutil<span class="token keyword">class</span> <span class="token class-name">CompressedRotatingFileHandler</span><span class="token punctuation">(</span>RotatingFileHandler<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""支持压缩的日志轮转处理器"""</span>    <span class="token keyword">def</span> <span class="token function">doRollover</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""重写doRollover方法来添加压缩功能"""</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stream<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>stream<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>stream <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token comment"># 如果备份文件已存在，需要先轮转这些文件</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>backupCount <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token comment"># 删除最旧的一个文件(如果存在)</span>            oldest_backup <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">&#125;</span></span><span class="token string">.</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>backupCount<span class="token punctuation">&#125;</span></span><span class="token string">.gz"</span></span>            <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>oldest_backup<span class="token punctuation">)</span><span class="token punctuation">:</span>                os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>oldest_backup<span class="token punctuation">)</span>            <span class="token comment"># 轮转现有的备份文件</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>backupCount <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                source <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">&#125;</span></span><span class="token string">.</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">.gz"</span></span>                dest <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">&#125;</span></span><span class="token string">.</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">.gz"</span></span>                <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>source<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>dest<span class="token punctuation">)</span><span class="token punctuation">:</span>                        os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>dest<span class="token punctuation">)</span>                    os<span class="token punctuation">.</span>rename<span class="token punctuation">(</span>source<span class="token punctuation">,</span> dest<span class="token punctuation">)</span>            <span class="token comment"># 压缩当前日志为第一个备份</span>            dest <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">&#125;</span></span><span class="token string">.1.gz"</span></span>            <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f_in<span class="token punctuation">:</span>                    <span class="token keyword">with</span> gzip<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>dest<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f_out<span class="token punctuation">:</span>                        shutil<span class="token punctuation">.</span>copyfileobj<span class="token punctuation">(</span>f_in<span class="token punctuation">,</span> f_out<span class="token punctuation">)</span>                os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>self<span class="token punctuation">.</span>baseFilename<span class="token punctuation">)</span>        <span class="token comment"># 创建新的空日志文件</span>        self<span class="token punctuation">.</span>mode <span class="token operator">=</span> <span class="token string">'w'</span>        self<span class="token punctuation">.</span>stream <span class="token operator">=</span> self<span class="token punctuation">.</span>_open<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">KeywordFilter</span><span class="token punctuation">(</span>logging<span class="token punctuation">.</span>Filter<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> keyword<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>keyword <span class="token operator">=</span> keyword    <span class="token keyword">def</span> <span class="token function">filter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> record<span class="token punctuation">:</span> logging<span class="token punctuation">.</span>LogRecord<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">bool</span><span class="token punctuation">:</span>        <span class="token comment"># 仅当日志消息包含特定关键词时返回 True</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>keyword <span class="token keyword">in</span> record<span class="token punctuation">.</span>msg<span class="token keyword">class</span> <span class="token class-name">LoggerConfig</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    通用日志配置类，支持多模块共享日志设置    特性：    1. 按大小轮转    2. 日志压缩    3. 不同级别日志分文件    4. 支持控制台输出    5. 支持自定义过滤器    """</span>    _instance <span class="token operator">=</span> <span class="token boolean">None</span>    _initialized <span class="token operator">=</span> <span class="token boolean">False</span>    _loggers<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> logging<span class="token punctuation">.</span>Logger<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> cls<span class="token punctuation">.</span>_instance <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            cls<span class="token punctuation">.</span>_instance <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>LoggerConfig<span class="token punctuation">,</span> cls<span class="token punctuation">)</span><span class="token punctuation">.</span>__new__<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>        <span class="token keyword">return</span> cls<span class="token punctuation">.</span>_instance    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 log_dir<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'./logs'</span><span class="token punctuation">,</span>                 log_level<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> logging<span class="token punctuation">.</span>INFO<span class="token punctuation">,</span>                 max_bytes<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">,</span>  <span class="token comment"># 默认5MB</span>                 backup_count<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span>                 enable_console<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                 compress_logs<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                 custom_filters<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>                 extra_handlers<span class="token punctuation">:</span> List<span class="token punctuation">[</span>logging<span class="token punctuation">.</span>Handler<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        初始化日志配置        Args:            log_dir: 日志目录            log_level: 基础日志级别            max_bytes: 单个日志文件最大字节数            backup_count: 保留的备份文件数量            enable_console: 是否启用控制台输出            compress_logs: 是否压缩旧日志            custom_filters: 自定义过滤器列表            extra_handlers: 额外的处理器列表        """</span>        <span class="token comment"># 单例模式：确保只初始化一次</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_initialized<span class="token punctuation">:</span>            <span class="token keyword">return</span>        self<span class="token punctuation">.</span>log_dir <span class="token operator">=</span> log_dir        self<span class="token punctuation">.</span>log_level <span class="token operator">=</span> log_level        self<span class="token punctuation">.</span>max_bytes <span class="token operator">=</span> max_bytes        self<span class="token punctuation">.</span>backup_count <span class="token operator">=</span> backup_count        self<span class="token punctuation">.</span>enable_console <span class="token operator">=</span> enable_console        self<span class="token punctuation">.</span>compress_logs <span class="token operator">=</span> compress_logs        self<span class="token punctuation">.</span>custom_filters <span class="token operator">=</span> custom_filters <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>extra_handlers <span class="token operator">=</span> extra_handlers <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment"># 创建日志目录</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>log_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment"># 默认格式化器</span>        self<span class="token punctuation">.</span>default_formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span>            <span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_initialized <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">def</span> <span class="token function">get_logger</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> formatter<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> logging<span class="token punctuation">.</span>Logger<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        获取或创建logger        Args:            name: logger名称            formatter: 自定义格式化器        Returns:            logging.Logger: 配置好的logger        """</span>        <span class="token comment"># 如果已存在该logger，直接返回</span>        <span class="token keyword">if</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>_loggers<span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>_loggers<span class="token punctuation">[</span>name<span class="token punctuation">]</span>        <span class="token comment"># 创建logger</span>        logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token comment"># 清理可能存在的handlers</span>        <span class="token keyword">if</span> logger<span class="token punctuation">.</span>hasHandlers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>handlers<span class="token punctuation">.</span>clear<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 设置日志级别</span>        logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>log_level<span class="token punctuation">)</span>        formatter <span class="token operator">=</span> formatter <span class="token keyword">or</span> self<span class="token punctuation">.</span>default_formatter        <span class="token comment"># 添加控制台处理器</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>enable_console<span class="token punctuation">:</span>            console_handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span>            console_handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>            logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>console_handler<span class="token punctuation">)</span>        <span class="token comment"># 为不同级别创建单独的日志文件</span>        log_levels <span class="token operator">=</span> <span class="token punctuation">[</span>            <span class="token punctuation">(</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span> <span class="token string">'debug'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">,</span> <span class="token string">'info'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span>logging<span class="token punctuation">.</span>WARNING<span class="token punctuation">,</span> <span class="token string">'warning'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span>logging<span class="token punctuation">.</span>ERROR<span class="token punctuation">,</span> <span class="token string">'error'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span>logging<span class="token punctuation">.</span>CRITICAL<span class="token punctuation">,</span> <span class="token string">'critical'</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span>        <span class="token keyword">for</span> level<span class="token punctuation">,</span> level_name <span class="token keyword">in</span> log_levels<span class="token punctuation">:</span>            <span class="token keyword">if</span> level <span class="token operator">>=</span> self<span class="token punctuation">.</span>log_level<span class="token punctuation">:</span>                handler <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_rotating_handler<span class="token punctuation">(</span>                    os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>log_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"LOG_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>level_name<span class="token punctuation">&#125;</span></span><span class="token string">.log"</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    formatter<span class="token punctuation">,</span>                    level                <span class="token punctuation">)</span>                logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span>        <span class="token comment"># 添加额外的处理器</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>extra_handlers<span class="token punctuation">:</span>            <span class="token keyword">for</span> handler <span class="token keyword">in</span> self<span class="token punctuation">.</span>extra_handlers<span class="token punctuation">:</span>                handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>                logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span>        <span class="token comment"># 添加自定义过滤器</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>custom_filters<span class="token punctuation">:</span>            <span class="token keyword">for</span> filter_config <span class="token keyword">in</span> self<span class="token punctuation">.</span>custom_filters<span class="token punctuation">:</span>                filter_class <span class="token operator">=</span> filter_config<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'filter_class'</span><span class="token punctuation">)</span>                args <span class="token operator">=</span> filter_config<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'args'</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> filter_class<span class="token punctuation">:</span>                    filter_instance <span class="token operator">=</span> filter_class<span class="token punctuation">(</span><span class="token operator">**</span>args<span class="token punctuation">)</span>                    logger<span class="token punctuation">.</span>addFilter<span class="token punctuation">(</span>filter_instance<span class="token punctuation">)</span>        <span class="token comment"># 禁止向父logger传递日志</span>        logger<span class="token punctuation">.</span>propagate <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token comment"># 缓存logger实例</span>        self<span class="token punctuation">.</span>_loggers<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> logger        <span class="token keyword">return</span> logger    <span class="token keyword">def</span> <span class="token function">_create_rotating_handler</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                                 filename<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>                                 formatter<span class="token punctuation">:</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">,</span>                                 level<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> RotatingFileHandler<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        创建轮转处理器        Args:            filename: 日志文件名            formatter: 格式化器            level: 日志级别        Returns:            RotatingFileHandler: 配置好的处理器        """</span>        handler_class <span class="token operator">=</span> CompressedRotatingFileHandler <span class="token keyword">if</span> self<span class="token punctuation">.</span>compress_logs <span class="token keyword">else</span> RotatingFileHandler        handler <span class="token operator">=</span> handler_class<span class="token punctuation">(</span>            filename<span class="token punctuation">,</span>            maxBytes<span class="token operator">=</span>self<span class="token punctuation">.</span>max_bytes<span class="token punctuation">,</span>            backupCount<span class="token operator">=</span>self<span class="token punctuation">.</span>backup_count<span class="token punctuation">,</span>            encoding<span class="token operator">=</span><span class="token string">'utf-8'</span>        <span class="token punctuation">)</span>        handler<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token punctuation">)</span>        handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>        <span class="token keyword">return</span> handler<span class="token keyword">def</span> <span class="token function">setLogConfig</span><span class="token punctuation">(</span>module_name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 log_dir<span class="token operator">=</span><span class="token string">'./logs'</span><span class="token punctuation">,</span>                 log_level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span>                 max_bytes<span class="token operator">=</span><span class="token number">5</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">,</span>  <span class="token comment"># 5MB</span>                 backup_count<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>                 enable_console<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                 compress_logs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                 custom_filters<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 extra_handlers<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    快速设置并获取logger的便捷函数    Args:        module_name: 模块名称，用于标识日志来源        log_dir: 日志保存目录        log_level: 日志级别        max_bytes: 单个日志文件最大字节数        backup_count: 保留的备份文件数量        enable_console: 是否启用控制台输出        compress_logs: 是否压缩旧日志        custom_filters: 自定义过滤器列表，例如custom_filters=[&#123;'filter_class': KeywordFilter, 'args': &#123;'keyword': 'critical'&#125;&#125;]        extra_handlers: 额外的处理器列表    Returns:        logging.Logger: 配置好的logger实例    """</span>    <span class="token comment"># 初始化日志配置</span>    log_config <span class="token operator">=</span> LoggerConfig<span class="token punctuation">(</span>        log_dir<span class="token operator">=</span>log_dir<span class="token punctuation">,</span>        log_level<span class="token operator">=</span>log_level<span class="token punctuation">,</span>        max_bytes<span class="token operator">=</span>max_bytes<span class="token punctuation">,</span>        backup_count<span class="token operator">=</span>backup_count<span class="token punctuation">,</span>        enable_console<span class="token operator">=</span>enable_console<span class="token punctuation">,</span>        compress_logs<span class="token operator">=</span>compress_logs<span class="token punctuation">,</span>        custom_filters<span class="token operator">=</span>custom_filters<span class="token punctuation">,</span>        extra_handlers<span class="token operator">=</span>extra_handlers    <span class="token punctuation">)</span>    <span class="token comment"># 自动获取调用者的模块名称</span>    <span class="token keyword">if</span> module_name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        <span class="token keyword">import</span> inspect        caller_frame <span class="token operator">=</span> inspect<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        module_name <span class="token operator">=</span> caller_frame<span class="token punctuation">.</span>frame<span class="token punctuation">.</span>f_globals<span class="token punctuation">[</span><span class="token string">'__name__'</span><span class="token punctuation">]</span>        <span class="token comment"># 如果没有提供module_name，则通过调用栈获取调用者的__name__</span>    <span class="token keyword">return</span> log_config<span class="token punctuation">.</span>get_logger<span class="token punctuation">(</span>name<span class="token operator">=</span>module_name<span class="token punctuation">,</span> formatter<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token comment"># 使用示例</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    logger <span class="token operator">=</span> setLogConfig<span class="token punctuation">(</span><span class="token string">'test_module'</span><span class="token punctuation">)</span>    <span class="token comment"># 使用logger</span>    logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'这是一条调试日志'</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'这是一条信息日志'</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">'这是一条警告日志'</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">'这是一条错误日志'</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>critical<span class="token punctuation">(</span><span class="token string">'这是一条严重错误日志'</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>博客资源托管方案</title>
      <link href="/2025/03/03/Web/assets/"/>
      <url>/2025/03/03/Web/assets/</url>
      
        <content type="html"><![CDATA[<h1 id="博客资源托管方案">博客资源托管方案</h1><h2 id="资源托管方案">资源托管方案</h2><p>在搭建博客时，通常会涉及到一些静态资源的托管问题，比如图片、视频、音频等。这里列出几种常见的资源托管方案，供参考：</p><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr><th>标号</th><th>方案</th><th>具体方法</th><th>推荐指数</th></tr></thead><tbody><tr><td>1</td><td>本地</td><td>放在博客source文件夹下，使用相对路径，跟随博客同步到github仓库</td><td>★★</td></tr><tr><td>2</td><td>github repo</td><td>将assets资源放在单独的github仓库管理，并打tag</td><td>★★★</td></tr><tr><td>3</td><td>云服务器</td><td>将assets放在云服务器静态网页上</td><td>★★★★</td></tr><tr><td>4</td><td>npm</td><td>将assets发布到npm，经过 unpkg/jsDelivr CDN使用</td><td>★★★★</td></tr></tbody></table><h2 id="优缺点分析">优缺点分析</h2><h3 id="本地">1 本地</h3><ul><li>优点：<ul><li>方便，直接放在博客的source文件夹下，使用相对路径引用即可。</li><li>不需要额外的配置和管理。</li><li>跟随博客同步到github仓库，方便备份和版本控制。</li></ul></li><li>缺点：<ul><li>资源文件较大时，可能会导致博客仓库过大，影响克隆和下载速度。</li><li>仓库容量有限，超过限制后也难以清理。</li></ul></li></ul><h3 id="github-repo">2 github repo</h3><p>一般配合jsDelivr使用，cdn加速。</p><ul><li>优点：<ul><li>可以将assets资源放在单独的github仓库管理，避免影响博客仓库的大小。</li><li>可以使用github的版本控制和备份功能，方便管理和维护。</li></ul></li><li>缺点：<ul><li>需要额外的配置和管理，增加了复杂性。</li><li>并不推荐github做资源托管，限制大文件和高流量访问，尤其是直接调用raw.githubusercontent.com的资源。</li></ul></li></ul><h3 id="云服务器">3 云服务器</h3><ul><li>优点：<ul><li>可以将assets放在云服务器静态网页上，方便管理和维护。</li><li>资源完全自由掌控，可以设置访问权限和流量限制，隐私性最好。</li><li>容量和带宽取决于云服务器的配置。</li></ul></li><li>缺点：<ul><li>需要额外的配置和管理，增加了复杂性。</li><li>需要支付云服务器的费用，增加了成本。</li></ul></li></ul><h3 id="npm">4 npm</h3><ul><li>优点：<ul><li>可以将assets发布到npm，经过 unpkg/jsDelivr CDN使用，方便快捷。</li><li>可以使用npm的版本控制和备份功能，方便管理和维护。</li></ul></li><li>缺点：<ul><li>需要额外的配置和管理，增加了复杂性。</li><li>资源更新后需要重新发布到npm</li><li>包的容量有限制</li></ul></li></ul><h3 id="总结">总结</h3><p>将资源分开管理</p><ol type="1"><li>将配置文件放在source文件夹下，使用相对路径引用。</li><li>将不常变动的资源打包为assets，依据喜好放于云服务器或者npm(使用jsDelivr/unpkgCDN)。</li><li>经常变动的资源如果较大，放在云服务器或者githubrepo(使用jsDelivr)上，如果较小，放在source文件夹下。</li><li>十分推荐为博客(和云服务器，如果使用的话)添加CloudflareCDN加速，免费版足够使用。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>内容分发网络 (CDN)</title>
      <link href="/2025/02/26/TechnicalNotes/CDN/"/>
      <url>/2025/02/26/TechnicalNotes/CDN/</url>
      
        <content type="html"><![CDATA[<h2 id="内容分发网络-cdn">内容分发网络 (CDN)</h2><p>CDN，全称 Content DeliveryNetwork（内容分发网络），是一种通过分布式部署的服务器网络来加速和优化内容传输的技术。它的核心目标是让用户能够就近获取所需的网页、视频、图片、脚本等内容，从而提升访问速度和体验，同时减轻源站服务器的压力。</p><h3 id="原理">1 原理</h3><ol type="1"><li>内容缓存与分发 (Caching &amp; Distribution):这是CDN最基础也是最关键的技术。CDN会将网站的静态内容（如图片、视频、CSS/JS文件等）主动或被动地从源站服务器缓存到遍布全球的边缘节点服务器上。当用户发起访问请求时，请求会被引导至离用户地理位置最近、网络延迟最低的边缘节点，该节点会直接将缓存的内容响应给用户，避免了对源站的直接访问，极大地缩短了物理距离和数据传输时间。</li><li>负载均衡与智能调度 (Load Balancing &amp; Intelligent Scheduling):CDN拥有一个智能的“大脑”——全局负载均衡（GSLB）系统。这个系统会实时监控所有边缘节点的健康状况、负载情况以及到用户的网络链路质量。当用户发起请求时，GSLB会综合分析用户的IP地址、地理位置、运营商网络等信息，通过DNS解析或HTTP重定向等技术，将用户的请求“调度”到最优的边缘节点上，确保用户获得最快、最稳定的访问体验。</li></ol><h3 id="工作流程">2 工作流程</h3><p>用户使用CDN服务访问一个网站的完整流程，可以分解为以下几个关键步骤：</p><p><strong>前提：</strong>网站主已经将其域名接入CDN服务，并在DNS服务商处将域名解析指向CDN服务商提供的CNAME记录。</p><ol type="1"><li><strong>用户发起请求：</strong> 当用户在浏览器中输入网址（例如<code>www.example.com</code>）后，计算机会向本地DNS服务器（LocalDNS）发起域名解析请求。</li><li><strong>CNAME解析至CDN的DNS调度系统：</strong> LocalDNS从域名的权威DNS服务器获取到该域名被配置了一条CNAME记录，指向了CDN服务商的域名（例如<code>www.example.com.cdn.cloudflare.net</code>）。于是，LocalDNS会再次向CDN的DNS调度系统发起请求。</li><li><strong>智能调度，返回最优节点IP：</strong>CDN的DNS调度系统（即GSLB）接收到请求后，会根据一系列策略进行智能决策：<ul><li><strong>地理位置判断：</strong>分析用户的IP地址，判断其所在的地理位置和运营商网络。</li><li><strong>节点健康度检测：</strong>排除掉故障或负载过高的边缘节点。</li><li><strong>网络质量探测：</strong>评估各个节点到用户的网络延迟和丢包率。</li><li>综合以上信息，选择出一个对该用户来说“最优”的边缘节点，并将其IP地址返回给LocalDNS。</li></ul></li><li><strong>获取最优节点IP：</strong> LocalDNS将获取到的最优边缘节点的IP地址返回给用户的计算机。</li><li><strong>用户与边缘节点建立连接：</strong>用户的浏览器向这个最优的边缘节点IP地址发起HTTP/HTTPS请求，请求获取具体的网页内容（如一张图片）。</li><li><strong>边缘节点响应：</strong><ul><li><strong>缓存命中（Cache Hit）：</strong>如果该边缘节点已经缓存了用户请求的内容，并且缓存尚未过期，它会直接将内容发送给用户。这是最理想、最快速的情况。</li><li><strong>缓存未命中（Cache Miss）：</strong>如果该节点没有缓存该内容，或者缓存已经过期，它会代表用户向源站服务器发起请求，获取最新的内容。</li></ul></li><li><strong>回源与缓存：</strong>边缘节点从源站获取到内容后，一方面将其发送给用户，另一方面会按照预设的缓存策略（如缓存时间、缓存规则等）将内容存储在本地，以便后续有相同请求时可以直接响应。</li></ol><p>通过这样一套完整、自动化的流程，CDN成功地将用户访问流量分散到了各个边缘节点，既减轻了源站服务器的压力，又显著提升了终端用户的访问速度和体验。</p><h3 id="优势">3 优势</h3><p>凭借其独特的技术原理，CDN被广泛应用于各种互联网业务场景，其主要优势包括：</p><ul><li><strong>加速网站访问：</strong>显著减少网页加载时间，提升用户体验，降低用户流失率。</li><li><strong>分担源站压力：</strong>大部分用户请求由CDN边缘节点处理，大幅降低源站服务器的负载和带宽消耗成本。</li><li><strong>提升可用性与可靠性：</strong>单个或多个节点的故障不会影响整个服务的可用性，CDN的分布式架构提供了天然的冗余。</li><li><strong>增强安全性：</strong>CDN可以隐藏源站IP，并提供DDoS攻击防护、WAF（Web应用防火墙）等安全功能，保护源站免受网络攻击。</li><li><strong>支持大规模分发：</strong>尤其适用于视频点播、直播、大文件下载、游戏加速等需要处理海量并发请求和高带宽流量的场景。</li></ul><p>CDN已经成为现代互联网不可或缺的基础设施，它通过智能的调度和分布式的缓存，有效地解决了因地理距离和网络拥塞带来的访问延迟问题，为全球用户提供了更快速、更可靠、更安全的网络访问体验。</p><h3 id="使用">4 使用</h3><p>常见的 CDN 服务有 <strong>cdnjs</strong>、<strong>unpkg</strong> 和<strong>jsDelivr</strong>，它们各自有不同的特点和功能。</p><ol type="1"><li>unpkg类型：只支持 npm 包，npm 包加速<ul><li>提供方<ul><li>unpkg官方: <code>https://unpkg.com</code></li></ul></li><li>示例:<code>{url}/@{organization}/{package-name}@{version}/path/to/file</code></li></ul></li><li>jsdelivr类型：支持 npm 包 和 GitHub 仓库，npm 包 和 GitHub 仓库加速<ul><li>提供方<ul><li>jsdelivr官方: <code>https://cdn.jsdelivr.net</code></li><li>fastly: <code>https://fastly.jsdelivr.net</code></li><li>cloudflare: <code>https://testingcf.jsdelivr.net</code></li><li>gcore: <code>gcore.jsdelivr.net</code></li></ul></li><li>npm:<code>{url}/npm/@{organization}/{package-name}@{version}/path/to/file</code></li><li>GitHub:<code>{url}/gh/{organization}/{repository}@{version}/path/to/file</code></li></ul></li><li>cdnjs类型：只支持已加入的开源库，JavaScript/CSS 库加速<ul><li>基础URL: <code>https://cdnjs.cloudflare.com/ajax/libs/</code></li><li>示例:<code>https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js</code></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>git</title>
      <link href="/2025/02/25/TechnicalNotes/Git/"/>
      <url>/2025/02/25/TechnicalNotes/Git/</url>
      
        <content type="html"><![CDATA[<h2 id="gitignore">1. gitignore</h2><p><code>.gitignore</code> 文件用于指定哪些文件或目录在 Git版本控制中被忽略。它可以避免将不必要的文件（如编译产物、临时文件等）提交到版本库中。</p><pre class="language-ignore" data-language="ignore"><code class="language-ignore"><span class="token comment"># 忽略特定文件</span><span class="token entry string">secret.txt</span><span class="token comment"># 忽略特定目录</span><span class="token entry string">temp<span class="token punctuation">/</span></span><span class="token comment"># 忽略特定目录下的所有文件(但保留temp目录本身)</span><span class="token comment"># 如果 temp/ 目录是空的，Git 默认不会追踪空目录，通常要用 .gitkeep 之类的文件强制保留</span><span class="token entry string">temp<span class="token punctuation">/</span><span class="token operator">*</span></span><span class="token comment"># 忽略特定目录下的所有 .tmp 文件</span><span class="token entry string">temp<span class="token punctuation">/</span><span class="token operator">*</span>.tmp</span><span class="token comment"># 忽略特定目录下的所有文件和子目录，但保留某个文件</span><span class="token entry string">temp<span class="token punctuation">/</span><span class="token operator">*</span></span><span class="token entry string"><span class="token operator">!</span>temp<span class="token punctuation">/</span>keep.txt</span><span class="token comment"># 忽略特定目录下的所有文件和子目录，但保留某个目录</span><span class="token entry string">temp<span class="token punctuation">/</span><span class="token operator">*</span></span><span class="token entry string"><span class="token operator">!</span>temp<span class="token punctuation">/</span>keep<span class="token punctuation">/</span></span></code></pre><h2 id="清除已跟踪的文件">2. 清除已跟踪的文件</h2><p>不会删除 Git 提交历史中的文件，它 只删除工作区（workingdirectory）和未跟踪的文件，不会影响 Git 版本库的提交记录:</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clean <span class="token parameter variable">-n</span> <span class="token parameter variable">-d</span> <span class="token parameter variable">-x</span>  <span class="token comment"># 预览即将删除的文件</span><span class="token function">git</span> clean <span class="token parameter variable">-f</span> <span class="token parameter variable">-d</span> <span class="token parameter variable">-x</span>  <span class="token comment"># 确认无误后执行删除</span></code></pre><h2 id="回退以前的提交">3. 回退以前的提交</h2><h3 id="reset-硬回退">3.1 reset 硬回退</h3><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> reset <span class="token parameter variable">--hard</span> commit <span class="token function">id</span></code></pre><p>此时再推到远程仓库用 <code>git push</code> 会报错，需要用<code>git push -f</code> 强推上去</p><h3 id="revert-软回退">3.2 revert 软回退</h3><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> revert <span class="token parameter variable">-n</span> commit <span class="token function">id</span></code></pre><p>此时会在当前分支上生成一个新的提交，内容是将指定的提交回退到当前分支的状态。注意：<code>-n</code>选项表示不会立即创建一个新的提交，而是将变更放入暂存区</p><h2 id="提交规范">4. 提交规范</h2><h3 id="提交信息的基本结构">4.1 提交信息的基本结构</h3><p>一个标准的 Git 提交信息通常包括三部分：</p><pre class="language-none"><code class="language-none">&lt;类型&gt;(&lt;范围&gt;): &lt;简短描述&gt;&lt;详细描述&gt;&lt;页脚注释&gt;</code></pre><ul><li><strong>类型 (Type)</strong>：描述提交的类别，指明提交的性质。</li><li><strong>范围 (Scope)</strong>：指定变更的具体模块、功能或文件。</li><li><strong>简短描述</strong>：对本次提交的简要总结。</li><li><strong>详细描述</strong>（可选）：提供更详细的信息，解释为什么要做这个提交、如何做的、解决了什么问题等。</li><li><strong>页脚注释</strong>（可选）：链接到项目管理工具中的任务、bug或故事等。</li></ul><h3 id="常见的提交类型">4.2 <strong>常见的提交类型</strong></h3><p>这些类型通常是采用 <ahref="https://www.conventionalcommits.org/zh-hans">ConventionalCommits</a> 标准的常见值，目的是使提交信息一致且具有结构化。</p><ol type="1"><li><code>fix</code>: 类型 为 fix 的提交表示在代码库中修复了一个bug。</li><li><code>feat</code>: 类型 为 feat的提交表示在代码库中新增了一个功能。</li><li><code>BREAKING CHANGE</code>: 在脚注中包含 BREAKING CHANGE: 或<类型>(范围) 后面有一个 ! 的提交，表示引入了破坏性 API 变更。破坏性变更可以是任意<em>类型</em>提交的一部分。</li><li><code>build</code>:用于修改项目构建系统，例如修改依赖库、外部接口或者升级 Node版本等；</li><li><code>chore</code>:用于对非业务性代码进行修改，例如修改构建流程或者工具配置等；</li><li><code>ci</code>: 用于修改持续集成流程，例如修改 Travis、Jenkins等工作流配置；</li><li><code>docs</code>: 用于修改文档，例如修改 README 文件、API文档等；</li><li><code>style</code>:用于修改代码的样式，例如调整缩进、空格、空行等；</li><li><code>refactor</code>:用于重构代码，例如修改代码结构、变量名、函数名等但不修改功能逻辑；</li><li><code>perf</code>:用于优化性能，例如提升代码的性能、减少内存占用等；</li><li><code>test</code>:用于修改测试用例，例如添加、删除、修改代码的测试用例等。</li></ol><p>每次提交应该只包含一个逻辑变更，避免混合多个不相关的修改。</p><h2 id="git全局配置推荐">5. git全局配置推荐</h2><pre class="language-gitconfig" data-language="gitconfig"><code class="language-gitconfig">[user]    name &#x3D; xx    email &#x3D; xx@xx.com[http &quot;https:&#x2F;&#x2F;github.com&quot;]    proxy &#x3D; 127.0.0.1:7890       # GitHub 代理，可根据实际网络环境修改[core]    autocrlf &#x3D; true              # 自动转换换行符，Windows 推荐 true，macOS&#x2F;Linux 推荐 input    quotepath &#x3D; false            # 中文路径不再转义显示为 \uXXXX    editor &#x3D; code --wait         # 默认编辑器 VSCode，可改为 notepad 或 vim    preloadindex &#x3D; true          # 加快 git status 速度    fscache &#x3D; true               # Windows 下缓存文件系统信息，提高性能    untrackedCache &#x3D; true        # 加速未跟踪文件检测，特别是大仓库[color]    ui &#x3D; auto                    # Git 命令输出自动彩色显示（log、diff 等）[alias]    st &#x3D; status                  # git st → git status    co &#x3D; checkout                # git co → git checkout    br &#x3D; branch                  # git br → git branch    ci &#x3D; commit                  # git ci → git commit    df &#x3D; diff                     # git df → git diff    lg &#x3D; log --graph --decorate --pretty&#x3D;format:&#39;%C(yellow)%h%Creset -%C(cyan)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit --date&#x3D;relative                                  # 美观的提交历史，带分支、标签、图形化显示    last &#x3D; log -1 HEAD           # 查看最后一次提交    unstage &#x3D; reset HEAD --      # 取消暂存区文件    amend &#x3D; commit --amend --no-edit  # 修改最后一次提交但不更改提交信息[diff]    tool &#x3D; vscode    algorithm &#x3D; histogram        # 更智能的 diff 算法，处理大文件和移动代码块更准确    colorMoved &#x3D; plain           # 启用移动代码块检测，显示更直观    mnemonicPrefix &#x3D; true        # diff 前缀更易理解    renames &#x3D; true               # 自动检测文件重命名    indentHeuristic &#x3D; true       # 更智能的 diff 块划分    compactionHeuristic &#x3D; true   # 优化 diff 输出可读性[difftool &quot;vscode&quot;]    cmd &#x3D; code --wait --diff $LOCAL $REMOTE  # 使用 VSCode 作为 diff 工具[merge]    tool &#x3D; vscode[mergetool &quot;vscode&quot;]    cmd &#x3D; code --wait $MERGED                 # 使用 VSCode 作为 merge 工具[pull]    rebase &#x3D; false              # 避免默认 rebase，可根据个人习惯改 true[push]    default &#x3D; simple            # 避免推送到错误分支    autoSetupRemote &#x3D; true      # 新建分支时自动设置上游分支    followTags &#x3D; true           # 推送 commit 时自动推送关联的 tag[init]    defaultBranch &#x3D; master      # 初始化仓库默认分支名，可改为 main[column]    ui &#x3D; auto                   # 列表命令自动对齐输出[branch]    sort &#x3D; -committerdate       # 分支列表按最近提交时间排序[tag]    sort &#x3D; version:refname      # 标签列表按版本号排序[fetch]    prune &#x3D; true                # 自动删除远程已删除的分支    pruneTags &#x3D; true            # 自动删除远程已删除的标签    # fetch.all 可选，根据项目是否多远程决定是否开启    # git config --global fetch.all true[help]    autocorrect &#x3D; prompt        # 输入命令有误时提示纠正，避免误操作[commit]    verbose &#x3D; true              # 提交时显示 diff 内容，方便检查[rerere]    enabled &#x3D; true              # 启用冲突重用功能    autoupdate &#x3D; true           # 自动应用已解决的冲突[rebase]    autoStash &#x3D; true            # rebase 前自动 stash，完成后自动 pop    autoSquash &#x3D; true           # 自动处理 fixup!&#x2F;squash! 提交    missingCommitsCheck &#x3D; warn  # 丢失提交时警告[stash]    showPatch &#x3D; true            # stash show 默认显示 diff[status]    submoduleSummary &#x3D; true     # 显示子模块变更摘要    showStash &#x3D; true            # 显示 stash 数量    aheadBehind &#x3D; true          # 显示分支领先&#x2F;落后信息[index]    threads &#x3D; true              # 多线程加速索引操作</code></pre>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Live2D Widget使用说明</title>
      <link href="/2024/11/20/TechnicalNotes/live2d/"/>
      <url>/2024/11/20/TechnicalNotes/live2d/</url>
      
        <content type="html"><![CDATA[<h1 id="live2d-widget-使用说明">Live2D Widget 使用说明</h1><p>在网页中添加 Live2D 看板娘。兼容 PJAX，支持无刷新加载。(注：已不再需要配置依赖 jQuery 和 Font Awesome)</p><p>代码： <ahref="https://github.com/stevenjoezhang/live2d-widget">live2d-widget</a><a href="https://github.com/fghrsh/live2d_api">live2d_api</a></p><p>博文：<br /><a href="https://www.fghrsh.net/post/123.html">网页添加 Live2D看板娘</a> <a href="https://www.fghrsh.net/post/170.html">Live2D 看板娘API 迁移公告</a></p><h2 id="简单配置">1 简单配置</h2><p>只需要最基础的功能，那么只用将这一行代码加入 html 页面的<code>head</code> 或 <code>body</code> 中，即可加载看板娘：</p><pre class="language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://fastly.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span></code></pre><ol type="1"><li>添加代码的位置取决于你的网站的构建方式。例如，如果你使用的是 <ahref="https://hexo.io">Hexo</a>，那么需要在主题的模版文件中添加以上代码。对于用各种模版引擎生成的页面，修改方法类似。<br /></li><li>如果网站启用了 PJAX，由于看板娘不必每页刷新，需要注意将该脚本放到PJAX 刷新区域之外。</li></ol><h2 id="进阶配置">2 进阶配置</h2><h3 id="修改autoload.js文件">2.1 修改<code>autoload.js</code>文件</h3><p>下载<ahref="https://github.com/stevenjoezhang/live2d-widget">live2d-widget</a>源码，找到其中的<code>autoload.js</code>文件，修改其中的配置项。</p><p><code>autoload.js</code>会自动加载三个文件：<code>waifu.css</code>，<code>live2d.min.js</code>和 <code>waifu-tips.js</code>。<code>waifu-tips.js</code> 会创建<code>initWidget</code>函数，这就是加载看板娘的主函数。<code>initWidget</code> 函数接收一个Object 类型的参数，作为看板娘的配置。以下是配置选项：</p><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr><th>选项</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>live2d_path</code></td><td><code>string</code></td><td><code>https://fastly.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/</code></td><td>live2d-widget 路径，路径末尾的 <code>/</code> 一定要加上</td></tr><tr><td><code>apiPath</code></td><td><code>string</code></td><td><code>https://live2d.fghrsh.net/api/</code></td><td>API 路径，路径末尾的 <code>/</code> 一定要加上</td></tr><tr><td><code>cdnPath</code></td><td><code>string</code></td><td><code>https://fastly.jsdelivr.net/gh/fghrsh/live2d_api@latest/</code></td><td>CDN 路径，路径末尾的 <code>/</code> 一定要加上</td></tr><tr><td><code>tools</code></td><td><code>string[]</code></td><td>见 <code>autoload.js</code></td><td>加载的小工具按钮，可选参数</td></tr></tbody></table><p>1️⃣ <code>live2d_path</code> 是 live2d-widget的资源路径，可以自行修改。</p><p>下载<ahref="https://github.com/stevenjoezhang/live2d-widget">live2d-widget</a></p><ol type="1"><li>本地存放：将下载的仓库放在本地目录下，指向本地路径，例如<code>/assets/live2d-widget/</code>。如hexo，放在博客源文件目录下（<code>source</code> 目录），需要设置 <code>skip_render</code>。</li><li>云端存放：将下载的仓库上传到云端，指向云端路径<ul><li>github：使用 <code>jsdelivr</code>，例如<code>https://cdn.jsdelivr.net/gh/username/live2d-widget@latest/</code>，需要创建新的git tag 并推送至 GitHub 仓库中，否则此处的 <code>@latest</code>仍然指向更新前的文件。</li><li>npm：使用 <code>unpkg</code>或者<code>jsdelivr</code>，例如<code>https://unpkg.com/[package_name]/@latest/</code> 或<code>https://cdn.jsdelivr.net/npm/[package_name]/@latest/</code></li><li>云服务器: 新建<strong>静态</strong>项目，例如<code>https://example.com/path/to/live2d-widget/</code></li></ul></li></ol><p>2️⃣ <code>apiPath</code> 和 <code>cdnPath</code>两个参数设置其中一项即可。</p><p>下载<a href="https://github.com/fghrsh/live2d_api">live2d_api</a></p><ul><li><code>apiPath</code> 是后端 API 的 URL，可以自行搭建，并增加模型。<ul><li>需要支持 <code>GET</code> 请求，因此需要建站，以宝塔面板为例：<ul><li>首先，在宝塔面板创建新站点，设置好 PHP 版本（不是纯静态），并添加上SSL 证书。</li><li>然后，删去网站根目录 /www/wwwroot/api/下默认添加创建的所有文件。</li><li>打开 SSH 终端，把 Live2D API 源代码放到网站 live2d/ 目录</li></ul></li></ul></li><li><code>cdnPath</code> 则是类似于 <code>live2d_path</code>的路径，用于加载模型文件。<ul><li>本地存放和云端存放同上。</li></ul></li></ul><h3 id="添加autoload.js文件">2.2 添加<code>autoload.js</code>文件</h3><p>修改<code>live2d_path</code>后将这一行代码加入 html 页面的<code>head</code> 或 <code>body</code> 中，即可加载看板娘：</p><pre class="language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>&#123;live2d_path&#125;/autoload.js<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span></code></pre><h3 id="测试">2.3 测试</h3><p>不妨试试能否正常地通过浏览器打开 <code>autoload.js</code> 和<code>live2d.min.js</code>等文件，并确认这些文件的内容是完整和正确的。</p><h2 id="高级配置">3 高级配置</h2><p>修改样式和模型：自行查阅两个仓库的README.md文件和源码。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Twikoo评论系统配置</title>
      <link href="/2024/11/20/Web/Twikoo/"/>
      <url>/2024/11/20/Web/Twikoo/</url>
      
        <content type="html"><![CDATA[<h1 id="twikoo-评论系统配置">Twikoo 评论系统配置</h1><p>目前很多面板支持一键配置，以下是私有部署方式介绍</p><h2 id="云端部署">云端部署</h2><ol type="1"><li>在 <a href="https://twikoo.js.org/">TwiKoo 官网</a>找到云函数部署的私有部署方式，按照文档进行配置。</li><li><a href="https://nodejs.org/zh-cn/download">下载nodejs</a>选择最新的LTS版本，Linux，nvm，npm。按照命令安装。</li><li>指定镜像源：<code>npm config set registry https://registry.npmmirror.com</code></li><li>安装 Twikoo server:<code>npm i -g tkserver</code>查看是否安装成功：<code>npm ls tkserver -g</code></li><li>配置环境变量，主要关注以下几个参数：<ul><li><code>TWIKOO_DATA</code>：lokijs 数据库存储路径，默认是<code>./data</code>。</li><li><code>TWIKOO_PORT</code>：服务端口，默认是<code>8080</code>，注意需要在<strong>服务器安全组</strong>和<strong>宝塔面板</strong>中开放该端口</li><li><code>TWIKOO_IP_HEADERS</code>：在特殊情况下使用，如使用了CloudFlare CDN 它会将请求 IP 写到请求头的 cf-connecting-ip字段上，为了能够正确的获取请求 IP 你可以写成[“headers.cf-connecting-ip”]，默认是 []</li></ul></li><li>启动服务<ul><li>创建一个文件夹，如<code>/www/wwwroot/twikoo</code>，<strong>在该文件夹下</strong>执行启动命令即可将数据库文件存储在该文件夹下</li><li><code>export TWIKOO_DATA=/www/wwwroot/twikoo/data</code></li><li><code>export TWIKOO_PORT=xxxx</code></li><li><code>export TWIKOO_IP_HEADERS=["headers.cf-connecting-ip"]</code></li><li><code>source ~/.bashrc</code></li><li>检查环境变量是否设置成功(注意：有时设置不成功，可以直接添加到/root/.bashrc中)：<ul><li><code>export | grep TWIKOO</code></li><li>粗略启动服务<code>tkserver</code>并访问<code>http://ip:xxxx</code>查看是否成功</li></ul></li></ul></li><li>在<strong>新文件夹</strong>下<code>nohup tkserver &gt;&gt; tkserver.log 2&gt;&amp;1 &amp;</code>命令后台启动，访问<a href="http://服务端IP:端口号" class="uri">http://服务端IP:端口号</a>测试服务是否启动成功。但是概率存在使用<code>nohup &amp;</code>命令运行shell脚本，关闭终端仍然退出(可以试试exit命令注销终端)……</li><li>配置代理实现 HTTPS 访问:<ul><li>添加纯静态站点：PHP项目，twikoo.{域名}</li><li>配置ssl证书:let’s encrypt 手动解析 通配符修改对应的阿里云和cloudflare的DNS解析，也可能只需要修改cloudflare的DNS解析</li><li>添加反向代理: 代理名称：twikoo，目标URL：<ahref="http://127.0.0.1:%7B端口%7D"class="uri">http://127.0.0.1:{端口}</a>，代理域名：$host</li></ul></li><li>最终访问地址为：<a href="https://twikoo.%7B域名%7D"class="uri">https://twikoo.{域名}</a></li><li>注意定期备份数据库文件，防止数据丢失</li></ol><h2 id="前端配置">前端配置</h2><p>到博客配置文件中配置 envId 为 https:// 加域名（例如 <ahref="https://twikoo.yourdomain.com）"class="uri">https://twikoo.yourdomain.com）</a></p><h2 id="更新云端">更新云端</h2><ol type="1"><li>停止旧版本<code>kill $(ps -ef | grep tkserver | grep -v 'grep' | awk '{print $2}')</code></li><li>拉取新版本 <code>npm i -g tkserver@latest</code></li><li>回到上文第7步重新启动服务</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— A 基于值函数vs基于策略方法</title>
      <link href="/2024/09/19/RL/A%20%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0vs%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%96%B9%E6%B3%95/"/>
      <url>/2024/09/19/RL/A%20%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0vs%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="基于值函数-vs-基于策略方法">1. 基于值函数 vs 基于策略方法</h2><p>强化学习解决决策问题的思路可以分为两大类：一类是”评估价值后决策”，另一类是”直接优化决策”。这就是我们常说的<strong>基于值函数的方法</strong>和<strong>基于策略的方法</strong>。</p><h3 id="基于值函数的方法先评估再决策">1.1基于值函数的方法：先评估，再决策</h3><p>想象你是一个下棋的新手，如何提高棋艺？一个直观的想法是：学会评估每个局面的好坏，然后选择能达到最好局面的走法。这就是基于值函数方法的核心思想。</p><h4 id="核心理念">核心理念</h4><p>基于值函数的方法通过学习<strong>价值评估函数</strong>来指导决策：</p><ul><li><strong>值函数评估</strong>：基于值函数的方法主要通过估计状态值函数( V(s) ) 或动作价值函数 ( Q(s, a) )来评估每个状态或状态-动作对的优劣。这里，( V(s) ) 表示在状态 ( s )下能获得的期望累计奖励，而 ( Q(s, a) ) 则表示在状态 ( s ) 采取动作 ( a )后的期望累计奖励。</li><li><strong>策略间接导出</strong>：这些方法通常不直接表示策略，而是通过优化值函数，然后从中推导出最优策略。例如，在给定( Q(s, a) ) 的情况下，可以通过选择使 ( Q(s, a) )最大的动作来获得最优策略。</li><li><strong>状态价值函数</strong> <spanclass="math inline">\(V(s)\)</span>：评估”在状态s下，按照当前策略能获得多少长期奖励”</li><li><strong>动作价值函数</strong> <spanclass="math inline">\(Q(s,a)\)</span>：评估”在状态s下采取动作a，能获得多少长期奖励”</li></ul><p>有了这些价值评估，策略就水到渠成了——总是选择价值最高的动作。</p><h4 id="典型算法">典型算法</h4><p><strong>Q学习(Q-Learning)</strong>：最经典的值函数方法</p><ul><li>直接学习最优动作价值函数 <spanclass="math inline">\(Q^*(s,a)\)</span></li><li>采用”时间差分”的思想逐步改进Q值估计</li><li>属于off-policy算法，可以从任意行为策略中学习</li></ul><p><strong>深度Q网络(DQN)</strong>：Q学习的深度学习版本</p><ul><li>用神经网络近似Q函数，处理高维状态空间</li><li>引入经验回放和目标网络等技巧提高稳定性</li></ul><h3 id="基于策略的方法直接优化决策">1.2基于策略的方法：直接优化决策</h3><p>如果说值函数方法是”三思而后行”，那么策略方法就是”熟能生巧”——直接练习决策过程本身，通过不断试错来改进策略。</p><h4 id="核心理念-1">核心理念</h4><p>基于策略的方法将策略参数化为 <spanclass="math inline">\(\pi_\theta(a|s)\)</span>，直接优化策略参数 <spanclass="math inline">\(\theta\)</span>：</p><ul><li><strong>目标明确</strong>：最大化期望累积奖励 <spanclass="math inline">\(J(\theta) = \mathbb{E}_{\tau \sim\pi_\theta}[R(\tau)]\)</span></li><li><strong>梯度上升</strong>：利用策略梯度定理计算参数更新方向</li><li><strong>自然探索</strong>：随机策略天然具备探索能力</li></ul><h4 id="策略梯度定理">策略梯度定理</h4><p>策略优化的数学基础来自于著名的策略梯度定理：</p><p><span class="math display">\[\nabla_\theta J(\theta) =\mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdotQ^{\pi_\theta}(s,a)]\]</span></p><p>这个公式告诉我们：如果某个动作的价值高，就增加选择它的概率；反之则降低概率。</p><h4 id="典型算法-1">典型算法</h4><p><strong>REINFORCE算法</strong>：最基础的策略梯度方法</p><ul><li>使用蒙特卡洛方法估计回报</li><li>简单直接，但方差较大</li></ul><p><strong>PPO(Proximal PolicyOptimization)</strong>：现代策略优化的代表</p><ul><li>通过”信任区域”思想控制策略更新幅度</li><li>在样本效率和稳定性间取得良好平衡</li></ul><h2 id="两种方法的深度对比">1.3 两种方法的深度对比</h2><table><thead><tr><th>维度</th><th>基于值函数</th><th>基于策略</th></tr></thead><tbody><tr><td><strong>动作空间</strong></td><td>适合离散动作</td><td>天然支持连续动作</td></tr><tr><td><strong>策略复杂度</strong></td><td>简单确定性策略</td><td>支持复杂随机策略</td></tr><tr><td><strong>样本效率</strong></td><td>通常较高</td><td>相对较低</td></tr><tr><td><strong>探索机制</strong></td><td>需要额外设计(如ε-贪心)</td><td>策略本身包含探索</td></tr></tbody></table><h4 id="值函数方法的特点">值函数方法的特点</h4><ul><li>样本效率高，能从有限经验中高效学习</li><li>理论基础扎实，基于贝尔曼方程</li><li>在离散动作空间中表现优异</li><li>难以处理连续或高维动作空间</li><li>需要额外机制平衡探索与利用</li><li>函数近似可能导致不稳定</li></ul><h4 id="策略方法的特点">策略方法的特点</h4><ul><li>自然处理连续动作空间</li><li>策略表达能力强，可学习复杂行为</li><li>收敛性保证较好（在合适条件下）</li><li>样本需求量大</li><li>梯度估计方差高，需要降方差技巧</li><li>容易陷入局部最优</li></ul><h3 id="融合之道actor-critic方法">1.4 融合之道：Actor-Critic方法</h3><p>既然两种方法各有所长，自然有人想到将它们结合起来。这就是<strong>Actor-Critic方法</strong>的核心思想：</p><ul><li><strong>Actor(演员)</strong>：基于策略的组件，负责选择动作</li><li><strong>Critic(评论家)</strong>：基于值函数的组件，负责评估动作价值</li></ul><p>这种结合带来了显著优势：</p><ol type="1"><li>Critic帮助Actor减少梯度方差</li><li>Actor为Critic提供更好的探索策略</li><li>两者相互促进，加速收敛</li></ol><h2 id="动态规划中的经典对决策略迭代-vs-价值迭代">2.动态规划中的经典对决：策略迭代 vs 价值迭代</h2><p>在基于值函数的方法中，动态规划提供了两种经典的求解思路。需要注意的是，策略迭代和价值迭代都属于<strong>基于值函数</strong>的方法，它们的区别在于如何利用值函数来优化策略。</p><h3 id="策略迭代">2.1 策略迭代</h3><p>策略迭代是一种通过不断交替执行<strong>策略评估</strong>和<strong>策略改进</strong>来找到最优策略的方法。</p><ul><li><strong>策略评估</strong>：固定当前策略，计算该策略下所有状态的值函数(V^(s))，即评估在当前策略下，每个状态的长期奖励。</li><li><strong>策略改进</strong>：利用评估出的值函数，改进当前策略，即在每个状态下选择使值函数最大的动作。</li></ul><p>策略迭代的步骤是：</p><ol type="1"><li>初始化策略 (_0)。</li><li>对策略 (_i) 进行策略评估，计算对应的值函数 (V^{_i}(s))。</li><li>基于 (V^{<em>i}(s)) 改进策略，得到新的策略 (</em>{i+1})。</li><li>重复步骤 2 和 3，直到策略收敛到最优策略 (^*)。</li></ol><h3 id="价值迭代">2.2 价值迭代</h3><p>价值迭代通过直接更新值函数的方式来迭代逼近最优值函数(V^*(s))，然后通过值函数导出策略。</p><ul><li>每一步都会对每个状态 (s) 更新其值函数为最大化预期奖励的值</li><li>通过不断更新值函数，逐步逼近最优值函数。当值函数收敛时，可以通过选择最大化值函数的动作来获得最优策略。</li></ul><p>价值迭代的步骤是：</p><ol type="1"><li>初始化值函数 (V_0(s))。</li><li>对每个状态 (s)，更新值函数 (V_{i+1}(s))，使其最大化预期回报。</li><li>重复上述过程，直到值函数收敛。</li><li>当值函数收敛后，通过选择使 (V(s)) 最大的动作(a)，导出最优策略。</li></ol><h3 id="策略迭代-vs-价值迭代">2.3 策略迭代 vs 价值迭代</h3><ul><li><strong>策略迭代</strong>：交替执行完整的策略评估和策略改进，计算较精确的策略更新，通常收敛较快，但每次评估需要花费较多时间。</li><li><strong>价值迭代</strong>：在更新值函数的同时隐式优化策略，每次更新的粒度较小，虽然更新速度快，但可能需要更多迭代次数才能达到收敛。</li></ul><table><thead><tr><th style="text-align: center;">特性</th><th style="text-align: center;">策略迭代</th><th style="text-align: center;">价值迭代</th></tr></thead><tbody><tr><td style="text-align: center;">迭代步骤</td><td style="text-align: center;">策略评估 + 策略改进</td><td style="text-align: center;">值函数更新</td></tr><tr><td style="text-align: center;">每次迭代成本</td><td style="text-align: center;">高</td><td style="text-align: center;">低</td></tr><tr><td style="text-align: center;">迭代次数</td><td style="text-align: center;">少</td><td style="text-align: center;">多</td></tr><tr><td style="text-align: center;">收敛速度</td><td style="text-align: center;">快</td><td style="text-align: center;">慢</td></tr><tr><td style="text-align: center;">适用场景</td><td style="text-align: center;">小型 MDP，需精确解</td><td style="text-align: center;">大型 MDP，需快速近似</td></tr><tr><td style="text-align: center;">稳定性</td><td style="text-align: center;">高</td><td style="text-align: center;">可能震荡，需要调节参数</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— 12 DDPG算法</title>
      <link href="/2024/09/17/RL/12%20DDPG%E7%AE%97%E6%B3%95/"/>
      <url>/2024/09/17/RL/12%20DDPG%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="ddpg算法">12 DDPG算法</h2><p>深度确定性策略梯度是在动作空间无限的环境使用off-policy的actor-critic算法。它它的actor是一个确定性策略，通过梯度上升法来最大化Q值；它的critic是一个Q网络，通过梯度下降法来最小化Q值的TD误差。</p><h3 id="ddpg算法-1">12.1 DDPG算法</h3><h4 id="算法引出">12.1.1 算法引出</h4><ol type="1"><li>TRPO和PPO是on-policy的actor-critic算法，这意味着它们只能在当前策略上进行更新，样本效率较低。</li><li>DQN是off-policy的Q-learning算法，直接估计Q值，样本效率较高，但是但是它只能处理动作空间有限的环境，这是因为它需要从所有动作中挑选一个<spanclass="math inline">\(Q\)</span>值最大的动作。虽然可以将将动作空间离散化，但这比较粗糙，无法精细控制。</li><li>TRPO和PPO学习的是随机策略，而DDPG学习的确定性策略。随机策略可以表示为<spanclass="math inline">\(a\sim\pi_\theta(a|s)\)</span>，确定性策略可以表示为<spanclass="math inline">\(a=\mu_\theta(s)\)</span>。</li></ol><h4 id="算法公式">12.1.2 算法公式</h4><p>确定性策略梯度定理：</p><p><span class="math display">\[\nabla_\theta J(\mu_\theta)=\mathbb{E}_{s\sim \rho_\mu,a\sim\mu_\theta}[\nabla_\theta \mu_\theta(s)\nabla_aQ_\phi(s,a)|_{a=\mu_\theta(s)}]\]</span></p><p><strong>这个公式可以理解为</strong>：假定现在已有函数<spanclass="math inline">\(Q\)</span>，给定一个状态<spanclass="math inline">\(s\)</span>，但由于现在动作空间是无限的，所以无法遍历所有动作来得到最大的<spanclass="math inline">\(Q\)</span>值，因此我们用策略<spanclass="math inline">\(\mu_\theta(s)\)</span>来找到一个动作<spanclass="math inline">\(a\)</span>，使得<spanclass="math inline">\(Q(s,a)\)</span>最大。此时，<spanclass="math inline">\(Q\)</span>就是critic,<spanclass="math inline">\(\mu_\theta(s)\)</span>就是actor。</p><h4 id="算法描述">12.1.3 算法描述</h4><p>DDPG要用到4个网络：<spanclass="math inline">\(\mu,\mu&#39;,Q,Q&#39;\)</span>。其中<spanclass="math inline">\(\mu\)</span>是actor，<spanclass="math inline">\(Q\)</span>是critic，<spanclass="math inline">\(\mu&#39;\)</span>是actor的target网络，<spanclass="math inline">\(Q&#39;\)</span>是critic的target网络。</p><p>目标网络的的更新方式是<strong>软更新</strong>： <spanclass="math display">\[\omega^-=\tau\omega+(1-\tau)\omega^-\]</span> 其中<spanclass="math inline">\(\tau\)</span>是更新系数，通常很小的一个数。当<spanclass="math inline">\(\tau = 1\)</span>时，就和DQN更新方式一样了。</p><p>另外，由于函数<span class="math inline">\(Q\)</span>存在<spanclass="math inline">\(Q\)</span>值估值过高的问题，DDPG 采用了 Double DQN中的技术来更新<span class="math inline">\(Q\)</span>网络。 但是，由于DDPG采用的是确定性策略，它本身的探索仍然十分有限。作为一种离线策略的算法，DDPG在行为策略上<strong>引入一个随机噪声<spanclass="math inline">\(\mathcal{N}\)</span>来进行探索</strong>。</p><h4 id="算法流程">12.1.4 算法流程</h4><ol type="1"><li>随机初始化actor和critic的网络参数<spanclass="math inline">\(\theta\)</span>和<spanclass="math inline">\(\omega\)</span>，初始化噪声过程<spanclass="math inline">\(\mathcal{N}\)</span>。</li><li>复制<span class="math inline">\(\theta\)</span>和<spanclass="math inline">\(\omega\)</span>到<spanclass="math inline">\(\theta^-\)</span>和<spanclass="math inline">\(\omega^-\)</span>，初始化目标网络。</li><li>初始化经验回放池<spanclass="math inline">\(\mathcal{D}\)</span>。</li><li>for 轮次<span class="math inline">\(episode=1,2,...,E\)</span> do<ul><li>初始化随机过程<spanclass="math inline">\(\mathcal{N}\)</span>用来探索。</li><li>获取环境初始状态<span class="math inline">\(s_1\)</span>。</li><li>for 时间步 t=1,2,…,T do<ul><li>根据当前策略和噪声选择动作<span class="math inline">\(a_t =\mu_\theta(s_t)+\mathcal{N}\)</span>。</li><li>执行动作<span class="math inline">\(a_t\)</span>，得到新状态<spanclass="math inline">\(s_{t+1}\)</span>，奖励<spanclass="math inline">\(r_t\)</span>。</li><li>将<spanclass="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入<spanclass="math inline">\(\mathcal{D}\)</span>。</li><li>从<span class="math inline">\(\mathcal{D}\)</span>中随机采样<spanclass="math inline">\(N\)</span>个样本<spanclass="math inline">\({(s_i,a_i,r_i,s_{i+1})}_{i=1,2,...,N}\)</span>。</li><li>对每个样本<spanclass="math inline">\(i\)</span>，用目标网络计算<spanclass="math inline">\(y_i=r_i+\gammaQ_\phi(s_{i+1},\mu_\theta(s_{i+1}))\)</span>。</li><li>最小化目标损失函数<spanclass="math inline">\(L(\omega)=\frac{1}{N}\sum_i(y_i-Q_\omega(s_i,a_i))^2\)</span>，更新当前critic网络参数<spanclass="math inline">\(\omega\)</span>。</li><li>计算采样的<spanclass="math inline">\(N\)</span>个动作的策略梯度，更新当前actor网络参数<spanclass="math inline">\(\theta\)</span>。<spanclass="math inline">\(\nabla_\theta J \approx\frac{1}{N}\sum_i\nabla_\theta \mu_\theta(s_i)\nabla_aQ_\phi(s_i,a_i)|_{a=\mu_\theta(s_i)}\)</span></li><li>更新目标网络。<spanclass="math inline">\(\omega^-=\tau\omega+(1-\tau)\omega^-,\theta^-=\tau\theta+(1-\tau)\theta^-\)</span>。</li></ul></li><li>end for</li></ul></li><li>end for</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— 9 Actor-Critic算法</title>
      <link href="/2024/09/17/RL/9%20Actor-Critic%E7%AE%97%E6%B3%95/"/>
      <url>/2024/09/17/RL/9%20Actor-Critic%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="actor-critic算法">9 Actor-Critic算法</h2><p>Actor-Critic算法是基于<strong>值函数的方法和基于策略的方法的叠加</strong>。价值模块Critic 在<strong>策略模块 Actor采样的数据</strong>中学习分辨什么是好的动作，什么不是好的动作，进而<strong>指导Actor 进行策略更新</strong>。随着 Actor的训练的进行，其与环境交互所产生的数据分布也发生改变，这需要 Critic尽快适应新的数据分布并给出好的判别。</p><h3 id="actor-critic算法-1">9.1 Actor-Critic算法</h3><p>Actor-Critic算法既学习价值函数，又学习策略函数。不过本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。</p><p>在 REINFORCE算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE算法用蒙特卡洛方法来估计<spanclass="math inline">\(Q^{\pi_\theta}(s,a)\)</span>，能不能考虑<strong>拟合一个值函数来</strong>指导策略进行学习呢？这正是Actor-Critic 算法所做的。</p><p>Actor-Critic 算法估<strong>计一个动作价值函数<spanclass="math inline">\(Q\)</span>，代替蒙特卡洛采样得到的回报</strong>。</p><p>REINFORCE算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数；而Actor-Critic算法则可以在<strong>每一步之后都进行更新</strong>，并且不对任务的步数做限制。</p><h3 id="actor-critic算法描述">9.2 Actor-Critic算法描述</h3><p>Actor-Critic 算法分为两个部分：</p><ul><li>策略网络（Actor）：要做的是<strong>与环境交互</strong>，并在 Critic价值函数的指导下用策略梯度学习一个更好的策略。</li><li>价值网络（Critic）：要做的是<strong>通过 Actor与环境交互收集的数据学习一个价值函数</strong>，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助Actor 进行策略更新。</li></ul><p>更新方式：</p><ul><li>Actor采用策略梯度更新</li><li>Critic采用梯度下降法更新</li></ul><h3 id="actor-critic算法流程">9.3 Actor-Critic算法流程</h3><ol type="1"><li>初始化策略网络参数<spanclass="math inline">\(\theta\)</span>和价值网络参数<spanclass="math inline">\(\omega\)</span>。</li><li>for 轮次<span class="math inline">\(episode=1,2,...,E\)</span> do<ul><li>用当前策略<spanclass="math inline">\(\pi_\theta\)</span>与环境交互，得到轨迹<spanclass="math inline">\({s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T}\)</span>。</li><li>为每一步数据计算时序差分残差：<span class="math inline">\(\delta_t =r_t + \gamma V_w(s_{t+1}) - V_w(s_t)\)</span>。</li><li>更新价值网络参数<span class="math inline">\(\omega\)</span>：<spanclass="math inline">\(\omega \leftarrow \omega + \alpha_\omega \delta_t\nabla_w V_w(s_t)\)</span>。</li><li>更新策略网络参数<span class="math inline">\(\theta\)</span>：<spanclass="math inline">\(\theta \leftarrow \theta + \alpha_\theta \delta_t\nabla_\theta \log \pi_\theta(s_t,a_t)\)</span>。</li></ul></li><li>end for</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— 8 策略梯度算法</title>
      <link href="/2024/09/17/RL/8%20%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/"/>
      <url>/2024/09/17/RL/8%20%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="策略梯度算法">8 策略梯度算法</h2><p>本章介绍基于<strong>策略</strong>（policy-based）的方法，而策略梯度是基于策略的方法的基础。</p><h3 id="基于价值和基于策略">8.1 基于价值和基于策略</h3><p>Q-learning、DQN 及 DQN改进算法都是基于<strong>价值</strong>（value-based）的方法，其中Q-learning 是处理<strong>有限状态</strong>的算法，而 DQN可以用来解决<strong>连续状态</strong>的问题。</p><p><strong>基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。</strong></p><h3 id="策略梯度">8.2 策略梯度</h3><p>基于策略的方法首先需要将策略参数化。 假设目标策略<spanclass="math inline">\(\pi_{\theta}\)</span>是一个随机性策略，并且处处可微，其中<spanclass="math inline">\(\theta\)</span>是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个<strong>策略函数</strong>建模，<strong>输入某个状态，然后输出一个动作的概率分布</strong>。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。</p><p>策略学习的目标函数： <span class="math display">\[J(\theta)=E_{s_0}[V^{\pi_{\theta}}(s_0)]\]</span> 其中<spanclass="math inline">\(V^{\pi_{\theta}}(s_0)\)</span>是初始状态为<spanclass="math inline">\(s_0\)</span>时，使用策略<spanclass="math inline">\(\pi_{\theta}\)</span>得到的期望回报。</p><p>有了目标函数，可以将目标函数对参数<spanclass="math inline">\(\theta\)</span>求导，得到导数后，可以按照梯度上升的方法来最大化这个目标函数，从而得到最优策略。<strong>直观理解就是，梯度的修改是让策略更多地去采样到带来较高<spanclass="math inline">\(Q\)</span>值的动作，更少地去采样到带来较低<spanclass="math inline">\(Q\)</span>值的动作。</strong></p><p><strong>注意</strong>：策略梯度算法是<strong>on-policy</strong>的算法，即<strong>必须使用当前策略采样得到的数据来计算梯度</strong>。</p><p>在计算策略梯度的公式中，我们需要用到<spanclass="math inline">\(Q^{\pi_\theta}(s,a)\)</span>，可以采用多种方法对其进行估计。例如Reinforce算法。</p><h3 id="reinforce算法">8.3 Reinforce算法</h3><p>Reinforce算法是策略梯度算法中非常经典的一个算法，它使用<strong>蒙特卡洛方法</strong>来估计动作价值函数<spanclass="math inline">\(Q^{\pi_\theta}(s,a)\)</span>。</p><p>Reinforce算法的具体流程如下：</p><ol type="1"><li>初始化策略参数<spanclass="math inline">\(\theta\)</span>，并设定学习率<spanclass="math inline">\(\alpha\)</span>；</li><li>for 轮次<span class="math inline">\(episode=1,2,...\)</span> do<ul><li>用当前策略<spanclass="math inline">\(\pi_{\theta}\)</span>采样一条轨迹<spanclass="math inline">\({s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T}\)</span>；</li><li>计算当前轨迹每个时刻<spanclass="math inline">\(t\)</span>往后的回报<spanclass="math inline">\(\sum_{k=t}^T\gamma^{k-t}r_k\)</span>，记为<spanclass="math inline">\(G_t\)</span>；</li><li>对<span class="math inline">\(\theta\)</span>进行更新，<spanclass="math inline">\(\theta\leftarrow\theta+\alpha\sum_{t}^T\nabla_\theta\log\pi_\theta(a_t|s_t)G_t\)</span>。</li></ul></li><li>end for</li></ol><h3 id="总结">8.4 总结</h3><p>REINFORCE算法是策略梯度乃至强化学习的典型代表，智能体<strong>根据当前策略直接和环境交互</strong>，通过<strong>采样得到的轨迹数据</strong>直接<strong>计算出策略参数的梯度</strong>，进而更新当前策略，使其向最大化策略期望回报的目标靠近。</p><p>这种学习方式是典型的<strong>从交互中学习</strong>，并且其<strong>优化的目标（即策略期望回报）正是最终所使用策略的性能</strong>，这比基于价值的强化学习算法的优化目标（一般是时序差分误差的最小化）要更加直接。</p><p>REINFORCE算法理论上是能保证<strong>局部最优</strong>的，它实际上是<strong>借助蒙特卡洛方法采样轨迹来估计动作价值</strong>，这种做法的一大优点是可以得到<strong>无偏的梯度</strong>。但是，正是因为使用了蒙特卡洛方法，REINFORCE算法的梯度估计的方差很大，这主要是因为<strong>每条采样轨迹的回报值波动比较大</strong>，可能会造成一定程度上的不稳定，这也是Actor-Critic 算法要解决的问题。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— 3 马尔可夫决策过程</title>
      <link href="/2024/09/17/RL/3%20%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
      <url>/2024/09/17/RL/3%20%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="马尔可夫决策过程">3 马尔可夫决策过程</h2><p>如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。</p><h3 id="马尔可夫过程">3.1 马尔可夫过程</h3><h4 id="随机过程">3.2.1 随机过程</h4><p>时刻<span class="math inline">\(t+1\)</span>的状态<spanclass="math inline">\(S_{t+1}\)</span>概率可以表示为<spanclass="math inline">\(P(S_{t+1}|S_1,S_2,...,S_t)\)</span>，即未来的状态取决于过去的状态以及当前的状态。</p><h4 id="马尔可夫性质">3.2.2 马尔可夫性质</h4><p>当且仅当未来状态的概率分布<strong>只依赖于当前状态</strong>时，这个随机过程具有马尔可夫性质。即：<spanclass="math inline">\(P(S_{t+1}|S_t)=P(S_{t+1}|S_1,S_2,...,S_t)\)</span></p><p><strong>但是并不是说<spanclass="math inline">\(S_{t+1}\)</span>和历史完全没有关系</strong>，因为<spanclass="math inline">\(S_{t+1}\)</span>是由<spanclass="math inline">\(S_t\)</span>决定的，而<spanclass="math inline">\(S_t\)</span>又是由<spanclass="math inline">\(S_{t-1}\)</span>决定的，以此类推，<spanclass="math inline">\(S_{t+1}\)</span>和历史是有关系的，只是说<spanclass="math inline">\(S_{t+1}\)</span>和历史的关系可以通过当前状态<spanclass="math inline">\(S_t\)</span>来表示。</p><h4 id="马尔可夫过程-1">3.2.3 马尔可夫过程</h4><p>马尔可夫过程(马尔科夫链)是指具有<strong>马尔可夫性质</strong>的<strong>随机过程</strong>。</p><p>通常用元组<span class="math inline">\(\langleS,P\rangle\)</span>来表示马尔可夫过程，其中<spanclass="math inline">\(S\)</span>是<strong>有限数量的状态集</strong>，<spanclass="math inline">\(P\)</span>是<strong>状态转移矩阵</strong>。</p><p>假设状态集<span class="math inline">\(S\)</span>有<spanclass="math inline">\(n\)</span>个状态，状态转移矩阵<spanclass="math inline">\(P\)</span>是一个<spanclass="math inline">\(n\times n\)</span>的矩阵： <spanclass="math display">\[P=\begin{bmatrix}P(s_1|s_1) &amp; P(s_2|s_1) &amp; ... &amp; P(s_n|s_1) \\P(s_1|s_2) &amp; P(s_2|s_2) &amp; ... &amp; P(s_n|s_2) \\... &amp; ... &amp; ... &amp; ... \\P(s_1|s_n) &amp; P(s_2|s_n) &amp; ... &amp; P(s_n|s_n) \\\end{bmatrix}\]</span> <span class="math inline">\(P(s_i|s_j)\)</span>表示从状态<spanclass="math inline">\(s_j\)</span>转移到状态<spanclass="math inline">\(s_i\)</span>的概率，被称为<strong>状态转移函数</strong>。<strong>从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行之和为1</strong>。</p><p>举个例子如图所示： <img src="./images/markov-process.webp#50x50"title="图3.1: 马尔可夫过程的一个简单例子"alt="马尔可夫过程的一个简单例子" /> 可以写出如下的状态转移矩阵： <spanclass="math display">\[P=\begin{bmatrix}0.9 &amp; 0.1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0.5 &amp; 0 &amp; 0.5 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0.6 &amp; 0 &amp; 0.4 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.3 &amp; 0.7 \\0 &amp; 0.2 &amp; 0.3 &amp; 0.5 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\end{bmatrix}\]</span> 其中每一个节点都是一个状态(<spanclass="math inline">\(s_6\)</span>通常被称为终止状态)。</p><p>给定一个马尔可夫过程，我们就可以<strong>从某个状态</strong>出发，根据它的<strong>状态转移矩阵</strong>生成一个<strong>状态序列</strong>（episode），这个步骤也被叫做<strong>采样</strong>（sampling）。例如，从状态<spanclass="math inline">\(s_1\)</span>出发，根据状态转移矩阵<spanclass="math inline">\(P\)</span>，可以生成一个状态序列<spanclass="math inline">\(s_1\rightarrow s_2\rightarrow s_3\rightarrows_6\)</span>，也可以是<span class="math inline">\(s_1\rightarrows_1\rightarrow s_2\rightarrow s_3\rightarrow s_4 \rightarrows_5\rightarrow s_3\rightarrows_6\)</span>。<strong>生成这些序列的概率和状态转移矩阵有关</strong>。</p><h3 id="马尔可夫奖励过程">3.3 马尔可夫奖励过程</h3>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习—— 2 多臂老虎机</title>
      <link href="/2024/09/17/RL/2%20%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/"/>
      <url>/2024/09/17/RL/2%20%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="多臂老虎机">2 多臂老虎机</h2><p>多臂老虎机中的探索与利用（explorationvs. exploitation）问题一直以来都是一个特别经典的问题，理解它能够帮助我们学习强化学习。与强化学习不同，多臂老虎机不存在状态信息，只有动作和奖励，算是最简单的“和环境交互中的学习”的一种形式。</p><h3 id="问题介绍">2.1 问题介绍</h3><h4 id="问题定义">2.1.1 问题定义</h4><p>有一个拥有<spanclass="math inline">\(K\)</span>根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布<spanclass="math inline">\(R\)</span>。我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。</p><h4 id="形式化描述">2.1.2 形式化描述</h4><p>多臂老虎机问题可以表示为一个元组 <span class="math inline">\((A,R)\)</span></p><ul><li><span class="math inline">\(A\)</span>表示动作集合，一个动作表示拉动一根拉杆，<span class="math inline">\(A =\{a_1, a_2, ..., a_K\}\)</span></li><li><span class="math inline">\(R\)</span> 表示奖励概率分布，<spanclass="math inline">\(R(r|a)\)</span> 表示当动作<spanclass="math inline">\(a\)</span> 作用在环境上后得到的奖励<spanclass="math inline">\(r\)</span>的概率分布</li></ul><p>多臂老虎机的目标为最大化一段时间步内累积的奖励</p><h4 id="累计懊悔">2.1.3 累计懊悔</h4><p>至少存在一根拉杆，它的期望奖励不小于拉动其他任意一根拉杆的期望奖励。我们称这个拉杆为最优拉杆，对应的期望奖励为最优期望奖励。我们可以定义累计懊悔（cumulativeregret）为我们在一段时间内获得的奖励与最优期望奖励之间的差值总额。</p><p>MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。</p><h4 id="估计期望奖励">2.1.4 估计期望奖励</h4><p>为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望。</p><p>更新期望奖励通常采用增量法，即</p><p><span class="math display">\[Q_{t+1} = Q_t + \frac{1}{t+1} \left( r_t - Q_t \right)\]</span></p><p>如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为<spanclass="math inline">\(O(n)\)</span>。而采用增量式更新，时间复杂度和空间复杂度均为<span class="math inline">\(O(1)\)</span>。</p><h3 id="探索与利用的权衡">2.2 探索与利用的权衡</h3><p>在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如<spanclass="math inline">\(\epsilon\)</span>-贪婪算法、上置信界算法和汤普森采样算法等。</p><h3 id="epsilon-贪婪算法">2.3 <spanclass="math inline">\(\epsilon\)</span>-贪婪算法</h3><p><spanclass="math inline">\(\epsilon\)</span>-贪婪算法即在完全贪婪的基础上添加了噪声，以<spanclass="math inline">\(\epsilon\)</span>的概率进行探索(随机选择一根拉杆)，以<spanclass="math inline">\(1-\epsilon\)</span>的概率进行利用(选择以往经验中期望奖励估值最大的那根拉杆)。</p><p>随着探索次数的增加，可以让<spanclass="math inline">\(\epsilon\)</span>逐渐减小，从而逐渐增加利用的次数。因为随着探索次数的增加，我们对各个动作的评估会越来越准，因此也就可以不必多次尝试同一个动作。</p><p><strong>注意</strong>，<span class="math inline">\(\epsilon\)</span>不会在有限步数内减小到0(因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距)。</p><h3 id="上置信界算法">2.4 上置信界算法</h3><p>一根拉杆被拉取的次数越少，它的不确定性就越大；一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。引入不确定性度量<span class="math inline">\(U(a)\)</span>，它会随着一个动作被尝试次数的增加而减小。</p><p>上置信界算法（Upper Confidence Bound,UCB）是一种经典的<strong>基于不确定性的策略</strong>算法，它的思想用到了一个非常著名的数学原理：霍夫丁不等式。</p><p>UCB算法在每次选择拉杆前，<strong>先估计每根拉杆的期望奖励的上界</strong>，使得拉动每根拉杆的期望奖励只有一个较小的概率超过这个上界，<strong>接着选出期望奖励上界最大的拉杆</strong>，从而选择最有可能获得最大期望奖励的拉杆。</p><h3 id="汤普森采样算法">2.5 汤普森采样算法</h3><p>汤普森采样算法使用采样的方式，即根据当前每个动作 <spanclass="math inline">\(a\)</span>的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。</p><p>怎样得到当前每个动作<spanclass="math inline">\(a\)</span>的奖励概率分布并且在过程中进行更新？在实际情况中，我们通常用 Beta分布对当前每个动作的奖励概率分布进行建模。例如若某个拉杆被拉取了 <spanclass="math inline">\(K\)</span> 次，其中 <spanclass="math inline">\(m_1\)</span> 次奖励为 1，<spanclass="math inline">\(m_2\)</span> 次奖励为0，那么该动作的奖励服从参数为 <span class="math inline">\((m_1+1,m_2+1)\)</span> 的 Beta 分布。</p><h3 id="总结">2.6 总结</h3><ol type="1"><li><spanclass="math inline">\(\epsilon\)</span>-贪婪算法的累计懊悔是随时间线性增长的，而<spanclass="math inline">\(\epsilon\)</span>-衰减贪心算法，上置信界算法，汤普森采样算法，累计懊悔都是对数形式增长的。</li><li>MAB可以看作是一个<strong>无状态的强化学习</strong>，在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习——  1 初探强化学习</title>
      <link href="/2024/09/17/RL/1%20%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/09/17/RL/1%20%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="初探强化学习">1 初探强化学习</h2><p>本笔记基于<a href="https://hrl.boyuai.com/">动手学强化学习</a></p><p>实现<strong>序贯决策</strong>的机器学习方法就是当前讨论的主题—强化学习</p><h3 id="什么是强化学习">1.1 什么是强化学习</h3><p>强化学习是机器通过与环境交互来实现目标的一种计算方法。在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。</p><p>智能体有3种关键要素，即<strong>感知、决策和奖励</strong>。</p><h3 id="环境">1.2 环境</h3><p>强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。对于一个随机过程，其最关键的要素就是<strong>状态</strong>以及<strong>状态转移的条件概率分布</strong>。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。</p><p>如果在环境这样一个自身演变的随机过程中加入一个外来的干扰因素，即智能体的动作，那么环境的<strong>下一刻状态的概率分布</strong>将由<strong>当前状态</strong>和<strong>智能体的动作</strong>来共同决定，用最简单的数学公式表示则是：</p><p><span class="math display">\[下一状态=P(当前状态,智能体动作)\]</span></p><h3 id="目标">1.3 目标</h3><p>在强化学习中，我们关注<strong>回报的期望</strong>，并将其定义为<strong>价值（value）</strong>，这就是强化学习中智能体学习的优化目标。</p><h3 id="数据">1.4 数据</h3><p>数据是在智能体与环境交互的过程中得到的。</p><h4 id="占用度量">1.4.1 占用度量</h4><p>强化学习中有一个关于数据分布的概念，叫作<strong>占用度量（occupancymeasure）</strong>，其具体的数学定义和性质会在第3章讨论，在这里我们只做简要的陈述：</p><p>归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，<strong>采样到一个具体的状态动作对（state-actionpair）的概率分布</strong>。</p><p>占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。</p><p>根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。</p><ul><li>强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，<strong>强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的</strong>。</li><li>由于奖励建立在状态动作对之上，<strong>一个策略对应的价值其实就是一个占用度量下对应的奖励的期望</strong>，因此<strong>寻找最优策略对应着寻找最优占用度量</strong>。</li></ul><h3 id="监督学习和强化学习的区别">1.5 监督学习和强化学习的区别</h3><p><strong>监督学习</strong>：目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在<strong>训练数据独立同分布</strong>的假设下，这个优化目标表示最小化模型在整个数据分布上的<strong>泛化误差</strong>。</p><p><strong>强化学习</strong>：目标是<strong>最大化</strong>智能体策略在和动态环境交互过程中的<strong>价值</strong>。策略的价值可以等价转换成<strong>奖励函数在策略的占用度量上的期望</strong>。</p><p>监督学习和强化学习的优化目标相似，即都是在<strong>优化某个数据分布下的一个分数值的期望</strong>。但是二者<strong>优化的途径不同</strong>，监督学习直接通过优化<strong>模型对于数据特征的输出</strong>来优化目标，即<strong>修改目标函数而数据分布不变</strong>；强化学习则通过<strong>改变策略来调整智能体和环境交互数据的分布</strong>，进而优化目标，即<strong>修改数据分布而目标函数不变</strong>。</p><p>总结一下，强化学习和监督学习的区别在于：</p><ul><li>监督学习关注寻找一个<strong>模型</strong>，使其在<strong>给定数据分布下</strong>得到的损失函数的期望最小；</li><li>强化学习关注寻找一个<strong>智能体策略</strong>，使其在与动态环境交互的过程中<strong>产生最优的数据分布</strong>，即最大化该分布下一个给定奖励函数的期望。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制(Attention)</title>
      <link href="/2024/08/08/NLP/Attention/"/>
      <url>/2024/08/08/NLP/Attention/</url>
      
        <content type="html"><![CDATA[<h1 id="注意力机制">注意力机制</h1><h2 id="self-attention">1 Self-Attention</h2><p>Self-Attention是一种注意力机制，它允许模型在输入序列中的不同位置之间建立依赖关系。</p><figure><img src="./images/selfa-mha.webp#80x80"title="图1：Self-Attention and Multi-Head Attention"alt="Self-Attention" /><figcaption aria-hidden="true">Self-Attention</figcaption></figure><p>Self-Attention 机制的计算如下：</p><p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}} +\text{mask})V\]</span></p><h2 id="multi-head-attention">2 Multi-Head Attention</h2><p>多头注意力相比单一的自注意力，通过并行计算多个头的注意力分数，能够从多个子空间中提取信息，捕捉到更加丰富和复杂的特征，极大地提高了模型的表达能力和鲁棒性。</p><h3 id="mha变种">2.1 MHA变种</h3><p>为了加快注意力计算速度，通常会采用KV-Cache，但是随着上下文窗口或批量大小的增加，<strong>多头注意力(MHA)模型中与 KV 缓存大小相关的内存成本显著增长</strong></p><p>对于较大的模型，KV缓存大小成为瓶颈，键和值投影可以在多个头之间共享，而不会大幅降低性能，可以使用</p><ol type="1"><li>具有单个 KV投影的多查询注意(MQA)：只使用一个键值头，虽大大加快了解码器推断的速度，但MQA可能导致质量下降</li><li>具有多个 KV投影的分组查询注意力(GQA)：通过折中(多于一个且少于查询头的数量，比如4个)键值头的数量，使得经过训练的GQA以与MQA相当的速度达到接近多头注意力的质量</li></ol><p>GQA 变体在大多数评估任务上的表现与 MHA 基线相当，并且平均优于 MQA变体</p><figure><img src="./images/MHA_GQA_MQA.webp" title="图1：MHA_GQA_MQA"alt="MHA_GQA_MQA" /><figcaption aria-hidden="true">MHA_GQA_MQA</figcaption></figure><h3 id="gqa">2.2 GQA</h3><p>GQA的分组数是一个超参数，组数越大越接近MHA，推理延迟越大，同时模型精度也越高(原论文中当组数量从1逐渐上升到8时，模型推理的开销并没有明显的增长，在8以后推理开销显著变大)。MQA略微损失了模型精度，但是确实能够大幅降低推理开销，而如果选择了合适的分组数，GQA能够两者皆得。</p><p>在理论层，MQA和GQA对推理的帮助主要是以下两点</p><ol type="1"><li><strong>降低内存读取模型权重的时间开销</strong>：由于Key矩阵和Value矩阵数量变少了，因此权重参数量也减少了，需要读取到内存的数量量少了，因此减少了读取权重的等待时间</li><li><strong>KV-Cache空间占用降低</strong>：KV-Cache需要存储的参数量降低了head_num倍，从而提高KV-Cache的读写效率；另一方面，可以有空间来增大batch_size，从而提高模型推理的吞吐量</li></ol><p>注意<strong>MQA和GQA并没有降低Attention的计算量（FLOPs）</strong>，因为Key、Value映射矩阵会以广播变量的形式拓展到和MHA和一样，因此计算量不变，只是Key、Value参数共享。</p><h4 id="mqa">2.3 MQA</h4><p>MQA 让所有的头之间 共享 同一份 Key 和 Value矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value矩阵的参数量。但是，MQA 会导致模型性能和表达能力下降。</p><h2 id="cross-attention">3 Cross-Attention</h2><p>Cross-Attention是一种注意力机制，它允许模型在<strong>不同序列</strong>之间建立依赖关系。它的查询（Query）来自解码器，而键（Key）和值（Value）来自编码器。这种机制允许解码器在生成输出时，参考编码器处理后的输入信息。除此之外和Self-Attention 机制类似。</p><h2 id="其他">4 其他</h2><ol type="1"><li>mask取全1就对应双向注意力，mask取下三角矩阵就对应单向注意力</li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>计算图(Computation Graph)</title>
      <link href="/2024/08/08/NLP/CompGraph/"/>
      <url>/2024/08/08/NLP/CompGraph/</url>
      
        <content type="html"><![CDATA[<h1 id="计算图">计算图</h1><h2 id="什么是计算图">1. 什么是计算图</h2><p>计算图是由节点和边构成的有向图（Graph），其中：</p><ul><li>节点（Nodes）：表示操作（例如加法、乘法、激活函数等）或变量（例如输入、权重、偏置等）。</li><li>边（Edges）：表示数据在节点之间的流动，即操作的输入和输出之间的依赖关系。</li></ul><p>在计算图中，每个节点都对应一个操作或变量，边则表示这些操作之间的依赖顺序。</p><h2 id="计算图的优势">2. 计算图的优势</h2><ol type="1"><li>自动微分:能够自动计算复杂函数的梯度(链式法则)</li><li>并行计算:可以识别独立的操作并并行执行</li><li>优化:通过图重写、操作融合、内存优化等技术优化计算效率<ol type="1"><li>操作融合：将多个小操作合并为一个更大的操作，以减少数据传输和中间结果的存储需求，如y= 2x + 3x可优化为y = 5x</li><li>内存优化：通过在计算图中智能地分配和释放内存，减少内存占用</li><li>利用计算图的结构，框架可以识别出独立的操作，并在多个处理器或 GPU上并行执行这些操作</li></ol></li></ol><h2 id="计算图的构建">3. 计算图的构建</h2><p>前向计算的过程大概如下</p><figure><img src="./images/ForwardCal.webp" title="Forward Calculation"alt="Forward Calculation" /><figcaption aria-hidden="true">Forward Calculation</figcaption></figure><p>根据链式法则，反向计算的过程大概如下</p><figure><img src="./images/BackwardCal.webp" title="Backward Calculation"alt="Backward Calculation" /><figcaption aria-hidden="true">Backward Calculation</figcaption></figure><ol type="1"><li>先按着计算图往前计算得到各节点的值；</li><li>然后反方向计算出各直接连接的节点间（如x和z、y和z）的偏导；</li><li>最后对有效路径求和，即可得到r对a、b、c、d、e的偏导。</li></ol><h2 id="计算图的实现">4. 计算图的实现</h2><p>计算图的实现一般分为两种：</p><ol type="1"><li>静态计算图：在计算图构建完成后，图的结构和节点的计算顺序都是固定的，如TensorFlow 的计算图。</li><li>动态计算图：在计算图构建的过程中，可以根据需要动态地构建计算图的结构和节点的计算顺序，如PyTorch 的计算图。</li><li>混合计算图：结合了静态计算图和动态计算图的优点，如 TensorFlow 2.0的计算图。</li></ol><h2 id="总结">5. 总结</h2><p>计算图是深度学习框架的核心，它能够自动微分、并行计算、优化计算效率等，是实现深度学习算法的基础。</p><h2 id="参考文献">6. 参考文献</h2><ol type="1"><li><ahref="https://zhuanlan.zhihu.com/p/412542969">计算图的概念与理解</a></li><li><ahref="https://blog.csdn.net/wohu1104/article/details/106911071">浅显易懂的计算图、链式法则讲解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM 参数量及 FLOPs 计算</title>
      <link href="/2024/08/08/NLP/FLOPs/"/>
      <url>/2024/08/08/NLP/FLOPs/</url>
      
        <content type="html"><![CDATA[<h1 id="llm-参数量flops计算和中间激活值的存储">LLM参数量、FLOPs计算和中间激活值的存储</h1><p>分析基于Decoder-only的LLM框架</p><table><thead><tr><th style="text-align: center;">参数</th><th style="text-align: center;">符号</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr><td style="text-align: center;">Decoder层数</td><td style="text-align: center;">l</td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">Token嵌入维度</td><td style="text-align: center;">d</td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">Attention层嵌入维度</td><td style="text-align: center;">d</td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">MLP隐藏层维度</td><td style="text-align: center;">4d</td><td style="text-align: center;">通常设置为嵌入维度4倍</td></tr><tr><td style="text-align: center;">Attention head数量</td><td style="text-align: center;">n</td><td style="text-align: center;">要求其整除d</td></tr><tr><td style="text-align: center;">词表尺寸</td><td style="text-align: center;">V</td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">batch_size</td><td style="text-align: center;">b</td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">模型输入长度</td><td style="text-align: center;">s</td><td style="text-align: center;"></td></tr></tbody></table><h2 id="模型参数量">1. 模型参数量</h2><ol type="1"><li>Embedding层：<span class="math inline">\([V, d]\)</span> —— <spanclass="math inline">\(Vd\)</span></li><li>Self-Attention层(通常是没有带偏置项) —— <spanclass="math inline">\(4d^2 (+ 4d)\)</span><ul><li>Q K V 矩阵： <span class="math inline">\([d, d]\)</span> —— <spanclass="math inline">\(3d^2\)</span></li><li>O 矩阵：<span class="math inline">\([d, d]\)</span> —— <spanclass="math inline">\(d^2\)</span></li><li>如果带偏置，每个都是<span class="math inline">\(d\)</span> —— <spanclass="math inline">\(4d\)</span></li></ul></li><li>MLP层(通常带偏置项) —— <span class="math inline">\(8d^2 +5d\)</span><ul><li>X-&gt;H：<span class="math inline">\([d, 4d]\)</span> —— <spanclass="math inline">\(4d^2+4d\)</span></li><li>H-&gt;O：<span class="math inline">\([4d, d]\)</span> —— <spanclass="math inline">\(4d^2+d\)</span></li></ul></li><li>两个LayerNorm层 —— <span class="math inline">\(2 * 2d\)</span><ul><li>缩放参数 Scale：<span class="math inline">\([d]\)</span> —— <spanclass="math inline">\(d\)</span></li><li>平移参数 Bias：<span class="math inline">\([d]\)</span> —— <spanclass="math inline">\(d\)</span></li></ul></li></ol><table><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单个参数量</th><th style="text-align: center;">总参数量</th></tr></thead><tbody><tr><td style="text-align: center;">Embedding</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(Vd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(Vd\)</span></td></tr><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><span class="math inline">\(4d^2 (+4d)\)</span></td><td style="text-align: center;"><span class="math inline">\(4ld^2 (+4ld)\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><span class="math inline">\(8d^2 +5d\)</span></td><td style="text-align: center;"><span class="math inline">\(8ld^2 +5ld\)</span></td></tr><tr><td style="text-align: center;">LayerNorm</td><td style="text-align: center;">2l</td><td style="text-align: center;"><spanclass="math inline">\(2d\)</span></td><td style="text-align: center;"><spanclass="math inline">\(4ld\)</span></td></tr></tbody></table><p>总参数量：<span class="math inline">\(Vd + 12ld^2 + 9ld (+4ld)\)</span> 近似于 <span class="math inline">\(12ld^2\)</span></p><p>比如LLaMa-7B，<span class="math inline">\(V=128k, d=4096,l=32\)</span></p><ul><li>总参数量为<span class="math inline">\(128k*4096 + 12*32*4096^2 +9*32*4096 \approx 6.98B\)</span></li><li>粗略计算为<span class="math inline">\(12*32*4096^2 \approx6.44B\)</span></li></ul><h2 id="显存占用">2. 显存占用</h2><h3 id="训练过程">2.1 训练过程</h3><p>在训练神经网络的过程中，占用显存的大头主要分为四部分：</p><ol type="1"><li>模型参数</li><li>前向计算过程中产生的中间激活（中间激活的显存占用后面会详细介绍）</li><li>反向传递计算得到的梯度</li><li>优化器状态</li></ol><p>训练大模型时通常会采用AdamW优化器，并用混合精度训练来加速训练，基于这个前提分析显存占用。</p><p>在一次训练迭代中，每个可训练模型参数都会对应1个梯度，并对应2个优化器状态（Adam优化器梯度的一阶动量和二阶动量）。float16数据类型的元素占2个bytes，float32数据类型的元素占4个bytes。在混合精度训练中，会使用float16的模型参数进行前向传递和后向传递，计算得到float16的梯度；在优化器更新模型参数时，会使用float32的优化器状态、float32的梯度、float32的模型参数来更新模型参数。</p><p>设模型参数为<span class="math inline">\(N\)</span>，则显存占用为<spanclass="math inline">\(20N\)</span> bytes（不包括中间激活）。</p><ol type="1"><li>模型参数：<span class="math inline">\(2N\)</span> bytes (float16) +<span class="math inline">\(4N\)</span> bytes (float32)</li><li>梯度：<span class="math inline">\(2N\)</span> bytes (float16) +<span class="math inline">\(4N\)</span> bytes (float32)</li><li>优化器状态：<span class="math inline">\(2 \times 4N\)</span> bytes(float32)</li></ol><h3 id="推理过程">2.2 推理过程</h3><p>不需要存储梯度和优化器状态，也无需存储中间激活值，只需要存储模型参数，显存占用为<spanclass="math inline">\(2N\)</span> bytes (采用float16推理)。注：如果启用KV缓存，则需要额外存储KV缓存。</p><h2 id="flops">3. FLOPs</h2><p>LLM 中的主要运算是矩阵乘法，故考察 LLM计算量时，<strong>通常只关注矩阵乘法运算对应的浮点计算量</strong> <spanclass="math inline">\(m \times n\)</span> 矩阵乘以 <spanclass="math inline">\(n \times k\)</span> 矩阵(<em>n-1次加法和n次乘法</em> )的运算量为 <spanclass="math inline">\(mk(2n-1)\)</span>，但是 GPU 计算矩阵乘法时一般使用FMA (fused multiply–add) 进行计算，一次 FMA可以计算一个乘法和一个加法，因此实际的 FLOPs 计算量为 <spanclass="math inline">\(mk(2n)\)</span></p><p>假设输入的batch为<span class="math inline">\(b \times s \timesd\)</span>，其中<spanclass="math inline">\(b\)</span>为batch_size，<spanclass="math inline">\(s\)</span>为输入长度，<spanclass="math inline">\(d\)</span>为嵌入维度</p><ol type="1"><li>Embedding层：查表操作，不涉及矩阵乘法，故不计入FLOPs</li><li>预测多分类头(logits)将尺寸为 d 的隐藏向量映射为词表大小：<spanclass="math inline">\([b \times s \times d] \times [d \times V] = [b\times s \times V]\)</span> —— <spanclass="math inline">\(2bsVd\)</span></li><li>Self-Attention层：<span class="math inline">\(8bsd^2+4bs^2d\)</span><span class="math display">\[     Q=xW_{Q}, K=xW_{K}, V=xW_{V}\\     Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\\     x_{out}= AW_{O}+x\]</span><ul><li>计算 QKV 矩阵：<span class="math inline">\([b \times s \times d]\times [d \times d] = [b \times s \times d]\)</span> —— <spanclass="math inline">\(6bsd^2\)</span></li><li>计算注意力权重<span class="math inline">\(QK^T\)</span>：<spanclass="math inline">\([b \times n \times s \times d/n] \times [b \timesn \times d/n \times s] = [b \times n \times s \times s]\)</span> ——<span class="math inline">\(2bs^2d\)</span></li><li>汇聚价值信息<span class="math inline">\(AV\)</span>: <spanclass="math inline">\([b \times n \times s \times s] \times [b \times n\times s \times d/n] = [b \times n \times s \times d/n]\)</span> ——<span class="math inline">\(2bs^2d\)</span></li><li>拼接多头注意力: 不涉及矩阵乘法，故不计入FLOPs</li><li>输出矩阵<span class="math inline">\(O\)</span>: <spanclass="math inline">\([b \times s \times d] \times [d \times d] = [b\times s \times d]\)</span> —— <spanclass="math inline">\(2bsd^2\)</span></li></ul></li><li>MLP层：<span class="math inline">\(16bsd^2\)</span> <spanclass="math display">\[     h=Relu(x_{out}W_1+b_1)\\     h_{out}=hW_2+b2\\     x=h_{out}+x_{out}\]</span><ul><li>第一个线性层：<span class="math inline">\([b \times s \times d]\times [d \times 4d] = [b \times s \times 4d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li><li>第二个线性层：<span class="math inline">\([b \times s \times 4d]\times [4d \times d] = [b \times s \times d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li></ul></li><li>反向传播：反向传播过程中每个非第一层都有两次矩阵乘法操作，而相应的前向过程中只有一次（第一层的后向-前向FLOPs比率是1:1，而其他层后向-前向FLOPs比率是2:1）。随着网络层数增加，反向传播计算量会越来越接近正向传播计算量的两倍。</li></ol><table><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单次FLOPs</th><th style="text-align: center;">总FLOPs</th></tr></thead><tbody><tr><td style="text-align: center;">Embedding</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr><td style="text-align: center;">LM Head(logits)</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td></tr><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(8bsd^2+4bs^2d\)</span></td><td style="text-align: center;"><spanclass="math inline">\(8blsd^2+4bls^2d\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(16bsd^2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(16blsd^2\)</span></td></tr></tbody></table><p>前向传播总FLOPs：<span class="math inline">\(2bsVd + 24blsd^2 +4bls^2d\)</span>，近似为<span class="math inline">\(24blsd^2\)</span>加上反向传播的FLOPs，一次训练迭代的总计算量为<spanclass="math inline">\(72blsd^2\)</span></p><h3 id="计算量和参数量的关系">3.1 计算量和参数量的关系</h3><p>进一步考虑Token数据量为<spanclass="math inline">\(D=bs\)</span>，参数量为<spanclass="math inline">\(N=12ld^2\)</span></p><ul><li>推理：<span class="math inline">\(2DN\)</span></li><li>训练：<span class="math inline">\(6DN\)</span></li></ul><p>可以近似认为：在一次推理中，对于每个token，每个模型参数，需要进行2次浮点数运算，即一次乘法法运算和一次加法运算，而在一次训练中，需要进行6次浮点数运算。</p><h3 id="估计训练时间">3.2 估计训练时间</h3><p>训练神经网络的一次迭代分为三步</p><ol type="1"><li>前向传递计算损失函数；</li><li>后向传递计算梯度；</li><li>优化器更新模型参数。</li></ol><p>后向传递的耗时几乎是前向传递的两倍，相比之下，优化器更新的耗时几乎可以忽略。进一步的</p><p><span class="math display">\[训练时间 \approx \frac{6DN}{FLOPS \times {GPU 数量} \times {GPU利用率}}\]</span></p><p>如果使用激活重计算技术来减少中间激活显存需要进行一次额外的前向传递，则</p><p><span class="math display">\[训练时间 \approx \frac{8DN}{FLOPS \times {GPU 数量} \times {GPU利用率}}\]</span></p><p>一般来讲，GPU利用率一般在<spanclass="math inline">\(0.3-0.55\)</span>之间。</p><h2 id="中间激活值的存储">4 中间激活值的存储</h2><p><strong>前向传递过程中计算得到的，并在后向传递过程中需要用到的所有张量</strong>(不包含模型参数和优化器状态，但包含dropout操作需要用到的mask矩阵)</p><p>大模型在训练过程中通常采用混合精度训练，中间激活值一般是float16或者bfloat16数据类型的。在分析中间激活的显存占用时，<strong>假设中间激活值是以float16或bfloat16数据格式来保存的，每个元素占了2个bytes。唯一例外的是，dropout操作的mask矩阵，每个元素只占1个bytes。</strong></p><ol type="1"><li>Self-Attention层：<span class="math inline">\(11bsd+5bs^2n\)</span><ol type="1"><li>对于QKV，需要保存输入x：<span class="math inline">\([b \times s\times d]\)</span> —— <span class="math inline">\(2bsd\)</span></li><li>对于<spanclass="math inline">\(QK^T\)</span>，需要保存Q和K矩阵：<spanclass="math inline">\([b \times s \times d]\)</span> —— <spanclass="math inline">\(4bsd\)</span></li><li>对于Softmax，需要保存<spanclass="math inline">\(QK^T\)</span>：<span class="math inline">\([b\times n \times s \times s]\)</span> —— <spanclass="math inline">\(2bs^2n\)</span></li><li>会有一个dropout操作，需要保存mask矩阵，与<spanclass="math inline">\(QK^T\)</span>尺寸相同：<spanclass="math inline">\([b \times n \times s \times s]\)</span> —— <spanclass="math inline">\(bs^2n\)</span></li><li>之后计算A，需要保存V矩阵和softmax：<span class="math inline">\([b\times s \times d]\)</span>和<span class="math inline">\([b \times n\times s \times s]\)</span> —— <span class="math inline">\(2bsd +2bs^2n\)</span></li><li>计算输出矩阵O，需要保存A：<span class="math inline">\([b \times s\times d]\)</span> —— <span class="math inline">\(2bsd\)</span></li><li>会有一个dropout操作，需要保存mask矩阵，与A尺寸相同：<spanclass="math inline">\([b \times s \times d]\)</span> —— <spanclass="math inline">\(bsd\)</span></li></ol></li><li>MLP层：<span class="math inline">\(19bsd\)</span><ol type="1"><li>对于第一个线性层，需要保存输入x：<span class="math inline">\([b\times s \times d]\)</span> —— <spanclass="math inline">\(2bsd\)</span></li><li>激活函数需要保存输入：<span class="math inline">\([b \times s \times4d]\)</span> —— <span class="math inline">\(8bsd\)</span></li><li>对于第二个线性层，需要保存输入h：<span class="math inline">\([b\times s \times 4d]\)</span> —— <spanclass="math inline">\(8bsd\)</span></li><li>会有一个dropout操作，需要保存mask矩阵，与h尺寸相同：<spanclass="math inline">\([b \times s \times d]\)</span> —— <spanclass="math inline">\(bsd\)</span></li></ol></li><li>两个LayerNorm层：<span class="math inline">\(2 \times 2bsd\)</span>保存输入：<span class="math inline">\([b \times s \times d]\)</span> ——<span class="math inline">\(2bsd\)</span></li></ol><table><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单个占用</th><th style="text-align: center;">总占用</th></tr></thead><tbody><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(11bsd+5bs^2n\)</span></td><td style="text-align: center;"><spanclass="math inline">\(11blsd+5bls^2n\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(19bsd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(19blsd\)</span></td></tr><tr><td style="text-align: center;">LayerNorm</td><td style="text-align: center;">2l</td><td style="text-align: center;"><spanclass="math inline">\(2bsd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(4blsd\)</span></td></tr></tbody></table><p>中间激活值总显存占用：<spanclass="math inline">\(34blsd+5bls^2n\)</span> bytes</p><h3 id="中间激活值和模型参数的关系">4.1 中间激活值和模型参数的关系</h3><p>一次训练中，占用显存的四大部分中，模型参数、梯度和优化器状态的显存占用是与模型参数量成正比的，与输入数据量无关；而<strong>中间激活值的显存占用是与输入数据量（批次大小b和序列长度s）成正相关的</strong>。所以当训练时出现显存不足的情况时，可以通过减少batch_size来减少中间激活值的显存占用。</p><p>随着批次大小 b的增大，中间激活占用的显存远远超过了模型参数显存。通常会采用激活重计算技术来减少中间激活，理论上可以将中间激活显存从<spanclass="math inline">\(O(n)\)</span>降低到<spanclass="math inline">\(O(\sqrt{n})\)</span>(每<spanclass="math inline">\(\sqrt{n}\)</span>层存储一个检查点)。</p><p>激活重计算本质是<strong>时间换空间</strong>，通过在反向传播时重新计算部分激活值，而不是在前向传播时存储所有激活值，从而显著减少内存使用。</p><ol type="1"><li>前向传播阶段：在前向传播中，不存储所有层的激活值。仅存储一些关键的检查点（checkpoints），即在若干层之后保存一次激活值。</li><li>反向传播阶段：当需要计算梯度时，重新计算未存储的激活值。这意味着在反向传播阶段需要重新执行一部分前向传播计算。</li><li>内存与计算的权衡：通过减少激活值的存储来节省内存，但代价是增加了计算开销，因为需要重新计算激活值。</li></ol><p><img src="./images/vanilla-backprop.webp#50x50"title="图1: 原中间激活计算" alt="原中间激活计算" /> <imgsrc="./images/checkpointed-backprop.webp#50x50" title="图2: 激活重计算"alt="激活重计算" /></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>词嵌入(Embedding)</title>
      <link href="/2024/08/08/NLP/Embedding/"/>
      <url>/2024/08/08/NLP/Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="词嵌入embedding">词嵌入(Embedding)</h1><h2 id="位置编码positional-encoding">1 位置编码(PositionalEncoding)</h2><p>位置编码是Transformer模型中的一部分，用于为输入序列中的每个位置提供一个位置向量，提供时序信息。</p><h3 id="标准位置编码">1.1 标准位置编码</h3><p>如果输入序列长度为<code>seq_len</code>，词嵌入的维度为<code>d_model</code>。位置编码的计算公式如下：</p><p><span class="math display">\[PE_{(pos, 2i)} =\sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\\PE_{(pos, 2i+1)} =\cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\]</span></p><p>其中，<spanclass="math inline">\(pos\)</span>表示token在序列中的位置，<spanclass="math inline">\(i\)</span>表示维度。位置编码的维度与词嵌入的维度相同。偶数维度用<code>sin</code>函数编码，奇数维度用<code>cos</code>函数编码。</p><p>对于一个序列，位置编码的维度为<code>[seq_len, d_model]</code>。词嵌入和位置编码相加，作为模型的输入。</p><p>比如对于其中一个token，就是一个<code>[d_model]</code>的一维向量，对位相加一个<code>[d_model]</code>的一维位置编码向量。</p><h3 id="旋转位置编码rope">1.2 旋转位置编码(ROPE)</h3><p>ROPE（Rotary PositionEmbedding）通过对词向量进行旋转变换来编码位置信息,而不是像传统方法那样将位置编码直接加到词向量上。</p><p>如果输入序列长度为<code>seq_len</code>，词嵌入的维度为<code>d_model</code>。RoPE的计算公式如下：</p><ol type="1"><li>复数形式 <span class="math display">\[\text{RoPE}(x_m, m) = x_m \cdot e^{im\theta}\]</span></li><li>基本形式 <span class="math display">\[\theta_{pos,i} = \frac{pos}{10000^{\frac{2i}{d}}}\\\text{RoPE}(x, pos)_{2i} = x_{2i} \cos(\theta_{pos,i}) + x_{2i+1}\sin(\theta_{pos,i})\\\text{RoPE}(x, pos)_{2i+1} = x_{2i+1} \cos(\theta_{pos,i}) - x_{2i}\sin(\theta_{pos,i})\]</span></li><li>矩阵形式 <span class="math display">\[R_{\theta,pos,2i} = \begin{bmatrix}\cos(\theta_{pos,i}) &amp; -\sin(\theta_{pos,i}) \\\sin(\theta_{pos,i}) &amp; \cos(\theta_{pos,i})\end{bmatrix}\\\begin{bmatrix}x&#39;_{2i} \\x&#39;_{2i+1}\end{bmatrix} =\begin{bmatrix}x_{2i} \\x_{2i+1}\end{bmatrix} \cdot R_{\theta,pos,2i}\]</span></li></ol><h3 id="线性注意力偏置alibi">1.3 线性注意力偏置(ALiBi)</h3><p>不同于传统方法，ALiBi不直接修改tokenembeddings，而是在注意力计算过程中添加一个线性偏置项，这个偏置项随着序列中tokens的相对距离线性增加。</p><p>[(Q, K, V) = ( + B) V]</p><p>其中，位置偏置矩阵 ( B ) 定义为：</p><p>[B_{ij} = -|i - j| m]</p><h2 id="词向量word2vec">2 词向量(Word2Vec)</h2><p><strong>上下文相似的两个词，它们的词向量也应该相似</strong>，比如香蕉和梨在句子中可能经常出现在相同的上下文中，因此这两个词的表示向量应该就比较相似。</p><p>Word2Vec是一种词嵌入技术，它的目的是将词语映射到一个低维空间中，使得语义相近的词在这个空间中的距离也比较近。核心思想是通过训练神经网络，使得词向量的内积尽可能的大，而与之不相关的词向量的内积尽可能的小。</p><p>Word2Vec模型有两种：</p><ol type="1"><li>CBOW: 通过上下文预测中心词，即用<span class="math inline">\(w_{t-1},w_{t-2}, w_{t+1}, w_{t+2}\)</span>预测<spanclass="math inline">\(w_t\)</span></li><li>Skip-gram: 通过中心词预测上下文，即用<spanclass="math inline">\(w_t\)</span>预测<spanclass="math inline">\(w_{t-1}, w_{t-2}, w_{t+1}, w_{t+2}\)</span></li></ol><figure><img src="./images/Word2Vec.webp#80x80" title="图1：Word2Vec"alt="Word2Vec" /><figcaption aria-hidden="true">Word2Vec</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>微调(Fine-Tuning)</title>
      <link href="/2024/08/08/NLP/FineTuning/"/>
      <url>/2024/08/08/NLP/FineTuning/</url>
      
        <content type="html"><![CDATA[<h1 id="微调fine-tuning">微调(Fine-Tuning)</h1><h2 id="开源基座模型">1 开源基座模型</h2><p>大语言模型的训练分为两个阶段：（1）在海量文本语料上的无监督预训练，学习通用的语义表示和世界知识。（2）在小规模数据上，进行指令微调和基于人类反馈的强化学习，更好地对齐最终任务和人类偏好。几乎所有知识都是在预训练过程中学习到的，只需要有限的指令微调数据就可以生成高质量的回复。因此，基座模型的性能是至关重要的，如果基座模型的性能不够好，指令微调和强化学习也难以取得很好的效果。</p><p>主流的开源大语言模型主要有三个：LLaMA、ChatGLM和BLOOM。</p><table><thead><tr><th>模型</th><th>模型结构</th><th>位置编码</th><th>激活函数</th><th>layer norm</th></tr></thead><tbody><tr><td>LLaMA</td><td>Casual decoder</td><td>RoPE</td><td>SwiGLU</td><td>Pre RMS Norm</td></tr><tr><td>ChatGLM-6B</td><td>Prefix decoder</td><td>RoPE</td><td>GeGLU</td><td>Post Deep Norm</td></tr><tr><td>Bloom</td><td>Casual decoder</td><td>ALiBi</td><td>GeLU</td><td>Pre Layer Norm</td></tr></tbody></table><h3 id="模型对比">模型对比</h3><h4 id="词表扩充">1) 词表扩充</h4><p>LLaMA原模型的词表大小是32000，tokenizer主要是在英文语料上进行训练的，在中文上和多语种上效果比较差。</p><ol type="1"><li>LLaMA模型是在以英文为主的拉丁语系语料上进行训练的，训练语料不包含中文；</li><li>与tokenizer有关，词表规模小，可能将一个汉字切分为多个token，编码效率低，模型学习难度大。</li></ol><p>扩展中文词表后，单个汉字倾向于被切分为1个token，避免了一个汉字被切分为多个token的问题，提升了中文编码效率。</p><ol type="1"><li>在中文语料上使用Sentence Piece训练一个中文tokenizer。</li><li>将中文tokenizer与原始的 LLaMAtokenizer合并起来，通过组合二者的词汇表，最终获得一个合并的tokenizer，称为ChineseLLaMA tokenizer。</li><li>为了适应新的tokenizer，将transformer模型的embedding矩阵从<spanclass="math inline">\(V \times h\)</span> 扩展到 <spanclass="math inline">\(V&#39; \timesh\)</span>，新加入的中文token附加到原始embedding矩阵的末尾，确保原始词表表的embedding矩阵不受影响。</li><li>在中文语料上进一步预训练，冻结和固定transformer的模型参数，只训练embedding矩阵，学习新加入中文token的词向量表示，同时最小化对原模型的干扰。</li><li>在指令微调阶段，可以放开全部模型参数进行训练。</li></ol><h4 id="模型结构">2) 模型结构</h4><p>主流模型都采用decoder-only结构，其中LLaMA和BLOOM采用了casualdecoder，ChatGLM采用了prefix decoder。</p><figure><img src="./images/Model_Structure.webp" title="图1：模型结构"alt="模型结构" /><figcaption aria-hidden="true">模型结构</figcaption></figure><ol type="1"><li>casualdecoder：采用<strong>单向注意力</strong>掩码，以确保每个输入标记只能关注过去的标记和它本身。</li><li>prefixdecoder：对前缀标记执行<strong>双向注意力</strong>，并仅对生成的标记执行单向注意力。这样，与encoder-decoder类似，可以双向编码前缀序列并自回归的逐个预测输出标记，其中在编码和解码阶段共享相同的参数。</li><li>attention mask不同，prefix LM的prefix部分的token互相能看到，causalLM严格遵守只有后面的token才能看到前面的token的规则。</li><li>prefix decoder-only结构的ChatGLM存在一个劣势：训练效率低。causaldecoder结构会在所有的token上计算损失，而prefixdecoder只会在输出上计算损失，而不计算输入上的损失。在有相同数量的训练tokens的情况下，prefixdecoder要比causaldecoder的效果差，因为训练过程中实际用到的tokens数量要更少。</li></ol><p><strong>注</strong>：ChatGPT的成功已经证明了causaldecoder结构的大语言模型可以获得非常好的few-shot和zero-shot生成能力，通过指令微调可以进一步激发模型的能力。至于prefixdecoder结构的大语言模型能否获得相当的few-shot和zero-shot能力还缺少足够的验证。(1)zero-shot：训练集中没有某个类别的样本，但是如果我们可以学到一个映射，这个映射好到即使在训练的时候没看到这个类，但是在遇到的时候依然能通过这个映射得到这个新类的特征。(2)one-shot：模型在只见过一个训练样本的情况下，学习并完成新的任务。常见于图像识别领域。(3)few-shot：模型在见过少量（通常是几个到几十个）训练样本的情况下，学习并完成新的任务。</p><p><strong>Why decoder-only？</strong></p><p>Encoder在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到。主要作用是从输入序列中抽取特征，形成一个表示序列的上下文向量；Decoder 中因为有 mask机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息。主要作用是根据Encoder生成的上下文向量和之前已经生成的词，逐步生成目标序列中的每个词。</p><ol type="1"><li>用过去研究的经验说话，decoder-only的泛化性能更好</li><li>decoder-only支持一直复用KV-Cache，对多轮对话更友好</li><li>预训练任务难度问题，纯粹的decoder-only架构+next tokenpredicition预训练，每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高</li><li>双向attention的注意力矩阵容易退化为低秩状态，而causalattention的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强；</li><li>causalattention具有隐式的位置编码功能，打破了transformer的位置不变性，而带有双向attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。</li><li>Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。</li></ol><h4 id="layer-normalization">3) Layer Normalization</h4><p>在统计学中，协变量偏移指的是源域（S）和目标域（T）边缘分布的不一致，但是它们的条件分布却是相同的。对于深度学习来说，指的是训练集和测试集的输入特征分布不同（例如训练数据和测试数据拍摄条件不同），但在给定特征时，输出的分布是一致的。</p><ol type="1"><li>用训练集得到的模型在测试集上做性能评估，得到的不是模型的真实水平</li><li>训练集和测试集分布差异过大，我们训练得到的不是真实模型</li></ol><p>因此在机器学习中，我们期望数据是<strong>独立同分布</strong>的，这要求训练集和测试集的样本都是从同一个分布独立采样而来。但是在深层神经网络的训练中，当中间神经层的前一层参数发生改变时，该层的输入分布也会发生改变，也就是存在内部协变量偏移ICS问题。中间的神经层需要不断适应这种变化，这会降低整个网络的收敛速度。</p><p>可以通过固定每一层网络的输入值分布来减缓ICS问题:</p><ol type="1"><li>白化：通过调整输入数据的分布来减少协变量偏移的影响，比如PCA。即标准正态分布+去除相关性(协方差矩阵奇异值分解)。1）白化过程计算成本太高，如PCA中需要计算协方差矩阵，并在每一轮训练的每一层都执行该运算；2）白化过程改变了网络中每一层的分布，因此改变了网络层中本身数据的表达能力，底层网络学习到的参数信息会被白化操作丢失。</li><li>归一化：使得样本处于同一分布，且可以解决每批训练数据分布不同的问题，而且Normalization包含线性变换操作，可以让数据尽可能恢复本身的表达能力。即转换为均值为0，方差为1的标准正态分布。主要操作有：去均值、除以标准差、线性变换(缩放和平移)。</li></ol><p>常用的Normalization方法有Batch Normalization、LayerNormalization、Instance Normalization、GroupNormalization等。主要区别在于操作的特征维度不同。基本计算公式如下：</p><p><span class="math display">\[y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</span></p><p>其中，<span class="math inline">\(\gamma\)</span>和<spanclass="math inline">\(\beta\)</span>是可学习的参数，<spanclass="math inline">\(\mu\)</span>和<spanclass="math inline">\(\sigma\)</span>是均值和方差，<spanclass="math inline">\(\epsilon\)</span>是一个很小的数，防止分母为0。</p><p>假定输入数据的维度是<span class="math inline">\((N, C, H,W)\)</span>，其中<span class="math inline">\(N\)</span>是batchsize，<span class="math inline">\(C\)</span>是通道数，<spanclass="math inline">\(H\)</span>和<spanclass="math inline">\(W\)</span>是高和宽。 <strong>BatchNormalization：对整个批次的数据进行归一化。</strong> 在<spanclass="math inline">\(N H W\)</span>维度上求均值和方差，保留<spanclass="math inline">\(C\)</span>维度，对每个通道进行归一化。</p><p><span class="math display">\[\mu_c = \frac{1}{NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W}x_{nchw}\\\sigma_c^2 = \frac{1}{NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W}(x_{nchw} - \mu_c)^2 + \epsilon\\\]</span></p><p><strong>LayerNormalization：对单个样本的所有特征进行归一化。</strong> 在<spanclass="math inline">\(C H W\)</span>维度上求均值和方差，保留<spanclass="math inline">\(N\)</span>维度。</p><p><span class="math display">\[\mu_{n} = \frac{1}{CWH} \sum_{c=1}^{C} \sum_{h=1}^{H} \sum_{w=1}^{W}x_{nchw}\\\sigma_{n}^2 = \frac{1}{CWH} \sum_{c=1}^{C} \sum_{h=1}^{H}\sum_{w=1}^{W} (x_{nchw} - \mu_n)^2 + \epsilon\\\]</span></p><p><strong>InstanceNormalization：对每个样本的每个通道独立进行归一化。</strong> 在<spanclass="math inline">\(H W\)</span>维度上求均值和方差，保留<spanclass="math inline">\(N C\)</span>维度。在channel内部求均值和标准差</p><p><span class="math display">\[\mu_{nc} = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}\\\sigma_{nc}^2 = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{nchw} -\mu_{nc})^2 + \epsilon\\\]</span></p><p><strong>GroupNormalization：对每个样本的特定组别的特征进行归一化。</strong>将通道分为若干组，每组进行归一化。各组channel用其对应的归一化参数独立地归一化。</p><p><span class="math display">\[\mu_{ng} = \frac{1}{(C/G)HW} \sum_{c=1}^{C/G} \sum_{h=1}^{H}\sum_{w=1}^{W} x_{nchw}\\\sigma_{ng}^2 = \frac{1}{(C/G)HW} \sum_{c=1}^{C/G} \sum_{h=1}^{H}\sum_{w=1}^{W} (x_{nchw} - \mu_{ng})^2 + \epsilon\\\]</span></p><p><strong>总结</strong>：</p><ol type="1"><li>Batch Normalization：适用于CV领域<ol type="1"><li>处理序列数据时表现差，因为序列数据的维度是变化的，而BN是在固定维度上进行归一化的，因此不适用于RNN、LSTM等序列模型。</li><li>小batch size时效果差，因为每个batch的统计特性会有较大波动</li></ol></li><li>Layer Normalization：适用于RNN、LSTM等序列模型<ol type="1"><li>适用于序列数据，因为LN是在每个样本上进行归一化的，不受batchsize影响</li><li>适用于序列数据，因为LN是在序列长度上对每个样本进行归一化</li><li>对单个样本的所有特征进行统一归一化处理，如果这些特征来自不同的类别或具有不同的性质，可能会降低模型的表达能力。</li><li>不适应输入变化很大的数据</li></ol></li><li>Instance Normalization：图像的风格迁移<ol type="1"><li>对每个样本的每个特征进行归一化，因此它可以捕捉到更多的细节信息</li><li>可能会过度强调细节信息，忽视了更宏观的信息</li><li>计算成本较高，计算成本比BN和LN要高</li><li>不适应通道之间的相关性较强数据</li></ol></li><li>Group Normalization：适用于占用显存比较大的任务，如图像分割<ol type="1"><li>既可以捕获到Batch的统计特性，又可以捕获到样本的细节信息，是BN和IN的折中方案</li><li>对Batch size不敏感</li><li>性能取决于组的大小，计算成本比BN和LN要高</li></ol></li></ol><p>LLM采用Layer Normalization，而又可以细分为以下几种：</p><p><strong>按方法分类：</strong></p><ol type="1"><li>LayerNorm：对单个样本的所有特征进行归一化(标准正态分布+线性变换)<span class="math display">\[y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</span></li><li>RMSNorm：通过实验证明re-center操作并不重要，移除了LayerNorm中的均值项，可以看作LayerNorm在均值为0时的一个特例。(无需计算均值，提高了训练速度)<span class="math display">\[y = \gamma \frac{x}{RMS(x)}\\RMS(x) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2}\]</span></li><li>DeepNorm:不仅用作标准化，还作为残差连接的一部分，将前一层的输出与标准化后的输出相加。可以缓解爆炸式模型更新的问题。<span class="math display">\[x_{l+1}=LN(\alpha x_l + G_l(x_l, \theta_l))\]</span></li></ol><p><strong>按位置分类：</strong></p><figure><img src="./images/LN_Pos.webp"title="图2：Layer Normalization Position"alt="Layer Normalization Position" /><figcaption aria-hidden="true">Layer Normalization Position</figcaption></figure><ol type="1"><li>Post Layer Norm：在每个子层的输出后应用LayerNorm。 <strong><spanclass="math inline">\(x_{l+1} = LN(x_l +SubLayer(x_l))\)</span></strong><ol type="1"><li>Post Norm结构的最终效果是要好于Pre Norm的，只不过PostNorm要达到自己的最优效果，需要加Warmup等训练技巧，如果和PreNorm用一样的训练配置则效果不如Pre Norm。</li><li>上限高于Pre Norm，但是需要更多的训练技巧。</li><li>在深层的梯度范式逐渐增大，导致使用PostNorm的深层Transformer容易出现训练不稳定的问题</li></ol></li><li>Pre Layer Norm：在每个子层的输入前应用LayerNorm。 <strong><spanclass="math inline">\(x_{l+1} = SubLayer(LN(x_l)) +x_l\)</span></strong><ol type="1"><li>同一设置下，Pre Norm结构往往更容易训练，但是最终效果通常不如PostNorm</li><li>无形地增加了模型的宽度而降低了模型的深度，而在深度学习中深度通常比宽度更重要。也就是说PreNorm的深度有“水分”，一个L层的Pre Norm模型其实际等效层数不如L层的PostNorm模型，层数少就会导致效果变差。</li><li>相比于Post Norm，Pre Norm在深层的梯度范式近似相等，所以使用PreNorm的深层Transformer训练更加稳定</li></ol></li><li>Sandwich Norm：结合了Pre Norm和PostNorm的优点，同时在输入前和输出后应用LayerNorm。 <strong><spanclass="math inline">\(x_{l+1} = x_l +LN(SubLayer(LN(x_l)))\)</span></strong><ol type="1"><li>有效控制每一层的激活值，避免它们过大，模型能够更好地学习数据特征</li><li>训练不稳定，可能会导致训练崩溃</li></ol></li></ol><h4 id="激活函数">4) 激活函数</h4><p>FFN通常先将向量从维度d升维到中间维度4d，再从4d降维到d。计算公式如下：</p><p><span class="math display">\[FFN(x) = f(xW_1 + b_1)W_2 + b_2\]</span></p><p>其中，f(x)是非线性激活函数。广泛使用的激活函数有ReLU、GELU、Swish等。</p><p><span class="math display">\[ReLU(x) = max(0, x)\\GELU(x) = x \cdot \phi(x)\\Swish(x) = x \cdot \sigma(\beta x)\\\]</span></p><p>其中，<spanclass="math inline">\(\phi(x)\)</span>是标准正态分布的累积分布函数，<spanclass="math inline">\(\sigma(x)\)</span>是sigmoid函数。</p><p><span class="math display">\[\phi(x) = \frac{1}{2} \left(1 +erf\left(\frac{x}{\sqrt{2}}\right)\right)\\\sigma(x) = \frac{1}{1 + e^{-x}}\]</span></p><p>通过引入线性门控单元（GLU）来增强模型的非线性能力，FFN额外增加了一个权重矩阵，即下式中的V，共有三个权重矩阵，获得了更好的模型性能。</p><p><span class="math display">\[\begin{matrix}FFN(x) = (f(xW_1) \otimes xV)W_2\\ReGLU(x) = ReLU(xW) \otimes xV\\GeGLU(x) = GELU(xW) \otimes xV\\SwiGLU(x) = Swish_{\beta}(xW) \otimes xV\\  \end{matrix}\]</span></p><h2 id="高效参数微调方法">2 高效参数微调方法</h2><p>大语言模型的参数量进行全量微调成本很高。高效参数微调（PEFT）在微调大模型时只训练一小部分参数。高效参数微调方法有以下几方面优点：</p><ol type="1"><li>显存占用少，对硬件资源要求低</li><li>训练速度快，耗时更短</li><li>更低的存储成本，不同的任务可以共享大部分的权重参数</li><li>可能会有更好的模型性能，减轻了过拟合问题</li></ol><h3 id="提示微调prompt-tuning">2.1 提示微调(Prompt Tuning)</h3><p>prompttuning原本的含义指的是通过修改输入prompt来获得更好的模型效果。这里的提示是“硬提示（hardprompt）”。我们直接修改输入prompt，输入prompt是不可导的。例如，我们可以将输入prompt从“Whatis the capital of France?”修改为“Question: What is the capital ofFrance? Answer:”，引导模型生成想要的输出。</p><p>与“硬提示”相对应，“软提示微调（soft prompttuning）”将一个可训练张量与输入文本的embeddings拼接起来，这个可训练张量可以通过反向传播来优化，进而提升目标任务的模型效果。这里的可训练张量可以理解为prompt文本对应的embedding，是一个softprompt。<strong>冻结大模型原始的参数，只训练这个新增加的prompt张量</strong>。prompttuning随着基座模型参数量的增大效果会变好。 尺寸：<spanclass="math inline">\([virtual\_token\_num, embd\_dim]\)</span></p><p><strong>实际上都只是改变的输入文本，不改变LLMModel参数</strong>，但是硬提示是通过手动设计的自然语言提示来引导模型，通过改变输入文本来间接影响嵌入表示，而软提示通过<strong>可训练</strong>的提示嵌入直接影响输入表示，能够进行优化和微调，适用于更复杂和多样化的任务。</p><h2 id="前缀微调pre-fix-tuning">2.2 前缀微调(Pre-fix Tuning)</h2><p>prefix tuning与prompttuning相似，将一个特定任务的张量添加到输入，这个张量是可训练的，保持预训练模型的参数不变。主要区别如下：</p><ol type="1"><li>prefixtuning将prefix参数（可训练张量）添加到所有的transformer层的输入(也许就是传播后的embedding？)，而prompttuning只将可训练矩阵添加到输入embedding。prefixtuning会将prefix张量作为past_key_value添加到所有的transformer层。<strong>每个transformer层都有各自不同的可学习prefix，允许不同模型层进行更量身定制的适应。</strong></li><li>用一个独立的FFN来编码和优化prefix参数，而不是直接优化softprompt，因为它可能造成不稳定并损害性能。在更新完softprompt后，就不再使用FFN了。</li></ol><p>前者是直接作用在输入embedding上，后者是作用在所有transformer层的self-attention块，在计算得到K和V后，与可训练的prefix张量拼接起来。</p><p>尺寸：<span class="math inline">\([virtual\_token\_num, 2 \timeslayer\_num \times hidden\_size]\)</span> (2是因为K和V)</p><h2 id="适配器微调adapter-tuning">2.3 适配器微调(Adapter Tuning)</h2><figure><img src="./images/Adapter-Tuning.webp" title="图3：Adapter Tuning"alt="Adapter Tuning" /><figcaption aria-hidden="true">Adapter Tuning</figcaption></figure><p>首先是一个 down-project层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个up-project 结构将低维特征映射回原来的高维特征； 同时也设计了skip-connection 结构，确保了在最差的情况下能够退化为 identity<strong>在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter结构进行微调</strong></p><h3 id="llama-adapter">2.3.1 LLaMA-adapter</h3><p>LLaMA-adapter结合了prefix tuning和adapter。与prefixtuning类似，LLaMA-adapter在输入embed上添加了可训练的prompt张量。</p><ol type="1"><li>LLaMA-adapter引进了零初始化的注意力机制和门控机制。动机是adapter和prefixtuning结合了随机初始化的张量（prefix prompts和adapterlayers）很大可能会损害预训练语言模型的语义学知识，导致在训练初始阶段的微调不稳定和很高的性能损失。</li><li>LLaMA-adapter只给L个深层transformer层添加了可学习的adaptionprompts，而不是给所有的transformer层都添加。作者认为，这种方法可以更有效的微调专注于高级语义信息的语言表示。</li></ol><h2 id="低秩适配微调low-rank-adaptation-lora">2.4 低秩适配微调(Low-RankAdaptation, LoRA)</h2><figure><img src="./images/LoRA.webp" title="图4：LoRA" alt="LoRA" /><figcaption aria-hidden="true">LoRA</figcaption></figure><p>冻结了预训练的模型权重，并将可训练的秩分解矩阵(LoRA参数)注入到Transformer 架构的每一层中。</p><p>假设模型原有参数为<span class="math inline">\(W_0 \in \mathbb{R}^{d\times k}\)</span>，微调后的参数为<span class="math inline">\(W \in\mathbb{R}^{d \times k}\)</span>，即</p><p><span class="math display">\[W = W_0 + \Delta W\]</span></p><p>LoRA将<span class="math inline">\(\DeltaW\)</span>分解为两个低秩矩阵的乘积，即：</p><p><span class="math display">\[\Delta W = B \cdot A\]</span></p><p>其中，<span class="math inline">\(B \in \mathbb{R}^{d \timesr}\)</span>，<span class="math inline">\(A \in \mathbb{R}^{r \timesk}\)</span>，<spanclass="math inline">\(r\)</span>是低秩矩阵的秩，且<spanclass="math inline">\(r \ll min(d,k)\)</span>。</p><p><code>F.linear(input, self.weight, self.bias) + (self.lora_dropout(input) @ self.lora_right_weight @ self.lora_left_weight) * self.lora_scaling</code></p><ol type="1"><li><spanclass="math inline">\(B\)</span>是升维矩阵，将低维的输入向量升维到高维空间，<spanclass="math inline">\(A\)</span>是降维矩阵，将高维的输出向量降维到低维空间。矩阵的”升维”或”降维”作用取决于它在整个运算中的位置和作用,而不仅仅是它自身的维度。即<spanclass="math inline">\(B\)</span>的维度是<span class="math inline">\(d\times r\)</span>，这意味着它将<spanclass="math inline">\(r\)</span>维的输入向量变换到<spanclass="math inline">\(d\)</span>维的输出向量。而<spanclass="math inline">\(A\)</span>的维度是<span class="math inline">\(r\times k\)</span>，这意味着它将<spanclass="math inline">\(d\)</span>维的输入向量变换到<spanclass="math inline">\(r\)</span>维的输出向量。</li><li>为什么矩阵B被初始化为0，而矩阵A正常高斯初始化全0容易梯度消失(因为对称性)，全高斯初始化会引入过大噪声，所以B初始化为0，A初始化为高斯，是为了在训练开始时维持网络的原有输出(初始偏移为0)，但同时也保证在真正开始学习后能够更好的收敛(打破对称性)。</li><li><span class="math inline">\(\Delta W\)</span> 会通过<spanclass="math inline">\(\frac{\alpha}{r}\)</span>的方式进行缩放，其中<spanclass="math inline">\(\alpha\)</span>类似调整学习率，所以不如固定为1，只调节<spanclass="math inline">\(r\)</span>。</li><li>与adapter tuning相比，LoRA的区别在于：<ol type="1"><li>插入位置。LoRA是以残差连接的形式”并联”在Transformer的Q,K,V,O矩阵上(有研究表明只用于QV表现也不错)；而Adapter是插入在Feed-forwardLayer后面。</li><li>推理延迟。LoRA在训练完后其参数可以与原有预训练模型直接合并，变回单分支结构，不会引入额外的延迟；而Adapter由于引入了额外的串联网络层，因此会带来额外的延迟。</li><li>参数存储。使用LoRA进行微调，在训练完毕后只需要保存LoRA本身的参数；而使用Adapter则要保存整个原有模型的参数。</li></ol></li></ol><h3 id="模型量化quantization">2.4.1 模型量化(Quantization)</h3><p>模型量化是一种压缩网络参数的方式，它将神经网络的参数(weight)、特征图(activation)等原本用浮点表示的量值换用定点(整型)表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。这样可以减少模型的存储空间和计算量，提高模型的运行速度。</p><ol type="1"><li>可以减少内存和显存占用，给模型瘦身</li><li>能够提高运行速度，这可以从两方面理解<ol type="1"><li>在适配低精度的硬件下，量化模型的运算能直接用 int8 GEMM kernel计算</li><li>量化减少了单位数据的 bit 数，因而可以减少计算过程中的 IO 通信量</li></ol></li><li>可以增大 batch size</li></ol><h4 id="量化方法">量化方法</h4><ol type="1"><li>QAT(Quant-Aware Training)也可以称为在线量化(OnQuantization)。它需要利用额外的训练数据，在量化的同时结合反向传播对模型权重进行调整，意在确保量化模型的精度不掉点</li><li>PTQ (Post Training Quantization)也可以称为离线量化(OffQuantization)。它是在已训练的模型上，使用少量或不使用额外数据，对模型量化过程进行校准，可能伴有模型权重的缩放。<ol type="1"><li>训练后动态量化(PostDynamicQuantization)，不使用校准数据集，直接对每一层 layer通过量化公式进行转换，QLoRA 就是采用这种方法</li><li>训练后校正量化(Post CalibrationQuantization)，需要输入有代表性的数据集，根据模型每一层 layer的输入输出调整量化权重，GPTQ 就是采用这种方法。</li></ol></li></ol><p>计算公式如下：</p><p><span class="math display">\[\begin{align}{quantized\_value} ={round}(\frac) + {zero\_point}\\{float\_value} = ({quantized\_value} - {zero\_point}) \times {scale}\end{align}\]</span></p><p>通常 scale 为 <spanclass="math inline">\(\frac{max(abs(float\_value))}{2^{bit\_width-1}-1}\)</span></p><figure><img src="./images/Quant.webp" title="图5：Quantization"alt="Quantization" /><figcaption aria-hidden="true">Quantization</figcaption></figure><p><strong>对称量化</strong> 量化前后的 0点是对齐的，因此不需要记录零点。它适合对分布良好且均值为 0的参数进行量化（对于正负数不均匀分布的情况不够友好），因此对称量化常用于对weight 量化 <strong>非对称量化</strong> 量化前后 0点不对齐，需要额外记录一个 offset，也就是零点。非对称量化常用于对activation 做量化</p><p>为考虑其他更多情况，需要引入一种新的量化策略：<strong>Block-wisequantization(块级量化)，块级量化最终能使量化的精度损失减少</strong>。为了避免异常值(outlier)的影响，我们会将输入tensor(通常是神经网络的权重或激活值)分割成多个块(block)，然后每个块(block)单独做量化，有单独的缩放因子scale和零点zero</p><ol type="1"><li>可以为不同的数据块选择<strong>不同的量化策略</strong>。例如，某些块可以用更高的位宽量化，而其他块则可以用更低的位宽</li><li>尽管块级量化可以提供更好的精度，但由于每个块都有自己的量化参数，这可能会<strong>增加计算和存储的开销</strong></li></ol><h3 id="量化低秩适配qlora">2.4.2 量化低秩适配(QLoRA)</h3><p>QLoRA针对模型权重(weight)做量化，采用的是对称量化算法，量化过程基本与LoRA相同:</p><ol type="1"><li>4位NormalFloat量化：确保每个量化仓中有相同数量的值 即采用新的 NF(NormalFloat)数据类型，它是对于正态分布权重而言信息理论上最优的数据类型，同时，NF类型有助于缓解异常值的影响</li><li>双量化：对量化常量再次量化以节省额外内存的过程 即DoubleQuant，对于量化后的 scale 数据做进一步的量化</li><li>QLoRa还有统一内存分页：它依赖于NVIDIA统一内存管理，自动处理CPU和GPU之间的页到页传输，它可以保证GPU处理无错，特别是在GPU可能耗尽内存的情况下</li></ol><h4 id="nf数据类型">NF数据类型</h4><p>int4 的格点分布是均匀的，然而模型的权重通常服从均值为 0的正态分布，因此格点的分布和数据的分布不一致。 NF4的格点按照正态分布的分位数截取，格点分布两端稀疏，中间密集，格点分布与数据分布一致。这样格点分配的效率就大大增加了，同时精度受损也不会太大。</p><h4 id="双量化double-quant">双量化(Double Quant)</h4><p>QLoRA 将每 64 个参数为做一个 block，即 block_size = 64，每个 block计算一个 Scale。 由于量化后的 Scale 通常以 FP32 存储，在 block数众多的情况下，Scale 占用的显存也不可忽视。 因此，QLoRA 对 Scale进一步量化成 FP8，取 Double Quant 的 block size =256，因而进一步降低了显存消耗。</p><h4 id="统一内存分页">统一内存分页</h4>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flash Attention</title>
      <link href="/2024/08/08/NLP/FlashAttention/"/>
      <url>/2024/08/08/NLP/FlashAttention/</url>
      
        <content type="html"><![CDATA[<h1 id="flash-attention">Flash Attention</h1><p>FlashAttention 是一种具有 IO感知，且兼具快速、内存高效的新型注意力算法。为了缓解LLM输入输出序列s扩展时计算复杂度和空间复杂度都是<spanclass="math inline">\(O(s^2)\)</span>的问题。</p><h2 id="transformer复杂度">1 Transformer复杂度</h2><p>transformer模型中self-attention的计算量和储存复杂度随着序列长度 s​呈二次方增长，这限制了大语言模型的最大序列长度 s​ 的大小。</p><h3 id="计算复杂度">1.1 计算复杂度</h3><table><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单次FLOPs</th><th style="text-align: center;">总FLOPs</th></tr></thead><tbody><tr><td style="text-align: center;">Embedding</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr><td style="text-align: center;">LM Head(logits)</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td></tr><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(8bsd^2+4bs^2d\)</span></td><td style="text-align: center;"><spanclass="math inline">\(8blsd^2+4bls^2d\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(16bsd^2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(16blsd^2\)</span></td></tr></tbody></table><p>总FLOPs：<span class="math inline">\(2bsVd + 24blsd^2 +4bls^2d\)</span></p><h3 id="空间复杂度">1.2 空间复杂度</h3><table><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单个占用</th><th style="text-align: center;">总占用</th></tr></thead><tbody><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(11bsd+5bs^2n\)</span></td><td style="text-align: center;"><spanclass="math inline">\(11blsd+5bls^2n\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(19bsd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(19blsd\)</span></td></tr><tr><td style="text-align: center;">LayerNorm</td><td style="text-align: center;">2l</td><td style="text-align: center;"><spanclass="math inline">\(2bsd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(4blsd\)</span></td></tr></tbody></table><p>中间激活值总显存占用：<spanclass="math inline">\(34blsd+5bls^2n\)</span> bytes</p><h2 id="standard-attention">2 Standard Attention</h2><p>首先回顾Attention的计算过程：</p><p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V\]</span></p><p>其中，<span class="math inline">\(Q, K, V in \mathbb{R}^{s \timesd}\)</span>，<spanclass="math inline">\(d\)</span>是embedding维度，<spanclass="math inline">\(s\)</span>是序列长度。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理(NLP)简介</title>
      <link href="/2024/08/08/NLP/Introduction/"/>
      <url>/2024/08/08/NLP/Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="自然语言处理nlp">自然语言处理(NLP)</h1><p>自然语言处理(Natural Language Processing,NLP)是人工智能领域的一个重要分支，研究如何使计算机能够理解、处理和生成自然语言文本。NLP技术在信息检索、机器翻译、语音识别、情感分析、对话系统等领域有着广泛的应用。</p><h2 id="nlp基础">1. NLP基础</h2><h2 id="nlp任务">2. NLP任务</h2><h3 id="数据预处理">2.1 数据预处理</h3><p>NLP任务的第一步是数据预处理，包括收集语料库、文本清洗、分词、去掉停用词（可选）、标准化和特征提取等任务流程图如下：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>  graph LR    A[数据预处理] --&gt; B[收集语料库]    B --&gt; C[文本清洗]    C --&gt; D[分词]    D --&gt; E[标准化]    E --&gt; F[特征提取]  </pre></div><h4 id="收集语料库">2.1.1 收集语料库</h4><p>每个机器学习问题都从数据开始，例如电子邮件，帖子或推文列表。这些数据的集合称为语料库。语料库可以是一个文本文件，一个文件夹，一个网站，一个数据库或任何其他数据源。</p><h4 id="文本清洗">2.1.2 文本清洗</h4><p>文本清洗是指从文本中删除不必要的字符，例如标点符号、特殊字符、HTML标签等。文本清洗的目的是减少文本数据的噪声，使文本数据更加干净。</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr><th>数据类型</th><th>解释</th><th>常用格式</th></tr></thead><tbody><tr><td>结构化数据</td><td>数据以表格形式存储，每行代表一个样本，每列代表一个特征</td><td>CSV、Excel、数据库</td></tr><tr><td>非结构化数据</td><td>数据没有固定的格式，例如文本、图像、音频、视频等</td><td>文本、图像、音频、视频</td></tr><tr><td>半结构化数据</td><td>数据介于结构化数据和非结构化数据之间</td><td>XML、JSON、HTML</td></tr></tbody></table><h4 id="分词">2.1.3 分词</h4><p>分词是将文本分割成一个个单词或词组的过程。分词是NLP任务的第一步，也是NLP任务的基础。分词的目的是将文本数据转换为计算机可以理解的形式。</p><p>常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。目前，主流的中文分词技术采用的都是基于词典最大概率路径+未登录词识别（HMM）的方案，其中典型的代表就是jieba分词，一个热门的多语言中文分词包。</p><p>可以借助一些开源的分词工具，如jieba、NLTK、spaCy等。</p><h4 id="标准化">2.1.4 标准化</h4><p>标准化是将文本数据转换为统一的格式，例如将文本转换为小写、去掉标点符号、去掉数字等。标准化的目的是减少文本数据的噪声，使文本数据更加干净。包括：去掉停用词、词汇表、训练数据等等。</p><h5 id="a-去掉停用词">a 去掉停用词</h5><p>停用词是指在文本中频繁出现，但对文本分析没有实际意义的词，如“的”、“是”、“在”、“其中”、“况且”、“什么”等。但这一步不是必须的，要根据实际业务进行选择，像关键词挖掘就需要去掉停用词，而像训练词向量就不需要。</p><h5 id="b-词汇表">b 词汇表</h5><p>词汇表是为语料库建立一个所有不重复词的列表，每个词对应一个索引值，并索引值不可以改变。词汇表的最大作用就是可以将词转化成一个向量，即One-Hot编码。</p><p><strong>O<em>ne-Hot编码的缺点</em></strong>当词汇表的维度特别大的时候，就会导致经过One-Hot编码后的词向量非常稀疏，同时One-Hot编码也缺少词的语义信息。由于这些问题，才有了后面大名鼎鼎的Word2vec，以及Word2vec的升级版BERT。</p><h5 id="c-训练数据">c 训练数据</h5><p>我们在训练模型时，还需要提供训练数据。模型的学习可以大体分为两类：</p><ul><li>监督学习，在已知答案的标注数据集上，模型给出的预测结果尽可能接近真实答案，适合预测任务</li><li>非监督学习，学习没有标注的数据，是要揭示关于数据隐藏结构的一些规律，适合描述任务</li></ul><p>根据不同的学习任务，我们需要提供不同的标准化数据。一般情况下，标注数据的获取成本非常昂贵，非监督学习虽然不需要花费这样的成本，但在实际问题的解决上，主流的方式还选择监督学习，因为效果更好。</p><h4 id="特征提取">2.1.5 特征提取</h4><p>特征提取是将文本数据转换为计算机可以理解的特征向量的过程。转化的方式主要有两种：统计和Embedding</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>KV Cache</title>
      <link href="/2024/08/08/NLP/KV%20Cache/"/>
      <url>/2024/08/08/NLP/KV%20Cache/</url>
      
        <content type="html"><![CDATA[<h1 id="kv-cache">KV Cache</h1><p>KV-Cache是一种加速Transformer推理的策略，几乎所有自回归模型都内置了KV-Cache</p><h2 id="为什么需要kv-cache">1 为什么需要KV-Cache</h2><p>Transformer每一层分为两个部分，一个是Self-Attention，另一个是FeedForward Network(FFN)。</p><ol type="1"><li>Self-Attention: <span class="math inline">\(Attention(Q, K, V) =softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span></li><li>FFN: <span class="math inline">\(FFN(x) = ReLU(xW_1 + b_1)W_2 +b_2\)</span></li></ol><p>自回归模型采用每次推理都将前文整句输入模型，然后预测下一个token的方式，这种方式会存在相同结果的重复推理。令前一次待推理的文本长度为S，下一次为S+1，由于网络中的各项参数已经固定，因此两次推理对于前S个token的计算结果是完全相同的，包括Embedding映射，每一层、每一个注意力头下的KQV映射，注意力权重，以及后续的FFN层都在重复计算。</p><p>那么既然下一个token是由当前最后一个token的网络输出所决定的，那能不能仅输入最后一个token来进行推理？答案是否定的，虽然在结果层仅由最后一个token来决定，但是中间的注意力过程它的<strong>计算依赖于前文所提供的Key、Value向量</strong>(这就是前文信息)，因此也不能抛弃前文不管。</p><p>S+1位置token的推理依赖于两个要素:</p><ol type="1"><li>首先是当前第S个token在网络中完整forward一遍</li><li>其次是除最后一个token以外，之前所有的S-1位置的token在每一层、每个注意力头下的Key，Value信息。</li></ol><p>所以可以将Key、Value信息缓存下来，下次推理时直接使用，这就是KV-Cache的思想。</p><h2 id="kv-cache的实现">2 KV-Cache的实现</h2><p>从第二次推理开始，仅需要输入当前最后一个token，单独对该token做Q，K，V映射，将past_key_values中前文所有的K，V和该token的K，V进行拼接得到完成的Key、Value向量(只是简单的拼接矩阵)，最终和该token的Query计算注意力，拼接后的Key、Value也同步更新到past_key_values。</p><h2 id="存储结构">3 存储结构</h2><p>KV-Cache会将截止当前各个token在每一层、每个头的Key向量和Value向量存储在内存中，在HuggingFace的代码实现中使用past_key_values变量进行存储，past_key_values是一个矩阵，其维度为[n,2, b, h, s, d]，类似一个六维的矩阵，每个维度的含义如下</p><ol type="1"><li>第一维num_layers：在外层是以每一个堆叠的Block为单位，例如堆叠12层，则一共有12组Key、Value信息</li><li>第二维2：代表Key和Value这两个信息对象，索引0取到Key向量，索引1取到Value向量</li><li>第三维batch_size：代表batch_size，和输入需要推理的文本条数相等，如果输入是一条文本，则b=1</li><li>第四维 num_heads：代表注意力头的数量，例如每层有12个头，则h=12</li><li>第五维seq_len：代表截止到当前token为止的文本长度，在每一个历史token位置上该token在每一层每个头下的Key，Value信息</li><li>第六维d：代表Key、Value向量的映射维度，若token总的映射维度为768，注意力头数为12，则d=768/12=64</li></ol><p>可以发现每一步推理后的差异仅仅产生在seq_len这个维度上(seq_len维度大小会加1)，它是由新推理的那一个token所对应的Key，Value拼接到上一个past_key_values的seq_len维度中所导致的。例如 [12, 2, 1, 12, <strong>5</strong>, 64] -&gt; [12, 2, 1, 12,<strong>6</strong>, 64]</p><h2 id="kv-cache内存占用flops下降分析">4KV-Cache内存占用、FLOPs下降分析</h2><h3 id="内存占用">4.1 内存占用</h3><p>KV-Cache本质上是<strong>用空间换时间</strong>，存储的Key、Value矩阵会额外占用内存。</p><p>假设以float16精度(占用两个字节)来存储，每个token的存储占用公式如下：<span class="math display">\[2 \times n_{layers} \times 2 \times n_{heads} \times d\]</span></p><p>以LLaMa-7B-FP16为例，模型加载占用显存14GB，向量维度4096，堆叠32层，最大推理步长4096。若推理一个batch为2，长度为4096的句子，KV-Cache占用的存储空间为2×2×32×4096×2×4096=4294967296字节，约等于4GB，随着推理的batch增大，推理长度变长，KV-Cache占用的存储空间可能超过模型本身。</p><h3 id="flops下降">4.2 FLOPs下降</h3><p>另一方面KV-Cache极大地降低了FLOPs（浮点计算量），表面上KV-Cache省去了之前每个token的Key、Value的计算量，每个token在所有层下计算Key、Value的FLOPs公式如下：<span class="math display">\[2 \times 2 \times n_{layers} \times  d^2\]</span></p><p>其中d平方代表从tokenEmbedding到Key或者Value向量的过程，乘以2是矩阵相乘中逐位相乘再相加导致有两个操作，再乘以2代表Key、Value各一个。</p><p>以LLaMa-7B为例，推理一个batch为2，长度为4096的句子，光计算KV一共节省了(2×2×32×4096×4096)×4096×2=17592BFLOPs的计算量</p><p>额外的，不仅省去了前文所有token的Key、Value的映射，由此导致后续这些token的注意力权重计算，注意力的MLP层，FFN前馈传播层也都不需要再计算了，相当于推理阶段的计算复杂度永远等于只对一个token进行完整的forward推理，因此计算量大幅降低。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>采样</title>
      <link href="/2024/08/08/NLP/Sampling/"/>
      <url>/2024/08/08/NLP/Sampling/</url>
      
        <content type="html"><![CDATA[<h1 id="采样sampling">采样(Sampling)</h1><p>LLM在推理时，需要对模型输出的概率分布进行采样，以得到最终的输出。即在生成文本时，需要根据概率分布选择下一个词。</p><p>例如：在某一步的输出结果如下：</p><pre class="language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">token1</span><span class="token punctuation">:</span> <span class="token number">0.4</span><span class="token key atrule">token2</span><span class="token punctuation">:</span> <span class="token number">0.3</span><span class="token key atrule">token3</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token key atrule">token4</span><span class="token punctuation">:</span> <span class="token number">0.1</span></code></pre><p>这时应该根据什么规则选择下一个词呢？常用的采样方法有：</p><ol type="1"><li>贪心采样(Greedy Sampling)：选择概率最大的词作为下一个词。<ol type="1"><li>简单高效</li><li>可能会导致生成的文本过于单调和重复</li></ol></li><li>随机采样(Random Sampling)：根据概率分布随机选择下一个词。<ol type="1"><li>增加生成的多样性</li><li>生成的文本可能不够连贯和无意义</li></ol></li><li>束搜索(Beam Search)：维护一个大小为 k的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k个单词，然后保留总概率最高的 k 个候选序列。k=1时即为贪心采样。<ol type="1"><li>增加搜索空间，平衡生成的质量和多样性</li><li>可能会导致生成的文本之间的相似度很高，多样性不足，导致过于保守和不自然。</li></ol></li></ol><p>top-k 采样和 top-p采样是介于贪心解码和随机采样之间的方法，也是目前大模型解码策略中常用的方法。</p><h2 id="top-k采样">1 Top-k采样</h2><p>在每一步，只从概率最高的 k个单词中进行随机采样，而不考虑其他低概率的单词。 k=1时即为贪心采样。</p><p>Top-k 采样是对前面“贪心策略”的优化，它从排名前 k 的 token中进行抽样，允许其他分数或概率较高的token 也有机会被选中。在很多情况下，这种抽样带来的随机性有助于提高生成质量。</p><ol type="1"><li>top-k 优点：<ol type="1"><li>可以根据不同的输入文本<strong>动态调整候选单词的数量</strong>，而不是固定为k个。不同的输入文本可能会导致不同的概率分布，有些分布可能比较平坦，有些分布可能比较尖锐。如果分布比较平坦，那么前k个单词可能都有相近的概率，那么我们就可以从中进行随机采样；如果分布比较尖锐，那么前k 个单词可能会占据绝大部分概率，那么我们就可以近似地进行贪心解码。</li><li>可以通过调整 k的大小来<strong>控制生成的多样性和质量</strong>。一般来说，k越大，生成的多样性越高，但是生成的质量越低；k越小，生成的质量越高，但是生成的多样性越低。</li><li><strong>可以与其他解码策略结合使用</strong>，例如温度调节（TemperatureScaling）、重复惩罚（Repetition Penalty）、长度惩罚（LengthPenalty）等，来进一步优化生成的效果。</li></ol></li><li>top-k 缺点：<ol type="1"><li>可能会导致生成的<strong>文本不符合常识或逻辑</strong>。这是因为top-k采样<strong>只考虑了单词的概率</strong>，而没有考虑单词之间的语义和语法关系。例如，如果输入文本是“我喜欢吃”，那么即使饺子的概率最高，也不一定是最合适的选择，因为可能用户更喜欢吃其他食物。</li><li>可能会导致生成的文本<strong>过于简单或无聊</strong>。这是因为 top-k采样只考虑了概率最高的 k个单词，而没有考虑其他低概率但有意义或有创意的单词。例如，如果输入文本是“我喜欢吃”，那么即使苹果、饺子和火锅都是合理的选择，也不一定是最有趣或最惊喜的选择，因为可能用户更喜欢吃一些特别或新奇的食物。</li></ol></li></ol><p>因此，我们通常会考虑 top-k 和其它策略结合，比如 top-p。</p><h2 id="top-p采样">2 Top-p采样</h2><p>top-k 有一个缺陷，那就是“k值取多少是最优的？”非常难确定。于是出现了动态设置 token候选列表大小策略——即核采样（Nucleus Sampling）。</p><p>在每一步，只从累积概率超过某个阈值 p的最小单词集合中进行随机采样，而不考虑其他低概率的单词。即只考虑前 p的概率的单词。 这种方法也被称为核采样（nucleussampling），因为它只关注概率分布的核心部分，而忽略了尾部部分。</p><h2 id="温度调节">3 温度调节</h2><p>温度调节（TemperatureScaling）是一种简单有效的解码策略，它可以通过调整温度参数来控制生成的多样性和质量。</p><p>实现主要是将 logits 除以温度来实现温度采样，然后将其输入 Softmax并获得采样概率。</p><p>temperature这个参数可以告诉机器如何在质量和多样性之间进行权衡。较低的 temperature 意味着更高的质量，而较高的 temperature意味着更高的多样性。 当 temperature设置为零时，模型总是会选择具有最高可能性分数的token，从而导致模型生成的回复缺乏多样性，但却能确保总是选择模型评估出的最高质量的token来生成回复。当 temperature设置为较大时，概率分布趋向于<strong>均匀分布</strong>，从而导致模型生成的回复具有更高的多样性，但却可能导致生成的回复质量较低。</p><h2 id="联合采样">4 联合采样</h2><p>通常将 top-k、 top-p 和 temperature结合起来使用，以获得更好的生成效果。</p><p>使用的顺序一般是 top-k -&gt; top-p -&gt; temperature</p><p>例如对于上面的例子:</p><ol type="1"><li>先根据 top-k = 3 采样，得到候选词集合<code>{token1: 0.4, token2: 0.3, token3: 0.2}</code></li><li>再根据 top-p = 0.8 采样，得到候选词集合<code>{token1: 0.4, token2: 0.3}</code></li><li>最后根据 temperature = 0.7 归一化，得到候选词集合<code>{token1: 0.54, token2: 0.46}</code></li><li>根据概率分布<strong>随机</strong>选择下一个词，得到最终的输出。</li></ol><h2 id="惩罚机制">5 惩罚机制</h2><p>频率惩罚（frequencypenalty）：一种抑制重复生成的机制。它通过对已经生成的词语在下一次生成时进行惩罚来减少某些词的重复出现。存在惩罚（presencepenalty）：存在惩罚与频率惩罚类似，但它只关心词语是否已经出现过，而不关心出现的次数。存在惩罚通过减少已经出现过的词语的概率，鼓励生成新的词语，而不会明显抑制常用词的重复。</p><p>temperature参数通过在token选择过程中添加随机性来实现输出内容的多样性，而频率惩罚和存在惩罚则通过对已在文本中出现的token施加惩罚以增加输出内容的多样性。这使得对旧的和过度使用的token进行选择变得不太可能，从而让模型选择更新颖的token。</p><p>就像 temperature一样，频率惩罚和存在惩罚会引导我们远离“最佳的”可能回复，朝着更有创意的方向前进。然而，它们不像temperature那样通过引入随机性，而是通过精心计算的针对性惩罚，为模型生成内容增添多样性。在一些罕见的、需要非零temperature的任务中（需要对同一个提示语给出多个答案时），可能还需要考虑将小的频率惩罚或存在惩罚加入其中，以提高创造性。但是，对于只有一个正确答案且你希望一次性找到合理回复的提示语，当你将所有这些参数设为零时，成功的几率就会最高。</p><h2 id="参数调整">6 参数调整</h2><h3 id="只存在一个正确答案">1. 只存在一个正确答案</h3><pre class="language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">temperature</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token key atrule">frequency_penalty</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token key atrule">presence_penalty</span><span class="token punctuation">:</span> <span class="token number">0.0</span></code></pre><h3 id="需要创造性或者多样性">2. 需要创造性或者多样性</h3><pre class="language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">temperature</span><span class="token punctuation">:</span> 增加<span class="token key atrule">frequency_penalty</span><span class="token punctuation">:</span> 增加<span class="token key atrule">presence_penalty</span><span class="token punctuation">:</span> 增加</code></pre><p>当模型输出无意义内容或者胡言乱语时，需要降低<code>temperature top-k top-p</code>如果模型输出的内容看起来零散并且话题变化太快，应当降低<code>presence_penalty</code>如果有太多新奇和不寻常的词语，或者存在惩罚设置为零但仍然存在很多话题变化，应当降低<code>frequency_penalty</code></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/2024/08/08/NLP/Transformer/"/>
      <url>/2024/08/08/NLP/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="transformer">Transformer</h1><h2 id="模型概述">1. 模型概述</h2><p>Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attentionmechanism），这是一个关键的创新，使其在处理序列数据时表现出色。</p><h3 id="基本结构">1.1 基本结构</h3><p>transformer基本模型图如下图所示。模型的左半边Encoder部分可以看作是一个编码器，右半边Decoder部分可以看作是一个解码器，其中编码器是双向的，解码器是单向的需要循环迭代输出。</p><figure><img src="./images/Transformer_Structure.webp"alt="transformer基本结构" /><figcaption aria-hidden="true">transformer基本结构</figcaption></figure><h3 id="特点与创新">1.2 特点与创新</h3><ol type="1"><li>训练并行: 即所有字是同时训练的，这样就大大增加了计算效率</li><li>自注意力机制:核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。</li><li>多头注意力（Multi-HeadAttention）：自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。</li><li>堆叠层（StackedLayers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。</li><li>位置编码（PositionalEncoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。</li><li>残差连接和层归一化（Residual Connections and LayerNormalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。</li><li>编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。</li></ol><h2 id="模型细节">2. 模型细节</h2><h3 id="编码器encoder">2.1 编码器(Encoder)</h3><figure><img src="./images/Transformer_Encoder.webp" alt="编码器" /><figcaption aria-hidden="true">编码器</figcaption></figure><div class="mermaid-wrap"><pre class="mermaid-src" hidden>  graph LR    A[编码器] --&gt; B[输入部分 （Embedding）]    A --&gt; C[注意力机制 （Self-attention）]    A --&gt; D[多头注意力机制 （Multi-head Attention）]    A --&gt; E[残差连接 ResNet （Residual Network）]    A --&gt; F[层标准化 LN（Layer Normalization）]    A --&gt; G[前馈型神经网络 FFN （Feed Forward Network）]  </pre></div><h4 id="输入部分embedding">2.1.1 输入部分（Embedding）</h4><p>为了能够把文本数据或音频数据输入进Transformer这个黑盒子里面处理，我们必须先通过Embedding将其转换成编码标识，使其成为计算机能够处理的数字。</p><p>在理解Embedding层之前，我们需要关注于文本数据的特点，举例：“我爱北京天安门”，我们在做文本数据处理得时候，传统的汉字是不能给计算机做运算的，为了解决汉字运算的问题，提出了将汉字进行编码（或叫文本张量表示）的思想。目前主流的编码方式有one-hot编码及wordEmbedding。</p><h5 id="a.-one-hot编码">a. One-hot编码</h5><p>该编码格式较为傻瓜式，就是将词库中所有的单词，从[0,max_len-1]的进行编号，使用哪个词对应的编号位置部分置1，其余部分置0。</p><h5 id="b.-word2vec编码">b. word2vec编码</h5><p>word tovec，即文本推导张量，存在两个模式，一个是CBOW，一个是skipgram</p><ul><li>CBOW类似于我们在英文考试中的完形填空，即根据上下文推导中间的单词</li><li>skipgram与之相反，通过某个单词，推导上下文，计算机复杂度更大</li></ul><h5 id="c.-位置编码">c. 位置编码</h5><p>在Transformer中，我们需要将词向量和位置向量进行叠加，这样才能得到一个完整的词嵌入。位置编码的作用就是为了给词向量加上位置信息，使得词向量在空间中的位置不同，从而使得模型能够区分不同位置的词。</p><figure><img src="./images/Transformer_PositionalEncoding.webp"alt="位置编码" /><figcaption aria-hidden="true">位置编码</figcaption></figure><h5 id="具体例子">具体例子</h5><p>Transformer的输入部分其实就是<strong><em>词向量</em></strong> 和<strong><em>位置向量</em></strong>的叠加：比如下面一句话”我有一只猫”，经过分词之后的得到的token是<code>我</code><code>有</code> <code>一只</code> <code>猫</code>，那么<code>我</code><code>有</code> <code>一只</code> <code>猫</code> 的<strong><em>词向量</em></strong> 和<strong><em>位置向量</em></strong>的叠加后的向量再放在一起组成的矩阵就是这句话的<strong><em>词嵌入</em></strong></p><p><img src="./images/Transformer_WordEmbedding.webp" alt="词嵌入" /><img src="./images/Transformer_WordEmbedding2.webp" alt="词嵌入" /></p><h4 id="注意力机制self-attention">2.1.2注意力机制（Self-attention）</h4><p>在拿到一个Embedding的向量后，我们下一步要做的事就是利用Self-Attention机制去计算不同单词之间词向量的相似度</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之KTransformers</title>
      <link href="/2024/08/08/NLP/LLMModels/KTransformers/"/>
      <url>/2024/08/08/NLP/LLMModels/KTransformers/</url>
      
        <content type="html"><![CDATA[<h1 id="ktransformers">KTransformers</h1><p>KTransformers KTransformers 是一个灵活的、以 Python为中心的框架，旨在通过高级内核优化和放置/并行策略增强 transformers</p><p><ahref="https://github.com/kvcache-ai/KTransformers">KTransformers的Github链接</a><a href="https://github.com/deepseek-ai/DeepSeek-V2">DeepSeekV2的Github链接</a> <a href="https://arxiv.org/abs/2405.04434">DeepSeekV2的论文链接</a></p><h2 id="介绍">1. 介绍</h2><p>KTransformers 的核心是一个用户友好的、基于模板的注入框架。</p><figure><img src="./images/InjectStruction.webp#60x60"title="图1：KTransformers 架构" alt="KTransformers 架构" /><figcaption aria-hidden="true">KTransformers 架构</figcaption></figure><h2 id="优化技术">2. 优化技术</h2><p>KTransformers采用了多种优化技术，将需要两块80GB显存GPU的 DeepSeek-V2Q4_k_m 模型运行在21GB显存和136GB内存的台式计算机上。</p><h3 id="mla-注意力机制">2.1 MLA 注意力机制</h3><p>DeepSeek V2 的 MLA官方开源明确地解压了MLA的压缩表示，并缓存了解压后的键值对。针对此，KTransformers对DeepSeek V2 的MLA按照原文进行了实现，将解压缩矩阵直接吸收到 q_proj 和out_proj 权重中，减少了 KV缓存大小并增加了该算子的算力强度，从而大大优化了 GPU计算能力的利用率。</p><figure><img src="./images/DeepSeek-on-KTransformers.webp#100x100"title="图2：实现后的MLA 架构" alt="实现后的MLA 架构" /><figcaption aria-hidden="true">实现后的MLA 架构</figcaption></figure><h4 id="deepseek-v2-的mla">2.1.1 DeepSeek V2 的MLA</h4><p>DeepSeek V2 设计了MLA，它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持有效的推理。</p><figure><img src="./images/deepseekv2.webp#80x80" title="图3：DeepSeek MLA 架构"alt="DeepSeek MLA 架构" /><figcaption aria-hidden="true">DeepSeek MLA 架构</figcaption></figure><h5 id="mla的提出">2.1.1.1 MLA的提出</h5><p>MLA的核心是对键和值进行低秩联合压缩，以减少KV缓存:</p><figure><img src="./images/dsattn.webp#100x100" title="图4：MLA 架构"alt="MLA 架构" /><figcaption aria-hidden="true">MLA 架构</figcaption></figure><p>Attention的计算公式为<span class="math inline">\(A =\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)</span>，其中<spanclass="math inline">\(Q,K,V\)</span>分别为查询、键、值，<spanclass="math inline">\(d_k\)</span>为键的维度。</p><p>假定输入是<span class="math inline">\(x\)</span>，对<spanclass="math inline">\(x\)</span>做一个低秩转换：</p><p><span class="math display">\[\begin{aligned}    \textcolor{red}{c} &amp;= xW^C \quad W^C \in\mathbb{R}^{d_{\text{model}} \times d_c} \quad x \in \mathbb{R}^{n\times d_{\text{model}}}\end{aligned}\]</span></p><p>接着计算<strong>每一个head</strong>的<spanclass="math inline">\(Q,K,V\)</span>：</p><p><span class="math display">\[\begin{aligned}    Q &amp;= xW^Q \quad W^Q \in \mathbb{R}^{d_{\text{model}} \timesd_k}\\    K &amp;= xW^K = \textcolor{red}{c}W^{K&#39;}  \quad W^K \in\mathbb{R}^{d_{\text{model}} \times d_k} \quad W^{K&#39;} \in\mathbb{R}^{d_{\textcolor{red}{c}} \times d_k}\\    V &amp;= xW^V = \textcolor{red}{c}W^{V&#39;} \quad W^V \in\mathbb{R}^{d_{\text{model}} \times d_v} \quad W^{V&#39;} \in\mathbb{R}^{d_{\textcolor{red}{c}} \times d_v}\\    QK^T &amp;= xW^Q(\textcolor{red}{c}W^{K&#39;})^T =x(W^QW^{K&#39;T})\textcolor{red}{c}^T\end{aligned}\]</span></p><p>此时如果有<spanclass="math inline">\(W^{Q&#39;}=W^QW^{K&#39;T}\)</span>(<strong>会不会有精度损失？</strong>)且 <span class="math inline">\(W^{Q&#39;} \in\mathbb{R}^{d_{\text{model}} \times d_c}\)</span>，则<spanclass="math inline">\(QK^T=xW^{Q&#39;}\textcolor{red}{c}^T\)</span>，在推理过程中不需要再缓存<code>K</code><code>V</code> 矩阵，只需要缓存 <spanclass="math inline">\(\textcolor{red}{c}\)</span> 矩阵，如下：</p><p><span class="math display">\[\begin{aligned}A &amp;= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\&amp;=\text{softmax}(\frac{xW^{Q&#39;}\textcolor{red}{c}^T}{\sqrt{d_k}}){\textcolor{red}{c}W^{V&#39;}}\\\end{aligned}\]</span></p><p><strong><spanclass="math inline">\(\textcolor{red}{注意}\)</span></strong>：这里的<spanclass="math inline">\(\textcolor{red}{c}\)</span>是从<spanclass="math inline">\(x\)</span>得到的，也就是说在每一个Layer，所有的Attention-Head使用的都是一个同样的<spanclass="math inline">\(\textcolor{red}{c}\)</span>，而不是每个Head都使用不同的<code>K</code><code>V</code>矩阵。参数从<span class="math inline">\(2 \timeshead\)</span> 变为了 <spanclass="math inline">\(\textcolor{red}{1}\)</span>。</p><p>从上面的公式可以看出，<strong>MLA实际上是一个MHA，但是KV_Cache占用甚至少于GQA</strong>。再次回顾图3，可以清晰看出区别。</p><p>其中<span class="math inline">\(d_{\textcolor{red}{c}}\)</span>是超参数，<span class="math inline">\(d_{\text{model}}\)</span>是模型维度，<span class="math inline">\(d_k\)</span> 是键的维度，<spanclass="math inline">\(d_v\)</span> 是值的维度。</p><h6id="问题1和gqamha对比不同之处"><strong>问题1：和GQA，MHA对比不同之处</strong></h6><p>如果考虑所有头维度加起来的话:</p><p><span class="math display">\[n_{head} \times d_{k}=\begin{cases}    d_{model} &amp; \text{MHA} \\    (1,d_{model}) &amp; \text{GQA} \\\end{cases}\]</span></p><p><span class="math inline">\(W^Q\)</span>和<spanclass="math inline">\(W^K\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>，<span class="math inline">\(W^V\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_v}\)</span>。</p><p><span class="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{model} \timesd_{\textcolor{red}{c}}}\)</span>，<spanclass="math inline">\(W^{Q&#39;}\)</span>和<spanclass="math inline">\(W^{V&#39;}\)</span>不存在了，<spanclass="math inline">\(W^{O&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\textcolor{red}{c}} \timesd_{model}}\)</span>。DeepSeek V2 选择的是<spanclass="math inline">\(d_{\textcolor{red}{c}} = 4\timesd_k\)</span>。</p><p>所以MLA当<span class="math inline">\(d_{\textcolor{red}{c}} =d_k\)</span>时就是正常的MHA。</p><h6id="问题2和正常的attention对比为什么不进一步cache-x"><strong>问题2：和正常的Attention对比，为什么不进一步cachex</strong></h6><p>正常的Attention计算如下：</p><p><span class="math display">\[\begin{aligned}A &amp;= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\&amp;= \text{softmax}(\frac{xW^QW^{KT}x^T}{\sqrt{d_k}}){xW^V}\end{aligned}\]</span></p><p>和上文介绍的对比，为什么不直接保存<spanclass="math inline">\(x\)</span>，而去保存<spanclass="math inline">\(c\)</span>?</p><ol type="1"><li>保存<spanclass="math inline">\(x\)</span>就是不做KV_Cache的情况：或者说保存<spanclass="math inline">\(x\)</span>，把<spanclass="math inline">\(W^K\)</span>吸收到<spanclass="math inline">\(W^Q\)</span>中这时候的<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_{\text{model}}}\)</span>，而保存<spanclass="math inline">\(c\)</span>的时候<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_c}\)</span>。通常<span class="math inline">\(d_{\text{model}} \ggd_c\)</span>，所以保存<spanclass="math inline">\(x\)</span>的计算量更大。</li><li>保存全量KV_Cache就是通常的KV_Cache的做法</li><li>而保存<spanclass="math inline">\(c\)</span>则是折中：<strong>降低显存和加载代价,而提升计算代价</strong></li></ol><p>这也是整个方法的核心思想。</p><h5 id="不兼容rope">2.1.1.2 不兼容ROPE</h5><p>但是当加入ROPE后就会存在问题：</p><p><span class="math display">\[f_{\{q, k\}}\left(\boldsymbol{x}_{m}, m\right)=\boldsymbol{R}_{\Theta,m}^{d} \boldsymbol{W}_{\{q, k\}} \boldsymbol{x}_{m}\]</span></p><p>其中<span class="math inline">\(\boldsymbol{R}_{\Theta,m}^{d}\)</span> 是旋转矩阵，<spanclass="math inline">\(\boldsymbol{W}_{\{q, k\}}\)</span>是权重矩阵，<span class="math inline">\(\boldsymbol{x}_{m}\)</span>是输入。</p><p><span class="math display">\[\begin{aligned}Q &amp;= xW^QR^Q\\K &amp;= xW^KR^K\\QK^T &amp;= xW^QR^Q(xW^KR^K)^T\\    &amp;= xW^QR^Q(cW^{K&#39;}R^K)^T\\    &amp;= x(W^QR^{Q-K}W^{K&#39;T})c^T\\\end{aligned}\]</span></p><p>此时的<spanclass="math inline">\((W^QR^{Q-K}W^{K&#39;T})\)</span>因为加入了位置信息，所以不再是常数，所以无法像MLA一样进行优化。</p><p>所以DeepSeek V2 采取了一种混合的方法——每个Attention Head的Q、K新增<span class="math inline">\(dr\)</span>个维度用来添加RoPE，其中K新增的维度每个Head共享(注1)：</p><p><span class="math display">\[\begin{aligned}Q &amp;= [xW^{Q&#39;}, xW^{Qr}R^Q] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr})\\K &amp;= [cW^{K&#39;}, xW^{Kr}R^K] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr)}\\\end{aligned}\]</span></p><p>此时的矩阵中<spanclass="math inline">\(xW^{Q&#39;}\)</span>就不携带位置信息，所以可以像MLA一样进行优化。而<span class="math inline">\(xW^{Qr}R^Q\)</span>和<spanclass="math inline">\(xW^{Kr}R^K\)</span>则携带位置信息。</p><p>此时在推理过程KV_Cache中，只需要多缓存一个<spanclass="math inline">\(xW^{Kr}R^K\)</span>(命名为K^r)。 即只需要缓存<spanclass="math inline">\(\textcolor{red}{c}\)</span>和<spanclass="math inline">\(\textcolor{red}{K^r}\)</span>。</p><p>论文中选取的参数如下：</p><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td><span class="math inline">\(d_{\text{model}}\)</span></td><td>5120</td></tr><tr><td><span class="math inline">\(d_k\)</span></td><td>128</td></tr><tr><td><span class="math inline">\(d_c\)</span></td><td>512</td></tr><tr><td><span class="math inline">\(dr\)</span></td><td>64</td></tr></tbody></table><p>注1： 虽然不同的注意力头有各自独立的 <spanclass="math inline">\(k_{head}\)</span>，但这些新增的 <spanclass="math inline">\(dr\)</span>个维度是相同的，所有头在这些维度上使用的是相同的位置编码信息。这与标准多头注意力中的做法一致，在标准多头注意力中，尽管每个头有不同的键向量<spanclass="math inline">\(k_{head}\)</span>,但这些键向量的位置编码信息是相同的，因为每个AttentionHead输入都是一样的。这种共享有助于在不同的头之间保持位置信息的一致性，从而让每个头能够以一致的方式利用位置信息，尽管它们在关注的内容上可能有所不同。</p><h5 id="query-低秩">2.1.1.3. Query 低秩</h5><p>DeepSeek V2 还将 <span class="math inline">\(Q\)</span>也转化为了低秩矩阵，这并不会减少KV_Cache的存储量，文中称可以减少训练期间参数量和相应的梯度。</p><p><span class="math display">\[\begin{aligned}Q &amp;= [c^{&#39;}W^{Q&#39;}, c^{&#39;}W^{Qr}R^Q] \in\mathbb{R}^{d_{model} \times (d_{\text{k}}+dr})\\K &amp;= [cW^{K&#39;}, xW^{Kr}R^K] \in \mathbb{R}^{d_{model} \times(d_{\text{k}}+dr)}\\\end{aligned}\]</span></p><p>其中<spanclass="math inline">\(d_{c^{&#39;}}\)</span>是一个超参数，<spanclass="math inline">\(d_{c^{&#39;}}=1536\)</span>。</p><h4 id="kv_cache和flops的权衡">2.1.2 KV_Cache和FLOPs的权衡</h4><p><strong>减少部分KVCache，增加部分计算量，在保存全量KV_Cache和不保存KV_Cache之间取得了一个折中</strong>。如下：</p><p><strong>1. 训练阶段</strong> MLA基本相当于把MHA的每个Attention Head中的<span class="math inline">\(Q,K\)</span>维度从<spanclass="math inline">\(d_k\)</span>变为了<spanclass="math inline">\(d_c+dr\)</span>。</p><p><strong>2. 推理阶段</strong></p><p>由于<span class="math inline">\(d_{\textcolor{red}{c}} = 4\timesd_k\)</span>，实际上MLA在推理阶段做的这个转换，虽然能有效减少KVCache，但其推理的计算量是增加的。</p><h5 id="kv_cache">2.1.2.1 KV_Cache</h5><table><thead><tr><th style="text-align: center;">Attention Mechanism</th><th style="text-align: center;">KV_Cache per token</th><th style="text-align: center;">Capability</th></tr></thead><tbody><tr><td style="text-align: center;">MHA</td><td style="text-align: center;"><spanclass="math inline">\(2n_hd_hl\)</span></td><td style="text-align: center;">Strong</td></tr><tr><td style="text-align: center;">GQA</td><td style="text-align: center;"><spanclass="math inline">\(2n_gd_hl\)</span></td><td style="text-align: center;">Moderate</td></tr><tr><td style="text-align: center;">MQA</td><td style="text-align: center;"><spanclass="math inline">\(2d_hl\)</span></td><td style="text-align: center;">Weak</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">MLA</td><td style="text-align: center;"><span class="math inline">\((d_c+d_r)l\approx 4.5d_hl\)</span></td><td style="text-align: center;">Strong</td></tr></tbody></table><p>其中<span class="math inline">\(d_h\)</span>是head维度，<spanclass="math inline">\(n_h\)</span>是head数，<spanclass="math inline">\(n_g\)</span>是gqa head数。</p><p>所以大概等效于<spanclass="math inline">\(n_g=2.25\)</span>的GQA的KV_Cache，相比于8个head的GQA大概降低到了原来的0.28倍。</p><h5 id="flops">2.1.2.2 FLOPs</h5><p>因为正常来讲<span class="math inline">\(W^Q\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>，<span class="math inline">\(W^O\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_v \timesd_{\text{model}}}\)</span>。而在MLA中<spanclass="math inline">\(W^{Q&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_{\text{model}} \timesd_c}\)</span>，<spanclass="math inline">\(W^{O&#39;}\)</span>的大小是<spanclass="math inline">\(\mathbb{R}^{d_c \timesd_{\text{model}}}\)</span>。所以在推理阶段，MLA的计算量是增加的。</p><p><strong>具体增加了多少</strong></p><p>假设输入的batch为<span class="math inline">\(b \times s \timesd\)</span>，其中<spanclass="math inline">\(b\)</span>为batch_size，<spanclass="math inline">\(s\)</span>为输入长度，<spanclass="math inline">\(d\)</span>为嵌入维度</p><ol type="1"><li>Embedding层：查表操作，不涉及矩阵乘法，故不计入FLOPs</li><li>预测多分类头(logits)将尺寸为 d 的隐藏向量映射为词表大小：<spanclass="math inline">\([b \times s \times d] \times [d \times V] = [b\times s \times V]\)</span> —— <spanclass="math inline">\(2bsVd\)</span></li><li>Self-Attention层： <span class="math display">\[     Q&#39;=xW^{Q&#39;}\\     Attention =\text{softmax}(\frac{Q&#39;\textcolor{red}{c&#39;}^T}{\sqrt{d_k}}){\textcolor{red}{c}}\\     x_{out}= AW^{O&#39;}+x\]</span><ul><li>每个头的注意力计算：一下需要乘<spanclass="math inline">\(n_{heads}\)</span>次，每次计算如下：<ul><li>计算 Q’ 矩阵：<span class="math inline">\([b \times s \times d]\times [d \times d_{c&#39;}] = [b \times s \times d_{c&#39;}]\)</span>—— <span class="math inline">\(2bsdd_{c&#39;}\)</span></li><li>计算注意力权重<spanclass="math inline">\(Q&#39;{c&#39;}^T\)</span>：<spanclass="math inline">\([b \times s \times d_{c&#39;}] \times [b \times s\times d_{c&#39;}]^T = [b \times s \times s]\)</span> —— <spanclass="math inline">\(2bs^2d_{c&#39;}\)</span></li><li>汇聚价值信息<span class="math inline">\(Ac\)</span>: <spanclass="math inline">\([b \times s \times s] \times [b \times s \timesd_c] = [b \times s \times d_c]\)</span> —— <spanclass="math inline">\(2bs^2d_c\)</span></li></ul></li><li>拼接多头注意力: 不涉及矩阵乘法，故不计入FLOPs</li><li>输出矩阵<span class="math inline">\(O\)</span>: <spanclass="math inline">\([b \times s \times (n_{heads}\times d_c)] \times[(n_{heads}\times d_c) \times d] = [b \times s \times d]\)</span> ——<span class="math inline">\(2bsd(n_{heads}\times d_c)\)</span></li></ul></li><li>MLP层：<span class="math inline">\(16bsd^2\)</span> <spanclass="math display">\[     h=Relu(x_{out}W^1+b_1)\\     h_{out}=hW^2+b2\\     x=h_{out}+x_{out}\]</span><ul><li>第一个线性层：<span class="math inline">\([b \times s \times d]\times [d \times 4d] = [b \times s \times 4d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li><li>第二个线性层：<span class="math inline">\([b \times s \times 4d]\times [4d \times d] = [b \times s \times d]\)</span> —— <spanclass="math inline">\(8bsd^2\)</span></li></ul></li></ol><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr><th style="text-align: center;">模块</th><th style="text-align: center;">数量</th><th style="text-align: center;">单次FLOPs</th><th style="text-align: center;">总FLOPs</th></tr></thead><tbody><tr><td style="text-align: center;">Embedding</td><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;">0</td></tr><tr><td style="text-align: center;">LM Head(logits)</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2bsVd\)</span></td></tr><tr><td style="text-align: center;">Self-Attention</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(2bsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2blsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)\)</span></td></tr><tr><td style="text-align: center;">MLP</td><td style="text-align: center;">l</td><td style="text-align: center;"><spanclass="math inline">\(16bsd^2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(16blsd^2\)</span></td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">正常的MHA FLOPs</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;"><span class="math inline">\(2bsVd +24blsd^2 + 4bls^2d\)</span></td></tr></tbody></table><p>其中<span class="math inline">\(l\)</span>为层数，<spanclass="math inline">\(d\)</span>为嵌入维度，<spanclass="math inline">\(s\)</span>为输入长度，<spanclass="math inline">\(V\)</span>为词表大小，<spanclass="math inline">\(b\)</span>为batch_size，<spanclass="math inline">\(n_{heads}\)</span>为head数。</p><p><span class="math display">\[\begin{aligned}FLOPs(MLA/MHA)&amp;\approx\frac{16blsd^2+2blsn_{heads}(2dd_c+dd_r+2sd_c+sd_r)}{24blsd^2 +4bls^2d}\\&amp;= \frac{33d+17s}{24d+4s}(\textcolor{red}{存疑})\\\end{aligned}\]</span></p><h4 id="deepseek-v2-的moe">2.1.3 DeepSeek V2 的MOE</h4><p>另外 DeepSeek V2 是一个改进的MOE模型，部分改动如下：</p><ol type="1"><li>细粒度专家分割：通过将每个FFN专家进一步细分，这允许模型在保持参数总数不变的情况下，激活更多的、更细粒度的专家。这种策略使得各个专家能够专注于更细致的知识领域，提高了专家的专业化程度。</li><li>共享专家隔离：设置一部分专家作为“共享专家”，这些专家总是被激活，用于捕捉和整合常见的跨上下文知识。这样可以减少路由专家之间的知识冗余，避免重复知识，每个路由专家可以更专注于独特的知识领域。</li></ol><h3 id="改进的量化内核">2.2 改进的量化内核</h3><p>Transformers 的 Torch实现必须在处理之前将这些张量反量化为支持的数据类型，这会带来不必要的计算开销并增加内存流量。为了克服这个问题，我们加入了直接对量化数据类型进行操作的高级内核，从而优化了推理性能。</p><h3 id="计算强度引导的模型加载">2.3 计算强度引导的模型加载</h3><p>遵循按算术强度排列所有算子并尽可能将最密集的算子放在 GPU中的原则，策略性地只将计算最密集的参数存储在 GPU 上。</p><p>优先将 MoE 参数和词嵌入计算放在 CPU端以利用其更大的内存容量。其余参数（包括共享专家、注意模块中的投影和MLA）存储在 GPU VRAM 中。由于每个 token都会访问这些参数，因此将它们放置在 GPU上可以最大限度地发挥高内存带宽的优势。如果使用 Q4_K_M版本，此配置将导致大约 20.7 GB 的 VRAM 使用量和 136GB 的 DRAM内存请求，即使在本地桌面上也是可行的。</p><ol type="1"><li>MLA 运算符（包含 128 个头，具有共享的压缩键值表示）的算术强度为512。这使其成为最密集的运算符，尤其是在较小的推理批次大小期间。因此，它被分配给GPU 以利用张量核心。</li><li>DeepSeek-V2 中的每个转换器块包含 160 位混合专家 (MoE)专家，占总参数的 96%。但是，MoE 路由器为每个 token 仅激活这 160位专家中的 6 位。因此，此操作主要涉及批量通用矩阵向量乘法 (GEMV)，可以由CPU 高效处理。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><ahref="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA</a></li><li>知乎：如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2<ol type="1"><li><ahref="https://www.zhihu.com/question/655172528/answer/3491439374">回答1</a></li><li><ahref="https://www.zhihu.com/question/655172528/answer/3495218670">回答2</a></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之MInference</title>
      <link href="/2024/08/08/NLP/LLMModels/MInference/"/>
      <url>/2024/08/08/NLP/LLMModels/MInference/</url>
      
        <content type="html"><![CDATA[<h1 id="minference">MInference</h1><p>通过动态稀疏注意力加速长上下文llm的预填充 原文链接：<ahref="https://arxiv.org/abs/2407.02490">MInference 1.0: AcceleratingPre-filling for Long-Context LLMs via Dynamic Sparse Attention</a></p><h2 id="简介">1. 简介</h2><p>MInference是一种利用<strong>空间聚集模式</strong>的动态稀疏注意力来加速长序列预填充阶段的方法。</p><ol type="1"><li>将注意力头分为三种可以用于gpu上的高效稀疏计算类型: A-shape,Vertical-Slash, Block-Sparse。</li><li>采用核感知的最优稀疏模式搜索方法，离线确定每个注意力头的最优模式。</li><li>在推理过程中利用快速逼近方法为不同的输入构建动态稀疏掩码，然后应用这些掩码进行稀疏注意力计算。</li></ol><p>即<strong>只计算注意权值中最重要的部分</strong></p><h3 id="局限性">局限性</h3><ol type="1"><li>上下文长度较小时，构建动态索引所需的时间会因为注意力计算时间的减小而增大。</li><li>当使用较高的稀疏率时，模型性能可能会明显下降。</li></ol><h2 id="原理">2. 原理</h2><h3 id="稀疏注意力头类型">2.1 稀疏注意力头类型</h3><p>如图1和2所示，将注意力头分为<strong>三种类型</strong>。</p><ol type="1"><li><strong>A-shape</strong>:该模式的注意权重集中在初始令牌和局部窗口，表现出相对较高的稳定性。</li><li><strong>Vertical-Slash</strong>:该模式的注意权重集中在特定的标记(竖线)和固定间隔的标记(斜线)上。该图案中竖线和斜线的位置随上下文内容动态变化。</li><li><strong>Block-Sparse</strong>:该模式是最动态的，表现出更分散的分布，保持了空间聚类的一些特征。</li></ol><figure><img src="./images/attention_pattern.webp#50x50"title="图1: 注意力权重三种类型" alt="注意力权重三种类型" /><figcaption aria-hidden="true">注意力权重三种类型</figcaption></figure><figure><img src="./images/MInference_framework_crop.webp"title="图2: 简化三种类型" alt="简化三种类型" /><figcaption aria-hidden="true">简化三种类型</figcaption></figure><h3 id="稀疏注意力计算">2.2 稀疏注意力计算</h3><p>当使用稀疏注意力计算加速长上下文llm的预填充阶段时，<strong>注意力矩阵</strong>可以表示为:</p><p><span class="math display">\[\begin{equation}    \bm{A(M)} = \text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^\top -c(1-\bm{M}))    \tag{1}\end{equation}\]</span></p><p><span class="math inline">\(M_{i,j} \in \{0,1\}\)</span>表示注意矩阵第<spanclass="math inline">\({i,j}\)</span>项的动态稀疏掩码，<spanclass="math inline">\(c\)</span>是一个大的常数，确保softmax后<spanclass="math inline">\(M_{i,j} =0\)</span>那些不太重要的注意权重接近0。</p><p>所以<strong>目标函数</strong>为:</p><p><span class="math display">\[\begin{equation}    \begin{aligned}        \min \enspace &amp; \enspace \enspace |\bm{A}(\bm{M}) -\bm{A}_{\text{dense}}|, \\        \min \enspace &amp;  t_{\text{sparse}}(\bm{M}) +t_{\text{overhead}}(\bm{M}),    \end{aligned}    \tag{2}\end{equation}\]</span></p><h3 id="实现步骤">2.3 实现步骤</h3><h4 id="离线核感知最优稀疏模式搜索">2.3.1离线核感知最优稀疏模式搜索</h4><p>通过一个参考示例遍历搜索空间，以决定最优模式和设置。用于确定每个注意头将<strong>使用哪种稀疏模式</strong>，以及实际计算中<strong>模式的最佳设置</strong>(例如，VS模式中垂直/斜线的数量;或BS模式中top-k块的数量)</p><ol type="1"><li>初始化或获取全局的注意力掩码</li><li>按照公式(1)计算注意力权重</li><li>初始化最佳分数和对应参数</li><li>遍历每种模式（stream_llm、vertical_and_slash、block_sparse）及其参数组合，计算分数(类似于卷积核提取特定模式的特征？)<ol type="1"><li>vertical_and_slash垂直注意力：对查询-键点积进行softmax，累加所有列的注意力权重，并选择top-k列。斜线注意力：计算所有斜线元素的和，选择top-k斜线。合并：将垂直和斜线注意力矩阵组合。</li><li>stream_llm掩码计算：生成上下三角掩码矩阵，设置指定大小的垂直和斜线区域。应用掩码：将掩码应用于注意力权重矩阵。</li><li>block_sparse 块划分：将查询和键分成多个块，并对每个块进行池化。 选择top-k：计算块级别的注意力权重，选择前top-k个块。合并块稀疏注意力：将块稀疏矩阵扩展到原始维度</li></ol></li><li>记录所有信息，并更新最佳分数和参数</li><li>选择每个注意力头的最佳模式</li></ol><h4 id="稀疏度指标逼近与动态稀疏注意力计算">2.3.2稀疏度指标逼近与动态稀疏注意力计算</h4><p>在推理阶段，根据分配的模式和准确的输入对注意力矩阵进行在线估计，以动态确定我们的稀疏指数的空间分布。</p><figure><img src="./images/algo2.webp#100x100"title="图4: 稀疏度指标逼近与动态稀疏注意力计算"alt="稀疏度指标逼近与动态稀疏注意力计算" /><figcaptionaria-hidden="true">稀疏度指标逼近与动态稀疏注意力计算</figcaption></figure><h5 id="vertical-slash">1) Vertical-Slash</h5><ol type="1"><li>生成估计的注意力矩阵 <spanclass="math inline">\(\boldsymbol{\hat{A}}\)</span>:由于垂直线和斜线的连续性，我们将最后一个查询向量 <spanclass="math inline">\(\bm{Q}_{[-\text{last\_q}:]}\)</span> 和键向量<span class="math inline">\(\bm{K}\)</span> 进行矩阵乘法</li><li>用<spanclass="math inline">\(\boldsymbol{\hat{A}}\)</span>来确定垂直线 <spanclass="math inline">\(\bm{i}_v\)</span> 和斜线 <spanclass="math inline">\(\bm{i}_s\)</span> 的索引。</li><li>将垂直线和斜线的稀疏索引转换为稀疏格式 <spanclass="math inline">\(\bm{i}_{vs}\)</span>。</li><li>利用这些稀疏索引，我们执行注意力权重和注意力输出的块稀疏计算。</li></ol><h5 id="block-sparse">2) Block-Sparse</h5><ol type="1"><li>对 <span class="math inline">\(\bm{Q}\)</span> 和 <spanclass="math inline">\(\bm{K}\)</span> 应用均值池化以获得 ( ) 和 ()。</li><li>将这两个矩阵相乘以得到估计的块级注意力权重 <spanclass="math inline">\(\bm{\hat{A}}\)</span>。由于均值池化和矩阵乘法操作是可交换的，结果注意力权重大致等同于均值池化后的实际注意力权重。这使得我们能够以最小的开销近似实际注意力权重的块稀疏模式。</li><li>构建一个稀疏索引 <spanclass="math inline">\(\bm{i}_b\)</span>，并使用它来计算稀疏注意力权重和注意力输出。</li></ol><h4 id="利用优化后的gpu-kernel进行稀疏注意力计算">**2.3.3利用优化后的GPU kernel进行稀疏注意力计算*</h4><h5 id="stream-llm"><em>Stream-LLM</em></h5><ol type="1"><li>初始化块范围和掩码：根据滑动窗口和块大小计算要处理的键和值向量的列范围。</li><li>循环处理块：遍历范围内的块，加载对应的键和值向量。</li><li>计算查询-键点积并应用掩码：计算查询和键向量的点积，并应用滑动窗口掩码确保注意力机制的方向性。</li><li>更新累加器和softmax计算：计算softmax概率并更新累加器，最终得到注意力权重的加权和。</li></ol><h5 id="block-sparse-1"><em>Block-Sparse</em></h5><ol type="1"><li>初始化掩码和块计数：确定哪些查询向量在当前块内有效，并计算需要处理的块数量。</li><li>循环处理稀疏块：遍历每个稀疏块，加载相应的块索引和列索引。</li><li>加载键和值向量的块：使用生成的列索引加载对应的键向量和值向量块。</li><li>计算查询和键的点积，并应用因果掩码：计算查询和键向量的点积，并应用因果掩码以确保注意力机制的方向性。</li><li>更新累加器和softmax计算：通过softmax计算和累加更新累加器，最终得到注意力权重的加权和。</li><li>写回输出：将累加和除以归一化因子后写回输出。</li></ol><h5 id="vertical-slash-1"><em>Vertical-Slash</em></h5><ol type="1"><li><strong>索引排序和范围计算</strong>：<ul><li>根据竖线和斜线索引排序，并计算每个块的范围，确定哪些元素需要进行注意力计算。</li></ul></li><li><strong>块内并行计算</strong>：<ul><li>在每个块内分别计算竖线部分和斜线部分的注意力分数，通过加载查询、键、值向量块并进行点积计算。</li><li>计算竖线注意力分数</li></ul><ol type="1"><li>循环遍历块索引 block_index，在每个块内计算竖线部分的注意力分数qk。</li><li>使用 tl.dot(q, k) 计算查询向量和键向量的点积，得到注意力分数。</li><li>应用因果掩码 causal_mask确保仅计算当前时间步及之前的时间步，并排除掉不相关的未来时间步。</li><li>计算新的最大值 m_i_new 和比例因子 alpha 来更新注意力分数。</li><li>通过 p 加权并更新累加器 acc。</li></ol><ul><li>计算斜线部分</li></ul><ol type="1"><li>在另一个循环中，通过列索引 start_n遍历块内所有列，计算斜线部分的注意力分数和累加值。</li><li>同样使用 tl.dot(q, k) 计算查询向量和键向量的点积，并应用掩码操作m_mask 和 n_mask 确保计算的正确性。</li><li>计算新的最大值 m_i_new 和比例因子 alpha，并通过 p 更新累加器acc。</li></ol></li><li><strong>掩码应用和归一化</strong>：<ul><li>应用掩码确保只计算相关部分的注意力分数，并进行最大值更新和归一化处理。</li></ul></li><li><strong>最终输出</strong>：<ul><li>将归一化后的注意力输出写回到最终输出张量中。</li></ul></li></ol><h6 id="获取-vertical-slash-index-返回块计数块索引列计数和列索引">获取Vertical-Slash Index: 返回块计数、块索引、列计数和列索引</h6><ol type="1"><li><strong>排序竖线和斜线索引</strong>：<ul><li>对竖线索引进行增量排序 <code>IncrementalSort(i_v)</code></li><li>对斜线索引进行降序排序 <code>DescendingSort(i_s)</code></li></ul></li><li><strong>计算块数</strong>：<ul><li>计算块数量 ( N )</li></ul></li><li><strong>初始化输出</strong>：<ul><li>初始化块计数 <code>block count</code> ( c_{} ^{N} )</li><li>初始化块索引 <code>block index</code> ( i_{} ^{N k_v} )</li><li>初始化列计数 <code>column count</code> ( c_{} ^{N} )</li><li>初始化列索引 <code>column index</code> ( i_{} ^{N k_s} )</li></ul></li><li><strong>并行化处理</strong>：<ul><li>对每个块进行遍历，找到交叉竖线和斜线的位置，记录范围并更新块索引和列索引。</li></ul></li><li><strong>合并点（竖线索引和斜线索引范围）</strong>：<ul><li>通过合并点和范围来更新块和列信息。</li></ul></li></ol><h6 id="获取-vertical-slash-flash-attention">获取 Vertical-Slash FlashAttention</h6><ol type="1"><li><strong>初始化</strong>：<ul><li>缩放因子 ( )</li><li>初始化输出 ( O (0)^{S d_h} )</li></ul></li><li><strong>并行化处理（竖线部分）</strong>：<ul><li>对每个块进行遍历，加载查询块 ( Q_{} )，初始化输出块 ( O_{} )</li><li>初始化最大值 ( m ()^{B} ) 和累计量 ( l (0)^{B} )</li><li>遍历块索引，加载键和值块，计算注意力分数 ( S Q_{} K_{}^T )</li><li>应用掩码和缩放，更新最大值和累计量，计算加权和更新输出块 ( O_{})</li></ul></li><li><strong>并行化处理（斜线部分）</strong>：<ul><li>遍历列索引，加载键和值块，计算注意力分数 ( S Q_{} K_{}^T )</li><li>应用掩码和缩放，更新最大值和累计量，计算加权和更新输出块 ( O_{})</li></ul></li><li><strong>写回输出</strong>：<ul><li>对输出块进行归一化 ( O_{} (l^{-1}) O_{} )</li><li>保存最终输出 ( O_i O_{} )</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之Mistral</title>
      <link href="/2024/08/08/NLP/LLMModels/Mistral/"/>
      <url>/2024/08/08/NLP/LLMModels/Mistral/</url>
      
        <content type="html"><![CDATA[<h1 id="mistral-ai">Mistral AI</h1><h2 id="mistral-7b">Mistral-7B</h2><ol type="1"><li>采用了分组查询注意力(GQA)，显著加快了推理速度，还减少了解码期间的内存需求，允许更高的批处理大小，从而提高吞吐量</li><li>结合滑动窗口注意力(SWA)以有效处理任意长度的序列<ol type="1"><li>每个token最多可以关注来自上一层的W个token(注，滑动窗口之外的token仍然影响下一个单词预测)</li><li>固定的注意力长度意味着可以使用滚动缓存来限制的缓存大小</li></ol></li></ol><h2 id="mixtral-8x7b">Mixtral-8x7B</h2><figure><img src="./images/Mixtral8x7B.webp#50x50" title="图1：Mistral-8x7B"alt="Mistral-8x7B" /><figcaption aria-hidden="true">Mistral-8x7B</figcaption></figure><ol type="1"><li>FFN从一组 8 个不同的参数组中进行选择</li><li>在每一层，对于每个token，路由器网络选择其中的两个组(“专家”)来处理token并通过组合相加得到它们的输出</li><li>在各个层中仅有experts部分(FFN)是独立存在的，其余的部分(Attention等)则是各个expert均有共享的</li><li>路由(Gating/Router)本质是一个线性层，输入维度为隐层维度hidden_dim、输出维度为expert数num_experts。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之T-MAC</title>
      <link href="/2024/08/08/NLP/LLMModels/T-MAC/"/>
      <url>/2024/08/08/NLP/LLMModels/T-MAC/</url>
      
        <content type="html"><![CDATA[<h1 id="t-mac">T-MAC</h1><p>CPU上通过查找表进行低比特量化大模型部署 原文链接：<ahref="https://arxiv.org/abs/2407.00088">CPU Renaissance via Table Lookupfor Low-Bit LLM Deployment on Edge</a> GitHub链接：<ahref="https://github.com/microsoft/T-MAC">T-MAC</a></p><h2 id="问题提出">问题提出</h2><p>权重量化对于减少 LLM 在设备上的内存占用至关重要。然而，低位 LLM需要在推理过程中进行低精度权重和高精度激活的混合精度矩阵乘法(mpGEMM)。现有系统缺乏对 mpGEMM的原生支持，只能通过去量化权重来实现高精度计算。这种间接的方式可能会导致显着的推理开销。</p><h2 id="解决方案">解决方案</h2><figure><img src="./images/overview.webp#70x70" title="图1: T-MAC Overview"alt="T-MAC Overview" /><figcaption aria-hidden="true">T-MAC Overview</figcaption></figure><h3 id="t-mac简介">T-MAC简介</h3><p>一种基于查找表 (LUT)的方法，将传统的以数据类型为中心的<strong>乘法</strong>转换为<strong>按位表查找</strong>，专为CPU 上的高效低位 LLM（即权重量化 LLM）推理而设计。</p><p>直接支持 mpGEMM，无需反量化，简化为查表+加法操作</p><figure><img src="./images/fig_intro.webp#70x70"title="图2: T-MAC vs general practice for mpGEMM"alt="T-MAC vs general practice for mpGEMM" /><figcaption aria-hidden="true">T-MAC vs general practice formpGEMM</figcaption></figure><p>两个数的乘法可以转化为<strong>一个数</strong>乘以<strong>另一个数的每一位</strong>，然后<strong>移位并相加</strong>部分积。激活矩阵和权重矩阵之间的mpGEMM 被分解为激活矩阵和一位矩阵之间的 mpGEMM的一系列（=权重的位宽），然后将部分结果相加。因此，该方法可以支持激活和权重的任何位宽组合。</p><p><span class="math display">\[\begin{equation}A\times W=A\times(\sum^{n-1}_{i=0}2^i W_i)=\sum^{n-1}_{i=0}2^i A\timesW_i\end{equation}\]</span></p><p>对于混合精度 GEMM，𝐴 和 𝑊 分别是激活矩阵和权重矩阵。𝑛 是权重的位宽。𝑊𝑖 是 𝑊 的每一位矩阵。</p><h3 id="原理">原理</h3><h4 id="按位布局和乘法">按位布局和乘法</h4><p>整个计算的流程大概如下：</p><figure><img src="./images/LUTGEMV.webp#80x80"title="图3: LUT example for mpGEMM" alt="LUT example for mpGEMM" /><figcaption aria-hidden="true">LUT example for mpGEMM</figcaption></figure><h5 id="离线准备阶段">离线准备阶段</h5><p>在离线准备阶段，<code>n-bit</code>权重矩阵被分解为<code>n个1-bit</code>矩阵。由于<code>1-bit</code>只能代表两个值，对于具有<code>𝑔-bit</code>的组，可能的排列只有<spanclass="math inline">\(2^𝑔\)</span>。表大小为<spanclass="math inline">\([1, 2^𝑔]\)</span>。</p><p>在这里，表的大小是根据权重的行数来确定的，与权重的位宽无关。例如，对于一个<code>8-bit</code>且<code>g</code>行的权重矩阵，会分为<code>8</code>个一样的<code>1-bit</code>矩阵，矩阵的大小为<spanclass="math inline">\([1, 2^g]\)</span>。</p><h5 id="在线阶段">在线阶段</h5><p>对于在线阶段，给定 GEMM 的输入激活，T-MAC 构建一个表。在 LUT期间，一位权重矩阵的每个索引用于查找部分结果的表。部分结果的累加将是最终的GEMM结果。</p><p>和右边表的对应关系如下：</p><ol type="1"><li>左边是0001的时候，需要计算<code>D</code>的值</li><li>左边是0011的时候，需要计算<code>C + D</code>的值</li></ol><p>表中的<code>-</code>代表舍弃的值，因为这些值在计算中不会被用到。</p><h4 id="挑战">挑战</h4><ol type="1"><li>随机数据访问。有必要将表存放在快速on-chipmemory中以降低访问成本。</li><li>需要更多on-chipmemory。查找表需要<strong>保存激活向量</strong>与所有<strong>可能的位模式相乘的中间结果</strong>以组合为真正的结果。</li></ol><h5 id="以-lut-为中心的数据布局">以 LUT 为中心的数据布局</h5><ol type="1"><li><strong>将查找表放在on-chip memory上</strong> 利用 CPU上的查表向量指令 (TBL/PSHUF) 提升随机访存性能</li><li><strong>改变矩阵 axis计算顺序</strong>，以尽可能提升放入片上内存的有限 LUT的数据重用率。</li><li><strong>为查表单独设计最优矩阵分块</strong> (Tiling) 方式</li></ol><h5 id="表压缩方法">表压缩方法</h5><ol type="1"><li><strong>镜像整合</strong>。 LLM推理的查找表上下文中表值固有的对称属性提供了独特的优化机会。表中的每个正值自然地与其对应的负值配对，反映零值上的镜像。</li><li><strong>表量化</strong>。表量化的运行原理类似于权重和激活量化，旨在降低表值的精度以提高计算效率。例如：最初在查找表中以16 位浮点 (fp16) 表示的值可以通过缩放因子量化为 8 位整数 (int8)。</li></ol><p>实验宣称：表压缩对模型的推理精度的影响可以忽略不计，但是可以显著提高推理性能。</p><figure><img src="./images/table_reduction.webp#50x50"title="图4: 镜像合并使表长度减半;表量化减少了表宽度"alt="镜像合并使表长度减半。表量化减少了表宽度" /><figcaptionaria-hidden="true">镜像合并使表长度减半。表量化减少了表宽度</figcaption></figure><h2 id="评估">评估</h2><h3 id="精度损失">精度损失</h3><p>T-MAC为模型推理引入的误差可以忽略不计，同时提供显着的加速。快速聚合(FA)可以进一步提高性能，但代价是模型质量。</p><h4 id="相对于未量化𝑊𝐹𝑃16𝐴𝐹𝑃16gemv内核的归一化均方误差nmse">1.相对于未量化(𝑊𝐹𝑃16𝐴𝐹𝑃16)GEMV内核的归一化均方误差NMSE</h4><table><colgroup><col style="width: 21%" /><col style="width: 26%" /><col style="width: 22%" /><col style="width: 30%" /></colgroup><thead><tr><th style="text-align: center;"><spanclass="math inline">\(\textbf{MxKxN}\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{llama.cpp}\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{T-MAC}\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{T-MAC(+FA)}\)</span></th></tr></thead><tbody><tr><td style="text-align: center;">4096x4096x1</td><td style="text-align: center;">3.33e-03</td><td style="text-align: center;">3.35e-03</td><td style="text-align: center;">8.09e-03</td></tr><tr><td style="text-align: center;">11008x4096x1</td><td style="text-align: center;">3.44e-03</td><td style="text-align: center;">3.46e-03</td><td style="text-align: center;">8.27e-03</td></tr><tr><td style="text-align: center;">4096x11008x1</td><td style="text-align: center;">4.13e-03</td><td style="text-align: center;">4.15e-03</td><td style="text-align: center;">8.45e-03</td></tr></tbody></table><h4 id="wikitext-2-和-ambada_openai-的困惑度winogrande-的问答准确性">2.WikiText-2 和 ambada_openai 的困惑度，WinoGrande 的问答准确性</h4><table><colgroup><col style="width: 20%" /><col style="width: 20%" /><col style="width: 18%" /><col style="width: 23%" /><col style="width: 18%" /></colgroup><thead><tr><th style="text-align: center;"><spanclass="math inline">\(\textbf{Framework}\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{Throughput}\)</span> <br> <spanclass="math inline">\(Tokens/sec \uparrow\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{WikiText2}\)</span> <br> <spanclass="math inline">\(PPL\downarrow\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{lambada\_openai}\)</span> <br> <spanclass="math inline">\(PPL\downarrow\)</span></th><th style="text-align: center;"><spanclass="math inline">\(\textbf{WinoGrande}\)</span> <br> <spanclass="math inline">\(Acc.\uparrow\)</span></th></tr></thead><tbody><tr><td style="text-align: center;">Un-quantized</td><td style="text-align: center;">3.79</td><td style="text-align: center;">5.80</td><td style="text-align: center;">12.65</td><td style="text-align: center;">71.0</td></tr><tr><td style="text-align: center;">llama.cpp</td><td style="text-align: center;">5.65</td><td style="text-align: center;">5.96</td><td style="text-align: center;">12.95</td><td style="text-align: center;">70.8</td></tr><tr><td style="text-align: center;">T-MAC</td><td style="text-align: center;">7.34</td><td style="text-align: center;">5.96</td><td style="text-align: center;">12.95</td><td style="text-align: center;">70.8</td></tr><tr><td style="text-align: center;">T-MAC (+FA)</td><td style="text-align: center;">8.97</td><td style="text-align: center;">6.38</td><td style="text-align: center;">13.99</td><td style="text-align: center;">67.8</td></tr></tbody></table><h3 id="性能对比">性能对比</h3><p>在GPU上使用查找表尽管理论上计算复杂度降低了，但实际的内核性能比基于反量化的内核要差。这可能归因于GPU固定架构的限制，该架构为查找表提供的存储容量不足或表访问速度不够快。</p><p>因此本文探索了 CPU 上的查找表内核。</p><h4 id="逐步应用-t-mac-优化">1. 逐步应用 T-MAC 优化</h4><p>通过逐步应用 T-MAC 优化，Llama-2-7B/13B GEMV 内核在 M2-Ultra上的多线程性能如下图所示：</p><figure><img src="./images/ablation.webp#80x80" title="图5: 性能对比"alt="Performance comparison" /><figcaption aria-hidden="true">Performance comparison</figcaption></figure><p>S0-S5：不同的矩阵形状，TM：T-MAC，TQ：表量化，Perm.：排列，IL：交织，FA：快速聚合。</p><h4 id="和骁龙-x-elite-npu芯片的性能对比">2. 和骁龙 X-ELiteNPU芯片的性能对比</h4><p>X-ELite NPU 只能生成 10.4 个 token/sec，而使用 T-MAC 的 CPU在两核的情况下就能达到 12.6 个 token/sec，甚至最高能达到 22 个token/sec。考虑到 T-MAC的计算性能能随着位数的减少而线性提升（这在基于反量化的 GPU 和 NPU上是观察不到的），T-MAC 甚至可以在 2 个 bit 下匹敌单核 CPU 的 NPU。</p><table><colgroup><col style="width: 23%" /><col style="width: 23%" /><col style="width: 18%" /><col style="width: 34%" /></colgroup><thead><tr><th>Framework</th><th>Model</th><th>NUM_THREADS</th><th style="text-align: left;">Throughput <br> (tokens/sec)</th></tr></thead><tbody><tr><td>T-MAC (CPU)</td><td>llama-2-7b (W4)</td><td>2</td><td style="text-align: left;"><b>12.6</b></td></tr><tr><td>T-MAC (CPU)</td><td>llama-2-7b (W4)</td><td>4</td><td style="text-align: left;"><b>18.7</b></td></tr><tr><td>T-MAC (CPU)</td><td>llama-2-7b (W2)</td><td>1</td><td style="text-align: left;">9.3</td></tr><tr><td>T-MAC (CPU)</td><td>llama-2-7b (W2)</td><td>4</td><td style="text-align: left;"><b>28.4</b></td></tr><tr><td></td><td></td><td></td><td style="text-align: left;"></td></tr><tr><td>NPE (NPU)</td><td>llama-2-7b (W4)</td><td>-</td><td style="text-align: left;">10.4</td></tr></tbody></table><h4 id="和-nvidia-jetson-agx-orin上的性能对比">3. 和 NVIDIA Jetson AGXOrin上的性能对比</h4><p>NVIDIA Jetson AGX Orin 上的 Llama-2-7B (W2)的吞吐量/功率/能量比较（CPU 的 NUM_THREADS=12）</p><table style="width:100%;"><colgroup><col style="width: 23%" /><col style="width: 34%" /><col style="width: 17%" /><col style="width: 24%" /></colgroup><thead><tr><th>Framework</th><th style="text-align: left;">Throughput (tokens/sec)</th><th style="text-align: left;">Power (W)</th><th style="text-align: left;">Energy (J/token)</th></tr></thead><tbody><tr><td>llama.cpp (CPU)</td><td style="text-align: left;">7.08</td><td style="text-align: left;">15.0</td><td style="text-align: left;">2.12</td></tr><tr><td>llama.cpp (GPU)</td><td style="text-align: left;"><b>20.03</b></td><td style="text-align: left;">30.8</td><td style="text-align: left;">1.54</td></tr><tr><td>T-MAC (CPU)</td><td style="text-align: left;">15.62</td><td style="text-align: left;"><b>10.4</b></td><td style="text-align: left;"><b>0.66</b></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之PowerInfer</title>
      <link href="/2024/08/08/NLP/LLMModels/PowerInfer/Introduction/"/>
      <url>/2024/08/08/NLP/LLMModels/PowerInfer/Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="powerinfer">PowerInfer</h1><figure><img src="./images/PowerInfer.svg" alt="PowerInfer" /><figcaption aria-hidden="true">PowerInfer</figcaption></figure><h2 id="powerinfer-1.0">PowerInfer 1.0</h2><p>PowerInfer 是一款在配备单个消费级 GPU 的个人计算机 (PC)上运行的高速大型语言模型 (LLM) 推理引擎。PowerInfer 设计的关键在于利用LLM 推理固有的高局部性 ，其特点是神经元激活呈现幂律分布。</p><p>这种分布表明，一小部分神经元（称为热神经元）在输入过程中始终处于激活状态，而大多数神经元（冷神经元）则根据特定输入而变化。PowerInfer利用这种洞察力设计了 GPU-CPU 混合推理引擎：热激活神经元预加载到 GPU上以便快速访问，而冷激活神经元则在 CPU 上计算，从而显著减少 GPU内存需求和 CPU-GPU 数据传输。PowerInfer进一步集成了自适应预测器和神经元感知稀疏运算符，优化了神经元激活和计算稀疏性的效率。</p><ul><li>以局部为中心的设计：利用稀疏激活和“热”/“冷”神经元概念进行高效的 LLM推理，确保在较低的资源需求下实现较高的速度。</li><li>混合 CPU/GPU 利用率：无缝集成 CPU 和 GPU的内存/计算能力，实现工作负载平衡和处理速度更快。</li></ul><h2 id="powerinfer-2.0">PowerInfer 2.0</h2><p>PowerInfer-2是一个为智能手机上大型语言模型(LLM)高速推理而设计的框架，特别适用于大小超过设备内存容量的模型。核心是将LLM推理中典型的粗粒度矩阵计算分解为细粒度神经元集群计算。PowerInfer-2以神经元簇的粒度进行计算和I/O操作，神经元簇可以在计算过程中动态地由多个激活的神经元组成，神经元的数量由计算单元的计算能力决定，从而可以充分利用具有不同计算能力的XPU。</p><ul><li>在线部分提供神经元簇粒度的推理，包括四个协作组件：多态神经元引擎、内存中神经元缓存、灵活的神经元加载和神经元簇级别I /O 管道。</li><li>离线部分描述在线推理中涉及的每个组件的具体配置并指导在线过程。</li></ul><h2 id="turbo-sparse">Turbo Sparse</h2><p>提出了一种新的基于drelu的稀疏化方法，在保持性能的同时，将模型稀疏性提高到90%，在推理中实现了2-5倍的加速。</p><ul><li>将 ReLU 化过程中的原始基于 SwiGLU 的前馈网络（FFN）替换为基于 dReLU的前馈网络（FFN）。</li><li>在第一种改进的基础上，通过稀疏机制进一步优化模型性能，通过控制稀疏水平来调整模型的激活值。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之PowerInfer1.0</title>
      <link href="/2024/08/08/NLP/LLMModels/PowerInfer/PowerInfer1/"/>
      <url>/2024/08/08/NLP/LLMModels/PowerInfer/PowerInfer1/</url>
      
        <content type="html"><![CDATA[<h1 id="powerinfer">PowerInfer</h1><p>在消费级 GPU 上的快速大语言模型推理 原文链接：<ahref="https://arxiv.org/abs/2312.12456">PowerInfer: Fast Large LanguageModel Serving with a Consumer-grade GPU</a></p><h2 id="简介">1 简介</h2><p><strong>利用LLM推理中固有的高局部性特性:一个小子集的神经元（称为热神经元）在各种输入下始终被激活，而大多数神经元（称为冷神经元）则根据特定输入而变化。</strong></p><p>PowerInfer利用这一见解设计了一个GPU-CPU混合推理引擎：</p><ol type="1"><li>将热激活神经元预加载到GPU中以便快速访问，而冷激活神经元则在CPU上进行计算，从而显著减少了GPU内存需求和CPU-GPU数据传输。</li><li>集成自适应预测器和神经元感知稀疏算子，优化了神经元激活和计算稀疏性的效率。<ol type="1"><li>自适应:在DejaVu的基础上自适应两层MLP预测器(X-H-O)中隐藏层H的维度</li><li>神经元感知:预测出激活的神经元，得到要算出这个神经元需要哪一行/列的权重，然后只计算这一行/列的权重</li></ol></li></ol><p>模型越大，优势越明显:即主要针对的是模型比显存大时不能完全offload到GPU的情况。</p><h3 id="核心">1.1 核心</h3><p><strong>将少数的热神经元分配给GPU，而多数的冷神经元由CPU管理(GPU预加载经常激活的神经元的权重，而不太活跃的神经元的权重保留在CPU上)。离线预选择和预加载热激活神经元，并在运行时利用在线预测器来预测激活的神经元。</strong></p><h2 id="背景">2 背景</h2><h3 id="稀疏激活的可能性">2.1 稀疏激活的可能性</h3><figure><img src="./images/MLP.webp#60x60"title="图1: 激活的神经元表示为由红线包围的绿色行或列， FC1 的输出向量提供给 FC2 作为其输入向量。"alt="稀疏激活" /><figcaption aria-hidden="true">稀疏激活</figcaption></figure><p>在图 1 中，MLP 块的层 FC1 和 FC2通过矩阵乘法生成向量。每个输出元素都来自输入向量和神经元的点积（权重矩阵中的行/列）。ReLU等激活函数充当门，选择性地保留或丢弃向量中的值，影响 FC1 和 FC2中的神经元激活情况。</p><h3 id="对llm推理中的局部性的洞察">2.2 对LLM推理中的局部性的洞察</h3><p><strong>幂律激活</strong>LLM推理表现出高度的局部性，表明一组一致的神经元经常被激活，从而可以分为热神经元和冷神经元。</p><p><strong>CPU内快速计算</strong>如果激活的神经元驻留在CPU内存中，则在CPU上计算它们比将它们转移到GPU更快。</p><h2 id="powerinfer-设计">3 PowerInfer 设计</h2><ul><li>为了减少推理延迟，在推理过程中<strong>仅计算在线预测器预测为活动的神经元</strong>。</li><li>预加载策略使PowerInfer能够将<strong>大部分推理任务分配给GPU</strong>，因为加载在GPU上的热激活神经元构成了激活的很大一部分。</li><li>对于<strong>冷激活神经元，在需要计算时仅在 CPU上执行</strong>，消除了权重转移到 GPU 的需求。</li></ul><h3 id="架构与工作流程">3.1 架构与工作流程</h3><figure><img src="./images/architecture.webp#80x80" title="图2: PowerInfer架构"alt="PowerInfer架构" /><figcaption aria-hidden="true">PowerInfer架构</figcaption></figure><p>图 2 显示了 PowerInfer 的架构，包括离线和在线组件。</p><ul><li>离线组件分析llm的激活稀疏性，区分热神经元和冷神经元。</li><li>在线阶段，推理引擎将两种类型的神经元加载到 GPU 和 CPU中，并执行相应的计算。</li></ul><p><strong>LLM分析器和策略求解器(离线)</strong>: 该组件包括一个 LLM分析器，使用通用数据集得到。它监控所有层的神经元激活（步骤①），然后是将神经元分类为热或冷的策略求解器。求解器旨在将频繁激活的神经元分配给GPU，将其他神经元分配给CPU。在划分过程中使用整数线性规划来平衡工作负载（步骤 ②）。</p><p><strong>神经元感知LLM推理引擎(在线)</strong>:在处理用户请求之前，根据离线求解器的输出，在线引擎将两种类型的神经元分配到各自的处理单元(步骤③)。在运行时，引擎创建GPU 和 CPU 执行器，它们是运行在 CPU 侧的线程，以管理并发 CPU-GPU计算（步骤 ④）。该引擎还预测神经元激活并跳过非激活神经元。在 GPU内存中预加载的激活神经元在那里进行处理，而 CPU将其神经元的结果计算并传输到 GPU 以进行集成。该引擎在 CPU 和 GPU上使用稀疏神经元感知算子，<strong>专注于矩阵内的单个神经元行/列</strong>。</p><h3 id="单层示例">3.2 单层示例</h3><p>图 3 说明了 PowerInfer 在处理层神经元时如何协调 GPU 和 CPU。</p><ol type="1"><li>根据离线数据对神经元进行分类，将热激活神经元（索引 3、5、7）分配给GPU 内存，将其他神经元分配给 CPU 内存。</li><li>在接收到输入后，预测器识别当前层中的哪些神经元可能被激活。例如，它预测神经元3、4和5的激活。</li><li>但是通过离线统计分析识别的热激活神经元可能不能一致地匹配运行时激活行为。例如，在这种情况下，神经元7 虽然被标记为热激活，但被预测为不活跃。</li><li>然后 CPU 和 GPU都处理预测的活动神经元，忽略非活动神经元。GPU计算神经元3和5，而CPU处理神经元4。</li><li>一旦神经元4的计算完成，其输出被发送到GPU进行结果集成。</li></ol><figure><img src="./images/example.webp#80x80" title="图3: 单层示例"alt="单层示例" /><figcaption aria-hidden="true">单层示例</figcaption></figure><h2 id="神经元感知推理引擎">4 神经元感知推理引擎</h2><p><strong>PowerInfer中的在线推理引擎通过仅处理那些被预测为激活的神经元来减少计算负载。</strong></p><h3 id="自适应稀疏预测器">4.1 自适应稀疏预测器</h3><p>预测器的大小受两个主要因素的影响：LLM层的稀疏性及其内部偏度。(具有较高激活稀疏性或者高偏度的层简化了识别激活神经元的任务，从而允许较小的预测模型。)</p><p>PowerInfer在DejaVu的基础上为每个Transformer层设计了一种非固定大小的预测器的迭代训练方法(<strong>其实就是自适应两层MLP预测器(X-H-O)中隐藏层H的维度</strong>):</p><ul><li>首先基于层的稀疏配置文件建立基线预测器模型大小。</li><li>随后，在考虑内部激活偏度以保持准确性的情况下，迭代地调整模型大小。(MLP预测器通常包括输入、隐藏层和输出层。由于输入和输出层的维度由 Transformer层的结构决定，因此修改主要针对隐藏层)</li><li>在迭代调整过程中，隐藏层的维数根据观测到的偏度进行修改。(对于具有明显偏度的层，隐藏层的大小逐渐减小，直到精度降至95%以下。相反，对于具有最小偏度的层，则增加尺寸以提高精度。)</li></ul><p>通过这种方法，PowerInfer有效地将MLP预测器参数限制在LLM总参数的10%以内。</p><h3 id="神经元放置和管理">4.2 神经元放置和管理</h3><p>把神经元拆分开了，那计算的时候如何确保计算位置是对的？</p><p>创建两个神经元表，一个位于 CPU 中，另一个位于 GPU内存中。这些表将每个神经元与其矩阵中的原始位置相关联。在与输入张量相乘的过程中，由神经元表指导。</p><h3 id="gpu-cpu混合执行">4.3 GPU-CPU混合执行</h3><p>GPU-CPU两个单元独立计算它们各自的激活神经元，然后在 GPU上组合结果。</p><ol type="1"><li>在推理之前，构造一个计算图，每个节点代表一个 LLM推理算子，并将其存储在 CPU内存中的全局队列中。队列中的每个运算符都标有其先决条件运算符。</li><li>在推理过程中，主机操作系统创建的两种类型的执行器 pthreads 管理 CPU和 GPU的计算。它们从全局队列、检查依赖项中提取运算符并将它们分配给适当的处理单元。</li><li>GPU 和 CPU 使用他们的神经元感知运算符，GPU 执行器使用cudaLaunchKernel 等 API 启动 GPU 运算符，以及协调未占用 CPU内核以进行计算的 CPU 执行器。在执行运算符之前，CPU执行器还确定并行计算的必要线程计数。</li><li>为了管理运算符依赖关系，尤其是当 CPU 运算符的父节点在 GPU上处理时，确保 GPU 计算在 CPU 启动其运算符之前完成。</li></ol><p><strong>GPU和CPU选择性同步策略</strong>:在一个单元完成其神经元计算后，它等待另一个合并结果。由于 GPU神经元更频繁地被激活，PowerInfer 将合并操作分配给GPU。当CPU执行器没有激活神经元时绕过结果同步，使其能够继续后续块，从而提高整体效率。</p><h3 id="神经元感知算子">4.4 神经元感知算子</h3><p>考虑到llm的激活稀疏性，矩阵乘法操作可以使用稀疏算子绕过不活跃的神经元及其权重。PowerInfer引入了一种神经元感知算子，<strong>专注于矩阵中的单个行/列向量</strong>而不是整个矩阵。</p><p><strong>GPU 的神经元感知算子</strong>: 尽管GPU上<strong>向量-向量计算</strong> 比 <strong>矩阵-向量计算</strong>效率更低，但是在神经网络中通常有大量非活动神经元，基于<strong>向量-向量计算</strong>的算子可以跳过这些非活动神经元，避免不必要的计算和内存，并且不需要昂贵的矩阵转换。</p><p><strong>CPU 的神经元感知算子</strong>:CPU通常具有较低的并行性和矩阵计算效率，所以这种情况下<strong>向量-向量计算</strong>非常有利。CPU执行器为多核分配一个神经元感知算子，将神经元分成更小的批次进行并发激活检查。每个核心只处理其批次中的激活神经元，使用AVX2 等硬件向量扩展优化 <strong>向量-向量计算</strong>。</p><h2 id="神经元放置策略">5 神经元放置策略</h2><p>PowerInfer 的离线组件提供了一种放置策略，以指导每个神经元分配给 GPU或 CPU。</p><h3 id="离线分析">5.1 离线分析</h3><p>在确定每个神经元的位置之前，PowerInfer的离线分析器需要为每个神经元收集运行时推断数据(就是推理通用数据集然后统计频繁激活的神经元)。</p><ul><li>部署LLM来处理从多个通用数据集生成的请求。</li><li>分析器在 Transformer层内的每个块之后插入一个监控内核(检查层中的每个神经元是否在推理过程中被激活，如果是，增加神经元表中相应的计数)</li><li>在 GPU 上构建了一个神经元信息表，旨在跟踪每个神经元的激活计数。</li></ul><h3 id="神经元影响度量">5.2 神经元影响度量</h3><p>神经元影响度量测量每个神经元对LLM整体推理结果的贡献。我们通过利用分析激活频率准确反映运行时行为的事实来有效地计算该指标：</p><p><span class="math display">\[\begin{gather*}v_{i} = f_{i} \hspace{1cm}\forall i \in \mathbb{N} \tag{1}\end{gather*}\]</span></p><h3 id="神经元放置建模">5.3 神经元放置建模</h3><p>基于神经元影响度量，PowerInfer 利用求解器来优化 GPU中所有神经元的总影响。这个累积影响被表述为目标函数，如式 2所示。然后将该函数输入到整数线性规划框架中，以识别最大化函数的特定解。等式3 中定义的二元变量 <span class="math inline">\(a_{in}\)</span>表示神经元是否放置在处理单元 <span class="math inline">\(i\)</span>上。</p><p><span class="math display">\[\begin{gather*}Maximize \quad t_i= \sum_{e \in \mathbb{N}} a_{ie} * v_{e} \forall i \in\{GPU\} \tag{2} \\\sum_{i \in \mathbb{U}} a_{in} = 1 \quad\forall n \in \mathbb{N} \tag{3}\end{gather*}\]</span></p><h4 id="通信约束">5.3.1 通信约束</h4><p>在GPU上预加载的神经元数量受到层内通信开销的限制。如果预加载了太少的神经元，这种开销会否定GPU 提供的计算优势。因此，求解器必须识别最少数量的神经元来分配给 GPU进行处理。在不等式 4 中，<span class="math inline">\(C_{l}\)</span>是必须分配给第 <span class="math inline">\(l\)</span> 层的 GPU的神经元的最小计数。具体来说，GPU 上层 <spanclass="math inline">\(l\)</span> 的神经元数必须超过 <spanclass="math inline">\(C_{l}\)</span> 或等于 0。</p><p>在求解不等式 4 时，必须定义第 <span class="math inline">\(l\)</span>层单个神经元的计算时间和层内通信开销。在LLM推理中，特别是在较小的批处理大小下，该过程主要受内存带宽的限制。因此，神经元的计算时间大致等于一次访问所有权重所需的时间，如公式5所示。在较小的批处理大小下，层内数据传输的程度往往跨层一致，导致同步成本均匀。</p><p><span class="math display">\[\begin{gather*}C_{l} \cdot T_{l}^{GPU} + T_{sync} \leq C_{l} \cdot T_{l}^{CPU} \foralll \in \mathbb{L} \tag{4} \\T_{i}^{j} = M_{i} / Bandwidth_{j} \quad \forall j \in \mathbb{D},\forall i \in \mathbb{L} \tag{5} \\\end{gather*}\]</span></p><h4 id="内存约束">5.3.2 内存约束</h4><p>神经元放置进一步受到处理单元的内存容量的限制，如不等式 6 所示。</p><p>我们引入了一个辅助二进制变量<spanclass="math inline">\(y_l\)</span>，它可以是1或0。这个变量决定了任何神经元是否分配给第<spanclass="math inline">\(l\)</span>层的GPU。为方便起见，还引入了足够大的<span class="math inline">\(K\)</span>。</p><p>等式 7 和 8 被制定为对这种约束进行建模。当 <spanclass="math inline">\(y_l\)</span> 为 1 时，表示该层 GPU上的神经元放置，鉴于 <span class="math inline">\(K\)</span>足够大，这两个不等式有效地变为 <span class="math inline">\(y_l \leq\sum_{e \in N_l} a_{ie} \leq K\)</span>。相反，如果 <spanclass="math inline">\(y_l\)</span> 设置为 0，表示层 <spanclass="math inline">\(l\)</span> 的 GPU 上没有神经元放置，则不等式减少到<span class="math inline">\(\sum_{e \in N_l} a_{ie} = 0\)</span>。</p><p><span class="math display">\[\begin{gather*}\sum_{n \in N} a_{jn} \cdot M_{n} &lt; MCap_j \quad \forall j \in\mathbb{U} \tag{6} \\\sum_{e \in N_l} a_{ie} \geq C_l \cdot y_l \quad \forall l \in\mathbb{L}, \forall i \in \{GPU\} \tag{7} \\\sum_{e \in N_l} a_{ie} \leq K \cdot y_l \quad \forall l \in \mathbb{L},\forall i \in \{GPU\} \tag{8}\end{gather*}\]</span></p><h4 id="ilp优化">5.3.3 ILP优化</h4><p>随后，求解器利用整数线性规划 (ILP) 来优化目标函数，符合等式/不等式 3到 8 的所有约束。为了加快 IPL过程并实现近似解，主要策略是将每一层的神经元聚合成批次以进行集体放置分析。</p><h2 id="附录">附录</h2><h3 id="dejavu">DejaVu</h3><p>核心方法是在推理阶段，利用当前输入<spanclass="math inline">\(X\)</span>动态地选择部分网络参数来进行推理，而不使用全参。所以可以将其看作是一种“动态剪枝”的方法。</p><p>原文中指出: 在实际推理过程中，我们可以只使用约20%的Attentionhead和约5%的MLP神经元，就能达到和全参模型差不多的效果。</p><figure><img src="./images/ContextualSparsity.svg#80x80" title="图4: DejaVu"alt="DejaVu" /><figcaption aria-hidden="true">DejaVu</figcaption></figure><p>如何基于输入<span class="math inline">\(X\)</span>从预训练好的LLMA中快速产生LLM B，使得LLM B在<spanclass="math inline">\(X\)</span>上的推理结果与LLM A在<spanclass="math inline">\(X\)</span>上的推理结果尽可能一致？</p><p>在DejaVu中使用了两个模型来做预测，这两个模型的实现都使用了一个两层MLP。第一个模型用来预测MHA中哪些head是“高效的”；第二个模型用来预测MLP中哪些神经元是“高效的”（神经元实际上对应于参数矩阵的某一列或某一行）。</p><p>以预测Attention的head编号为例，假设head数为256，只需要将MLP的输出层的大小设为256，并为每一个输出使用sigmoid来做一个二分类即可（“选择”or“不选择”）。训练数据依靠一个完整的、训练好的LLM来产生。在这个LLM的推理过程中，通过记录它的Attention输入和Attention输出，并计算不同head的L2Norm，再基于一个L2 Norm的阈值t将head分为正例和负例。</p><p>假设当前的Transformer模块为网络的第 <spanclass="math inline">\(l\)</span>层(不同网络层使用的稀疏性预测模型是不同的)，它对应的MHA和MLP稀疏性预测模型记为<span class="math inline">\(\mathsf{SP}_A^{l}\)</span> 和 <spanclass="math inline">\(\mathsf{SP}_M^{l}\)</span> ，输入为<spanclass="math inline">\(y_l\)</span>。 大概过程如下：</p><p><span class="math display">\[\begin{align*}    S_A^l  \leftarrow \mathsf{SP}_A^{l}(y_l), \quad    \widetilde{y}_l \leftarrow \mathsf{MHA}^{l}_{S_A^l}(y_l), \\    S_M^l \leftarrow \mathsf{SP}_M^{l}(\widetilde{y}_l ), \quad    \widehat{y}_l \leftarrow \mathsf{MLP}^{l}_{S_M^l}( \widetilde{y}_l)\end{align*}\tag{-1}\]</span></p><p>其中，<span class="math inline">\(S_A^l\)</span>和<spanclass="math inline">\(S_M^l\)</span>分别是Attention和MLP的上下文稀疏度。</p><ol type="1"><li>KV cache 缺失的处理:对于在计算过程中，会为不被选择的注意力头保存一份过往token的副本，在下一次计算时，如果这个头被选择了，并且缺少KVcache，此时就会加载存储的token嵌入并一起计算K/V</li><li>即使减少了部分FFN和注意力头，稀疏预测开销可能很容易增加而不是减少端到端延迟:因为注意力和MLP块的计算必须等待稀疏预测器的决定。就延迟而言，这种开销可能超过了注意力和MLP块的节省。鉴于此，引入了一种前瞻性稀疏预测方法。研究发现了：嵌入特征缓慢演变，即可以用<spanclass="math inline">\(y_l\)</span>来预测<spanclass="math inline">\(l+1\)</span>层的稀疏性。从而实现并行化。 <spanclass="math display">\[   \begin{align*}   \widetilde{y}_l \leftarrow \mathsf{MHA}^{l}_{S_A^l}(y_l), \quad   \widehat{y}_l \leftarrow \mathsf{MLP}^{l}_{S_M^l}( \widetilde{y}_l ),\\   S_{A}^{l+1} \leftarrow \mathsf{SP}_A^{l}(y_l), \quad   S_M^{l+1}  \leftarrow \mathsf{SP}_M^{l}(y_l),   \end{align*}   \tag{-2}\]</span></li><li>其他trick:将稀疏矩阵的索引和乘法操作合并为一个单一的内核，以及采用了其他一些内存对齐技术以保证高效执行。</li></ol><p>原文链接：<a href="https://arxiv.org/abs/2310.17157">Deja Vu:Contextual Sparsity for Efficient LLMs at Inference Time</a> <ahref="https://zhuanlan.zhihu.com/p/673848224">参考</a></p><h3 id="其他">其他</h3><ol type="1"><li>大于总内存的模型怎么放到机器上<ol type="1"><li>对于PowerInfer1，模型首先在CPU上加载，然后将热激活神经元根据模型保存的激活文件信息卸载到GPU上。</li><li>至于PowerInfer-2，则是创建一个LRU缓存，将频繁使用的(热激活)神经元放到DRAM中，超出LRU缓存大小的(冷)神经元放在RAM，然后计算的时候如果命中LRU缓存，直接计算，如果没有，则跳过该神经元计算下一个神经元，并同时开始IO操作。这样可以先计算在DRAM中的神经元，然后再计算在RAM中的神经元，这种并行可以减少IO操作的影响。</li></ol></li><li>DejaVu 增加了计算怎么比原来计算快<ol type="1"><li>DejaVu 通过动态剪枝的方式，减少了计算量</li><li>稀疏预测器进行并行预测，在第t步时预测第t+1步的稀疏性，减少等待时间</li><li>Kernel fusion: 将稀疏矩阵的索引和乘法操作合并为一个单一的内核</li><li>内存对齐技术: 减少IO操作的影响</li></ol></li><li>为什么在下投影层实现AXPY算子来计算稀疏矩阵，而上投影层则是正常的vec_dot形式的矩阵乘法<ol type="1"><li>这是考虑到了神经元的激活在下投影层是按列的，导致了下投影层加载时的不连续性，decoding阶段LLM推理是memorybandwidthbound的，实现思路就是尽可能保证连续的load，并且跳过不必要的memoryaccess。尽管GPU上sharedmemory是可以从HBM先不连续的load进来再进行计算，但是CPU上的cache没办法显式控制，AXPY可以保证CPU/GPU都能做到连续的memoryaccess，因此设计了AXPY。</li><li>如图1所示，在下投影计算的时候，每一行的元素是稀疏激活的，激活的列数据是连续的，因此在计算时先判断是否激活，如果激活则将对应的列加载到cache中，然后进行计算，如果不激活则跳过该列。将以行为基准的稀疏矩阵转换为以列为基准的稀疏矩阵。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之Turbo Sparse</title>
      <link href="/2024/08/08/NLP/LLMModels/PowerInfer/TurboSparse/"/>
      <url>/2024/08/08/NLP/LLMModels/PowerInfer/TurboSparse/</url>
      
        <content type="html"><![CDATA[<h1 id="turbo-sparse">Turbo Sparse</h1><p>通过最少的激活参数实现 LLM 的最先进性能 原文链接：<ahref="https://arxiv.org/abs/2406.05955">Turbo Sparse: Achieving LLM SOTAPerformance with Minimal Activated Parameters</a></p><h2 id="简介">1. 简介</h2><p>提出了一种新的基于drelu的稀疏化方法，在保持性能的同时，将模型稀疏性提高到90%，在推理中实现了2-5倍的加速。</p><h3 id="性能提升">1.1 性能提升</h3><p>通过将我们的神经元稀疏化方法应用于Mistral和Mixtral模型，每次推理迭代分别只有25亿个和43亿个参数被激活，同时实现了更强大的模型性能。评估结果表明，这种稀疏性实现了2-5倍的解码加速。在手机上，我们的<ahref="https://huggingface.co/PowerInfer">TurboSparse-Mixtral-47B</a>实现了11tokens/s的推理速度。</p><h2 id="概述">2 概述</h2><h3 id="背景">2.1 背景</h3><p>为了解决现有密集模型固有的效率问题，条件计算已经成为一种关键方法，它指的是激活网络中的部分神经元。</p><ol type="1"><li>混合专家(MoE):通过在训练前手动设置模型架构上的约束来引入条件计算，例如确定要激活的专家数量。这种技术通过一个称为专家路由的过程，选择性地激活模型的特定部分，以响应特定的输入，从而显著提高效率；</li><li>利用ReLU激活函数自然产生的稀疏激活，它自然地输出零元素；</li><li>门控 MLP块。在这种块中，激活函数的输出被门控，以便在每次迭代中只激活一部分神经元。</li></ol><p>ReLUization 是一种现有的最先进的方法，用 ReLU替换原始激活函数并继续预训练。尽管这种方法具有潜力，但往往难以达到所需的激活稀疏度水平，并可能导致性能下降。</p><p>我们认为现有 ReLUification 方法的失败可归因于两个主要原因。</p><ul><li>首先，简单地用 ReGLU 替换 SwiGLU 是低效的，因为它只将稀疏性从 40%增加到大约70%。这表明有必要对模型架构进行更深入的研究，以实现更高水平的稀疏性。</li><li>其次，预训练数据的多样性有限，当前方法中训练tokens数量不足导致能力恢复不完整。因此，扩大预训练数据集的多样性并增加训练tokens的数量是提高模型性能的关键步骤。</li></ul><h3 id="创新">2.2 创新</h3><p>为了应对这些挑战，我们首先对现有的 ReLUfication方法进行了综合分析，并确定其缺点源于 GLU 组件中的负激活。</p><p>因此，我们提出了一个名为 dReLU 的有效激活函数。</p><ol type="1"><li>高效的dReLU激活函数:使用不到150B个tokens，不到典型预训练tokens(通常为15T tokens)的1%。</li><li>稀疏激活模型:两种稀疏激活TurboSparse-Mistral7B和TurboSparse-Mixtral-47B模型都比原始版本表现出更好的性能。</li><li>实际推理加速:可以实现2-5倍的加速。值得注意的是，即使在TurboSparse-Mixtral-47B上没有GPU，我们也可以实现高达10tokens/s的速度。</li></ol><h2 id="drelu">3 dReLU</h2><h3 id="核心">3.1 核心</h3><ul><li><strong>第一种改进</strong> 将 ReLU 化过程中的原始基于 SwiGLU的前馈网络（FFN）替换为基于 dReLU 的前馈网络（FFN）。</li><li><strong>第二种改进</strong>是在第一种改进的基础上，通过稀疏机制进一步优化模型性能，通过控制稀疏水平来调整模型的激活值。</li></ul><figure><img src="./images/architecture.webp#80x80"title="图1: dReLU Sparsification" alt="dReLU Sparsification" /><figcaption aria-hidden="true">dReLU Sparsification</figcaption></figure><h3 id="drelu-激活函数">3.2 dReLU 激活函数</h3><p>常用的 <span class="math inline">\(Gated-MLP\)</span>块由三个全连接层组成，并执行以下计算：</p><p><span class="math display">\[\begin{aligned}\text{Gate}(x) &amp;\coloneqq F_{act} (x W_{gate}) \\\text{Up}(x) &amp;\coloneqq x W_{up} \\\text{Combined}(x) &amp;\coloneqq \text{Gate}(x) * \text{Up}(x) \\\text{Gated-MLP}(x) &amp;\coloneqq \text{Combined}(x) W_{down}\end{aligned}\]</span></p><p>其中 <span class="math inline">\(F_{act}\)</span>代表不同的激活函数，<span class="math inline">\(W_{gate}\)</span>、<spanclass="math inline">\(W_{up}\)</span> 和 <spanclass="math inline">\(W_{down}\)</span> 是权重矩阵。</p><h4 id="使用-drelu-激活函数">3.2.1 使用 dReLU 激活函数</h4><p>引入了一种新的激活函数 <spanclass="math inline">\(dReLU\)</span>：</p><p><span class="math display">\[\begin{aligned}\text{Combined}_{\text{dReLU}}(x) &amp;\coloneqq \max(0, x W_{gate}) *\max(0, x W_{up})\end{aligned}\]</span></p><p>因此结合原本的 <span class="math inline">\(Gated-MLP\)</span>块，我们可以得到 <span class="math inline">\(Gated-MLP\)</span> 块的<span class="math inline">\(dReLU\)</span> 版本：</p><ol type="1"><li>使用 ((0, x W_{gate})) 计算门控激活（Gate）。</li><li>使用 ((0, x W_{up})) 计算向上投影（Up）。</li><li>将上述两步结果相乘以获得 Combined。</li><li>最后，将 Combined 通过下投影矩阵 (W_{down}) 生成最终输出。</li></ol><p><span class="math display">\[\begin{aligned}\text{Gate}(x) &amp;\coloneqq \max(0, x W_{gate}) \\\text{Up}(x) &amp;\coloneqq \max(0, x W_{up}) \\\text{Combined}_{\text{dReLU}}(x) &amp;\coloneqq \text{Gate}(x) *\text{Up}(x) \\\text{Gated-MLP}(x) &amp;\coloneqq \text{Combined}_{\text{dReLU}}(x)W_{down}\end{aligned}\]</span></p><h4 id="引入稀疏性机制">3.2.2 引入稀疏性机制</h4><p>在上述 dReLU 激活函数的基础上，引入稀疏性机制，通过选择前 k%的激活值，来控制模型的稀疏水平(在公式 3 的基础上加入以下):</p><ol type="1"><li>计算 Combined 并生成其绝对值的前 k% 的掩码（Mask）。</li><li>将 Combined 乘以 Mask，以保留仅前 k% 的激活值。</li><li>将处理后的 Combined 通过下投影矩阵 (W_{down}) 生成最终输出。</li></ol><p><span class="math display">\[\begin{aligned}\text{Mask}(x) &amp;\coloneqq \text{Top}_k(|\text{Combined}(x)|)\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}\text{Gated-MLP}(x) &amp;\coloneqq (\text{Combined}(x) * \text{Mask}(x))W_{down}\end{aligned}\]</span></p><h3 id="pytorch-实现大概">3.3 PyTorch 实现(大概)</h3><h4 id="drelu-激活函数-1">3.3.1 dReLU 激活函数</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">dReLU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">GatedMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>GatedMLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_gate <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_up <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_down <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>drelu <span class="token operator">=</span> dReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        gate <span class="token operator">=</span> self<span class="token punctuation">.</span>drelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_gate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        up <span class="token operator">=</span> self<span class="token punctuation">.</span>drelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_up<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        combined <span class="token operator">=</span> gate <span class="token operator">*</span> up        out <span class="token operator">=</span> self<span class="token punctuation">.</span>W_down<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>        <span class="token keyword">return</span> out</code></pre><h4 id="引入稀疏性机制-1">3.3.2 引入稀疏性机制</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">dReLU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">SparseGatedMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SparseGatedMLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_gate <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_up <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_down <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>drelu <span class="token operator">=</span> dReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>k <span class="token operator">=</span> k  <span class="token comment"># 稀疏水平</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        gate <span class="token operator">=</span> self<span class="token punctuation">.</span>drelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_gate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        up <span class="token operator">=</span> self<span class="token punctuation">.</span>drelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_up<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        combined <span class="token operator">=</span> gate <span class="token operator">*</span> up                <span class="token comment"># 在 Combined 中保留前 k% 的激活值</span>        abs_combined <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>combined<span class="token punctuation">)</span>        top_k_values<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>abs_combined<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>k <span class="token operator">*</span> combined<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        min_top_k <span class="token operator">=</span> top_k_values<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>abs_combined<span class="token punctuation">)</span>        mask <span class="token operator">=</span> abs_combined <span class="token operator">>=</span> min_top_k        masked_combined <span class="token operator">=</span> combined <span class="token operator">*</span> mask<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                out <span class="token operator">=</span> self<span class="token punctuation">.</span>W_down<span class="token punctuation">(</span>masked_combined<span class="token punctuation">)</span>        <span class="token keyword">return</span> out</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM模型之PowerInfer2.0</title>
      <link href="/2024/08/08/NLP/LLMModels/PowerInfer/PowerInfer2/"/>
      <url>/2024/08/08/NLP/LLMModels/PowerInfer/PowerInfer2/</url>
      
        <content type="html"><![CDATA[<h1 id="powerinfer-2">PowerInfer-2</h1><p>在智能手机上的快速大语言模型推理 原文链接：<ahref="https://arxiv.org/abs/2406.06282">PowerInfer-2: Fast LargeLanguage Model Inference on a Smartphone</a></p><h2 id="简介">1 简介</h2><p>PowerInfer-2是一个为智能手机上大型语言模型(LLM)高速推理而设计的框架，<strong>特别适用于大小超过设备内存容量的模型</strong>。</p><h3 id="性能提升">1.1 性能提升</h3><p>PowerInfer-2 是第一个为 TurboSparse-Mixral-47B模型提供服务的系统，智能手机每秒生成 11.68个tokens。对于完全在内存中拟合的模型，PowerInfer-2 在保持与 llama.cpp 和MLC-LLM 相当的推理速度的同时，可以减少大约 40% 的内存使用。</p><ol type="1"><li>低推理延迟：最大限度地减少预填充阶段（TTFT）和解码阶段（TBT）的推理延迟；</li><li>低内存占用：减少推理过程中的内存使用，即使模型大小超过设备内存限制，也能实现LLM的低延迟推理；</li><li>灵活性：确保设计能够无缝适应具有不同计算、内存和存储容量的智能手机。</li></ol><h2 id="概述">2 概述</h2><h3 id="核心">2.1 核心</h3><p><strong>PowerInfer-2 的核心是将 LLM推理中典型的粗粒度矩阵计算分解为细粒度神经元集群计算。</strong></p><h3 id="神经元簇和架构">2.2 神经元簇和架构</h3><p>PowerInfer-2以神经元簇的粒度进行计算和I/O操作，<strong>神经元簇可以在计算过程中动态地由多个激活的神经元组成</strong>，神经元的数量由计算单元的计算能力决定，从而可以充分利用具有不同计算能力的XPU。</p><p>图 1展示了PowerInfer-2的整体架构，它分为在线（右部分）和离线（左部分）程序。</p><ul><li>在线部分提供神经元簇粒度的推理，包括四个协作组件：多态神经元引擎、内存中神经元缓存、灵活的神经元加载和神经元簇级别I /O 管道。</li><li>离线部分描述在线推理中涉及的每个组件的具体配置并指导在线过程。</li></ul><figure><img src="./images/architecture.webp#80x80"title="图1: PowerInfer-2整体架构" alt="PowerInfer-2整体架构" /><figcaption aria-hidden="true">PowerInfer-2整体架构</figcaption></figure><h2 id="在线部分">3 在线部分</h2><h3 id="多态神经元引擎">3.1 多态神经元引擎</h3><p>PowerInfer-2引入了一个多态神经元引擎，该引擎动态地将神经元组合成神经元簇，以利用LLM推理阶段和异质XPU的不同计算特征。用于预填充和解码阶段的两个计算工作流程如图2 所示。</p><figure><img src="./images/computing-workflow.webp#50x50"title="图2: PowerInfer-2的多态神经元引擎"alt="PowerInfer-2的多态神经元引擎" /><figcaption aria-hidden="true">PowerInfer-2的多态神经元引擎</figcaption></figure><ul><li><ol type="a"><li>预填充阶段采用以 NPU 为中心的工作流程，利用 NPU 进行计算，利用 CPU进行准备；</li></ol></li><li><ol start="2" type="a"><li>解码阶段以 CPU 为中心，仅使用 CPU 核心来利用稀疏激活。</li></ol></li></ul><h4 id="以-npu-为中心的预填充">3.1.1 以 NPU 为中心的预填充</h4><p>对于<strong>预填充阶段，神经元簇包含权重矩阵中的所有神经元</strong>，所有tokens都会同时处理。尽管这些tokens中的每一个都显示出高稀疏性并激活不同的神经元，但由于这些激活的聚合，总体稀疏性显着降低。因此，PowerInfer-2在预填充阶段不会使用预测器来计算激活的神经元，而是选择直接将所有神经元合并成一个大的神经元簇。</p><p>CPU核心不参与矩阵计算，在预填充阶段只为NPU执行必要的准备任务。</p><ul><li>首先，由于内存有限，PowerInfer-2 在预填充阶段依赖 CPU核心将闪存中存储的权重加载到内存中。</li><li>其次，由于当前的 NPU 不支持直接使用量化权重进行计算，PowerInfer-2 在NPU 计算之前使用 CPU 内核对数据进行反量化。</li></ul><p>图 2-a 演示了 CPU 和 NPU 如何协作在 Transformer层粒度中执行预填充阶段推理。 NPU 计算需要使用与 CPU共享的有限内存。因此，在 NPU 计算开始之前，CPU应将所需的矩阵权重预加载到该共享内存中。</p><ol type="1"><li>在NPU进行任何矩阵乘法之前，多个CPU中核从神经元缓存中读取量化矩阵权重，并提前将这些矩阵权重反量化为fp16，最终将结果存储在CPU和CPU之间的共享内存中。</li><li>同时，PowerInfer-2使用另一个大核将下一层的所有矩阵权重异步预加载到神经元缓存中。</li><li>中核的反量化、NPU的计算和大核的I/O操作同时进行，以减少I/O开销。</li></ol><p>值得注意的是，由于预填充阶段涉及密集矩阵而不是稀疏计算,通过 I/O进行加权加载可以利用顺序读取将大块数据加载到内存中，从而最大限度地利用UFS 的 I/O 带宽。</p><h4 id="以cpu为中心的解码">3.1.2 以CPU为中心的解码</h4><p>与预填充阶段不同，解码阶段在每次迭代期间集中于单个token，表现出显着的稀疏性，因为权重矩阵中只有一小部分神经元（大约10%）被激活并参与计算。因此，对于解码阶段，<strong>调用预测器来识别在开始计算之前将激活哪些神经元，然后引擎将这些激活的神经元合并成一个小的神经元簇，并利用CPU 核心动态计算神经元簇</strong>:</p><ul><li>当批量大小为 1 时(即就一个 token)，CPU 内核上矩阵向量计算的延迟低于NPU 上的延迟。</li><li>由于稀疏性导致激活的神经元数量减少，CPU 内核最适合 XPU中的这些较轻且稀疏的计算。</li></ul><p>具体来说，PowerInfer-2 在解码阶段利用 CPU 内核来计算注意力块和 FFN块。</p><ul><li>尽管注意力块没有表现出稀疏性，但当输入只是单个向量时，CPU内核仍然提供较低的计算延迟。</li><li>对于 FFN 块，PowerInfer-2 首先将 FFN块的输入向量传递给预测器，<strong>预测器预测 FFN权重矩阵中的哪些神经元需要激活，并将它们合并到神经元簇中</strong>。然后，每个CPU 核心获取一个集群并计算集群内的这些神经元和输入向量。</li></ul><p>图 2-b 说明了不同CPU核进行的解码阶段推断，总结如下：</p><ol type="1"><li>CPU核心首先从神经元缓存中读取注意力块的权重，并使用输入向量进行计算。</li><li>运行预测器来确定后续权重矩阵中神经元的激活状态。</li><li>CPU核心将激活的神经元分成几个簇，每个核心负责用输入向量计算其簇内激活的神经元，并最终聚合结果。</li><li>如果这些神经元位于神经元缓存内，CPU内核将使用输入向量计算它们。在缓存未命中的情况下，即神经元不在神经元缓存中，CPU核心上运行的 I/O线程将神经元异步加载到缓存中，最终通知计算线程完成计算。</li></ol><h3 id="内存中神经元缓存">3.2 内存中神经元缓存</h3><p>PowerInfer-2 引入了针对 LLM内各种数据类型量身定制的分段神经元缓存设计。它将缓存划分为多个区域，每个区域都有特定的预取和逐出策略。</p><p>注意力块权重较小且激活较少，在整个运行时预加载并保留。</p><p>相比之下，FFN块容易频繁激活热神经元，对这些神经元使用基于最近最少使用 (LRU)的动态驱逐策略。这种方法确保热神经元更有可能保留在缓存中，而冷神经元经常被逐出并按需从闪存加载。重要的是，逐出过程不涉及写入存储，而只是丢弃内存中的权重。</p><p>PowerInfer-2 利用经典的双队列方法来实现其 LRU神经元缓存，该缓存以单个神经元的粒度管理 LLM权重。该系统维护两个双向链表队列，标记为活动和非活动，其中队列中神经元的顺序由它们最近访问的时间决定，最近访问的神经元位于队列的头部。</p><p>**LRU 缓存管理器的工作流程如下：在运行时，所有神经元最初都会加入非活动队列。重新访问时，它们被提升到活动队列的前面。已经在活动队列中的神经元在后续访问时被移动到头部。为了管理缓存容量，当活动队列占满缓存空间的90% 时，活动队列尾部的神经元将被移至非活动队列，直到活动队列的占用率降至90%以下。如果缓存达到容量，非活动队列尾部的神经元将被丢弃，为新条目腾出空间。*</p><h3 id="灵活的神经元加载">3.3 灵活的神经元加载</h3><p>PowerInfer-2 通过自适应捆绑和加载神经元来最大限度地减少 I/O开销，这是由模型的量化决定的。</p><h4 id="自适应捆绑">3.3.1 自适应捆绑</h4><p>在推理过程仍然不可避免地导致未缓存神经元的 I/O 操作，为了优化 I/O读取吞吐量并最小化 I/O 操作，PowerInfer-2还捆绑相关神经元。尽管在单个FFN权重矩阵内的共激活在移除热神经元后变得不常见，但不同矩阵中相应位置的神经元通常会一起激活。因此PowerInfer-2<strong>选择根据神经元粒度而不是矩阵结构来存储神经元权重</strong>。</p><p>**例如，Gate、Up、Down 矩阵中第 i 个神经元的共激活概率高达80%，所以将 Gate、Up 和 Down 矩阵中第 i个神经元的权重连接到单个条目中。*</p><h4 id="灵活的神经元加载-1">3.3.2 灵活的神经元加载</h4><p>考虑到不同模型的量化方法和 UFS I/O 的固有特性，PowerInfer-2进一步引入了不同模型的不同 I/O 加载策略。</p><p>对于没有量化的模型，由于每个神经元占用的存储空间较大，PowerInfer-2使用更大粒度的随机读取来提高I/O带宽。**例如，Llama-7B-FP16 中的单个神经元占用 8KB，来自 Gate、Up 和 Down矩阵的神经元的总大小为 24KB。 PowerInfer-2 通过一次随机 I/O读取，有效地将整个 24KB 激活包传输到内存中。*</p><p>对于 4 位量化模型，捆绑包大小设置为 8KB。以 Llama-7B模型为例，每个神经元被量化为 4 位精度并占用 2.5KB（量化的 int4 值为2KB，量化组的 FP16 尺度为 0.5KB），组合束大小达到 7.5KB。为了与存储介质4KB 的最小读取粒度保持一致，PowerInfer-2 在捆绑包中额外补充了0.5KB，将总数四舍五入为 8KB。然而，PowerInfer-2 并没有在单个 I/O操作中加载这些 8KB 包，而是选择了 4KB 粒度。因为我们发现两次单独的 4KB随机读取的带宽超过了单个 8KB 读取的带宽，从而优化了 I/O 读取过程。</p><p>此外，考虑到这些神经束内共激活的可能性为 80%，这些成束神经元仍有近20% 的概率不被共激活。因此，组合两个 4KB随机读取可能会导致带宽浪费。为了缓解这个问题，对于使用 4位量化的模型，PowerInfer-2 会延迟第二个 4KB读取，直到获得门神经元乘法的结果。(PowerInfer-2使用预测器来确定门矩阵内神经元的激活，并根据此信息启动束第一部分的负载。之后，如果门神经元的输出（通过激活函数）非零，PowerInfer-2将继续加载捆绑包的第二部分，从而最大限度地减少不必要的 I/O 操作。)</p><h3 id="神经元簇级管道">3.4 神经元簇级管道</h3><p>为了减少 I/O 延迟，PowerInfer-2引入了一种新颖的管道机制，可以同时处理神经元集群和 I/O 操作(将计算与 I/O活动重叠来隐藏 I/O 开销)。图 3是两种类型的管道，结合了矩阵向量乘法和五个核心（4 个计算核心和 1 个 I/O核心）上的 I/O 操作。</p><figure><img src="./images/neurou-pipeline.webp#60x60"title="图3: 神经元簇级管道" alt="神经元簇级管道" /><figcaption aria-hidden="true">神经元簇级管道</figcaption></figure><ul><li><ol type="a"><li>矩阵级管道将管道分成孤立的矩阵单元；</li></ol></li><li><ol start="2" type="a"><li>PowerInfer-2 中的神经元簇级管道打破了矩阵障碍，并将其计算和 I/O操作混合在神经元簇粒度中。</li></ol></li></ul><h4 id="矩阵级管道">3.4.1 矩阵级管道</h4><p>一种简单的方法是矩阵级重叠，它发出 I/O命令从存储中检索矩阵神经元，同时处理内存中已有的神经元。当存储中的神经元被加载时，它们会立即被处理。虽然这种矩阵级重叠方法可以在一定程度上隐藏计算过程中I/O操作的成本，但它仍然要求系统等待矩阵内所有神经元的完成，包括从存储中获取的神经元，然后再继续下一个。</p><p>如图 3-a所示，假设一个矩阵包含8个神经元簇，其中4个位于内存中，其余4个位于存储中。一部分I/O 操作可以隐藏在缓存的神经元计算后面。但由于 I/O 时间较长，仍然存在CPU 内核必须等待 I/O 完成的情况。</p><h4 id="神经元簇级管道-1">3.4.2 神经元簇级管道</h4><p>为了消除 I/O 操作的等待时间，PowerInfer-2引入了神经元簇级管道机制，如图3-b所示。通过将神经元簇作为粒度，可以在来自多个矩阵的神经元簇计算中重叠I/O 操作。</p><p>具体来说，PowerInfer-2打破了矩阵计算之间的障碍；一旦一个神经元簇完成计算，它立即开始计算内存中下一个矩阵中的神经元簇。该机制有效减少了等待气泡。</p><p>PowerInfer-2将神经元簇的执行过程分为5个连续的阶段，创建了多个计算线程和一个I/O线程来分别处理这5个阶段的计算和I/O操作：</p><ol type="1"><li>通过预测器(Pred)确定Gate、Up和Down矩阵的行/列是否被激活;</li><li>从存储器(GIO)中读取Gate矩阵的行权值;</li><li>计算Gate矩阵的行与输入向量(GC)的乘积;</li><li>从存储(UDIO)中读取Up和Down矩阵的行/列;</li><li>分别计算Up和Down矩阵的行/列与输入向量的乘积(UDC)。</li></ol><p>图 4 展示了计算和 I/O 线程如何工作来实现神经元簇管道。</p><ol type="1"><li>在每个FFN块开始时，所有神经元最初都处于Pred阶段，被插入到计算队列中。</li><li>计算线程处理这些神经元，仅将那些激活的神经元推进到后续阶段。</li><li>然后这些激活的神经元合并成神经元簇。</li><li>如果神经元簇的 Gate权重在内存中可用，则神经元簇进入GC阶段并返回到计算队列。如果不是，则将其设置为GIO 并移至 I/O 队列。</li><li>同时，计算线程继续处理队列中的下一个神经元簇。</li><li>同时，I/O 线程从 I/O 队列中取出神经元，根据需要执行 I/O 任务。</li><li>UDIO 和UDC 的执行遵循与GC 和GIO 类似的模式。</li></ol><figure><img src="./images/neurou-pipeline-workflow.webp#70x70"title="图4: 神经元簇级管道工作流" alt="神经元簇级管道工作流" /><figcaption aria-hidden="true">神经元簇级管道工作流</figcaption></figure><h2 id="离线部分">4 离线部分</h2><p>为了自动适应不同的模型或智能手机，在在线推理开始之前，对于最初在新智能手机上服务的每个模型执行一次离线过程。此过程涉及接收三种类型的输入：模型权重、用户输入和硬件规格。它输出一个执行计划，描述在线推理中涉及的每个组件的配置并指导在线过程。</p><h3 id="生成执行计划">4.1 生成执行计划</h3><p>PowerInfer-2的离线规划器通过分析模型权重、用户输入和硬件规格来生成执行计划，输出计算、内存和I/O 的配置。</p><ol type="1"><li>输入配置：<ul><li>硬件：硬件配置的参数，如CPU FLOPS、I/O吞吐量和内存带宽。</li><li>用户：用户自定义的参数，如CPU约束、内存限制、译码速度下界等。</li><li>Model：离线分析器收集的关于模型的参数，例如模型的大小、稀疏度级别和缓存特征等。</li></ul></li><li>输出配置：<ul><li>计算：根据CPU和NPU的计算强度确定不同阶段或层中CPU和NPU的使用比例。</li><li>内存：为了实现内存使用和推理性能之间的平衡，规划器允许用户在运行PowerInfer-2 之前设置所需的推理速度。根据设置的速度，PowerInfer-2计算所需的最佳缓存大小。</li><li>I/O：规划器触发分析器来测量模型的稀疏性以及冷热神经元的分布。</li></ul></li></ol><h2 id="附录">附录</h2><h3 id="llm-in-a-flash">LLM in a flash</h3><p>将模型参数存储在闪存中，但将其按需带入DRAM，解决超过DRAM可用容量的llm的挑战。</p><ol type="1"><li>减少数据加载量<ol type="1"><li>有选择性地把部分参数常驻在DRAM中。而这部分常驻的参数就是Attention参数和Embedding参数，原因是因为它们占比较小。</li><li>借鉴了DejaVu。每次在推理时动态加载MLP预测为激活神经元对应的参数。</li><li>滑动窗口。保留处理过去k个token时的激活神经元所对应的参数在DRAM中，并在处理当前token时只对：1）部分多余的参数进行删除；2）缺少的参数进行加载。</li></ol></li><li>提高传输数据量(读取大块数据) 未来改进:把向上投影的第i列和向下投影的第i行捆绑存储。当激活第i个中间神经元时，这两部分数据会同时被使用。通过在闪存中将这些对应的列和行一起存储，可以将数据整合成更大的块进行读取。</li></ol><p>原文链接: <a href="https://arxiv.org/abs/2312.11514">LLM in a flash:Efficient Large Language Model Inference with Limited Memory</a> <ahref="https://zhuanlan.zhihu.com/p/673775476">参考</a> <ahref="https://zhuanlan.zhihu.com/p/673572594">翻译</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>A100 vs V100</title>
      <link href="/2024/07/05/TechnicalNotes/Devices/"/>
      <url>/2024/07/05/TechnicalNotes/Devices/</url>
      
        <content type="html"><![CDATA[<h2 id="a100-vs-v100">A100 vs V100</h2><table><colgroup><col style="width: 26%" /><col style="width: 36%" /><col style="width: 36%" /></colgroup><thead><tr><th style="text-align: center;">特性</th><th style="text-align: center;">A100</th><th style="text-align: center;">V100</th></tr></thead><tbody><tr><td style="text-align: center;">架构</td><td style="text-align: center;">Ampere (GA100)</td><td style="text-align: center;">Volta (GV100)</td></tr><tr><td style="text-align: center;">CUDA核心数量</td><td style="text-align: center;">6,912</td><td style="text-align: center;">5,120</td></tr><tr><td style="text-align: center;">SM数量</td><td style="text-align: center;">108</td><td style="text-align: center;">80</td></tr><tr><td style="text-align: center;">张量核心数量</td><td style="text-align: center;">640</td><td style="text-align: center;">640</td></tr><tr><td style="text-align: center;">张量核心精度支持</td><td style="text-align: center;">FP64, TF32, FP16, BF16, INT8, INT4</td><td style="text-align: center;">FP32, FP16</td></tr><tr><td style="text-align: center;">张量核心性能</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr><td style="text-align: center;">显存容量</td><td style="text-align: center;">40GB / 80GB HBM2e</td><td style="text-align: center;">16GB / 32GB HBM2</td></tr><tr><td style="text-align: center;">显存带宽</td><td style="text-align: center;">1555 GB/s</td><td style="text-align: center;">900 GB/s</td></tr><tr><td style="text-align: center;">NVLink带宽</td><td style="text-align: center;">600 GB/s</td><td style="text-align: center;">300 GB/s</td></tr><tr><td style="text-align: center;">PCIe支持</td><td style="text-align: center;">PCIe 4.0</td><td style="text-align: center;">PCIe 3.0</td></tr><tr><td style="text-align: center;">功耗 (TDP)</td><td style="text-align: center;">400W</td><td style="text-align: center;">300W</td></tr><tr><td style="text-align: center;">多实例GPU (MIG)</td><td style="text-align: center;">支持</td><td style="text-align: center;">不支持</td></tr><tr><td style="text-align: center;">稀疏性加速</td><td style="text-align: center;">支持(<span class="math inline">\(\approx2 \times V100\)</span>)</td><td style="text-align: center;">不支持</td></tr></tbody></table><h3 id="架构">架构</h3><p>A100引入了<strong>TensorFloat-32 (TF32) TensorCore</strong>以及<strong>结构化稀疏功能</strong>：</p><ul><li>运行速度比 V100 FP32 FMA 操作快 10 倍(稀疏性快 20 倍)</li><li>FP16/FP32 混合精度，A100 Tensor Core 的性能是 V100 的 2.5倍(稀疏性则提高到 5 倍)</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>IDE缓存清理</title>
      <link href="/2024/07/01/TechnicalNotes/IDECacheClear/"/>
      <url>/2024/07/01/TechnicalNotes/IDECacheClear/</url>
      
        <content type="html"><![CDATA[<h2 id="ide缓存清理">IDE缓存清理</h2><h3 id="visual-studio">Visual Studio</h3><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr><th>缓存类型</th><th>路径</th><th>清理方法</th></tr></thead><tbody><tr><td>NuGet包缓存</td><td><code>%UserProfile%\.nuget</code></td><td>执行<code>dotnet nuget locals all --clear</code>命令</td></tr></tbody></table><h3 id="visual-studio-code">Visual Studio Code</h3><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr><th>缓存类型</th><th>路径</th><th>清理方法</th></tr></thead><tbody><tr><td>扩展缓存</td><td><code>%UserProfile%\.vscode\extensions</code></td><td>删除<code>.vscode\extensions</code>文件夹</td></tr></tbody></table><h3 id="android-studio">Android Studio</h3><table><thead><tr><th>缓存类型</th><th>路径</th><th>清理方法</th></tr></thead><tbody><tr><td>Gradle缓存</td><td><code>%UserProfile%\.gradle</code></td><td>删除<code>.gradle</code>文件夹</td></tr><tr><td>Android SDK缓存</td><td><code>%UserProfile%\.android</code></td><td>删除<code>.android</code>文件夹</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 10 注意力机制</title>
      <link href="/2024/06/30/DL/10%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/06/30/DL/10%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="注意力机制">10 注意力机制</h2><h3 id="注意力提示">10.1 注意力提示</h3><p>自主性的与非自主性的注意力提示解释了人类的注意力的方式，查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。下面是通过这两种注意力提示用神经网络来设计注意力机制的框架:</p><figure><img src="./images/10.1_attention.svg"title="图10.1：注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向"alt="查询、键和值" /><figcaption aria-hidden="true">查询、键和值</figcaption></figure><h3 id="注意力汇聚nadaraya-watson-核回归">10.2注意力汇聚：Nadaraya-Watson 核回归</h3><h4 id="生成数据集">10.2.1 生成数据集</h4><p><span class="math display">\[y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon\tag{10.1}\]</span></p><pre class="language-python" data-language="python"><code class="language-python">n_train <span class="token operator">=</span> <span class="token number">50</span>  <span class="token comment"># 训练样本数</span>x_train<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>n_train<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>   <span class="token comment"># 排序后的训练样本</span><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">2</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> x<span class="token operator">**</span><span class="token number">0.8</span>y_train <span class="token operator">=</span> f<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n_train<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 训练样本的输出</span>x_test <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment"># 测试样本</span>y_truth <span class="token operator">=</span> f<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>  <span class="token comment"># 测试样本的真实输出</span>n_test <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>  <span class="token comment"># 测试样本数</span><span class="token comment"># 绘制训练数据 真实值 测试结果</span><span class="token keyword">def</span> <span class="token function">plot_kernel_reg</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">)</span><span class="token punctuation">:</span>    d2l<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> <span class="token punctuation">[</span>y_truth<span class="token punctuation">,</span> y_hat<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'x'</span><span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Truth'</span><span class="token punctuation">,</span> <span class="token string">'Pred'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> <span class="token string">'o'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="平均汇聚">10.2.2 平均汇聚</h4>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 8 循环神经网络</title>
      <link href="/2024/06/30/DL/8%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/06/30/DL/8%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="循环神经网络">8 循环神经网络</h2><h3 id="序列模型">8.1 序列模型</h3><p><strong>自回归模型AR</strong>:自回归模型是时间序列模型的一种，它假设当前时间点的值是之前若干时间点值的线性组合。简单来说，AR模型根据过去的观察值来预测未来的值。</p><p><span class="math display">\[\begin{aligned}X_t &amp;= c + \sum_{i=1}^{p} \phi_i X_{t-i} + \epsilon_t \\\text{其中} &amp; \\X_t &amp;\text{ 是时间序列在时间 } t \text{ 的值} \\c &amp;\text{ 是一个常数项} \\\phi_i &amp;\text{ 是自回归系数} \\p &amp;\text{ 是模型的阶数，即使用多少个过去的时间点来预测当前值} \\\epsilon_t &amp;\text{ 是误差项，通常假设为白噪声}\end{aligned}\tag{8.1}\]</span></p><p><strong>隐变量自回归模型LV-AR</strong>:隐变量自回归模型是在自回归模型的基础上引入了隐变量。隐变量是指那些不能直接观察到，但可以通过其他观察变量间接推断出来的变量。LVAR模型的基本思想是，观察到的时间序列是由一些隐藏的动态过程驱动的。包含了观测方程和状态方程两部分：</p><p><span class="math display">\[\begin{aligned}&amp; \textbf{观测方程} \\Y_t &amp;= \Lambda X_t + \epsilon_t \\\text{其中} &amp; \\Y_t &amp;\text{ 是在时间 } t \text{ 观测到的变量} \\\Lambda &amp;\text{ 是隐变量与观测变量之间的关系矩阵} \\X_t &amp;\text{ 是时间 } t \text{ 的隐变量} \\\epsilon_t &amp;\text{ 是观测噪声}\end{aligned}\tag{8.2}\]</span></p><p><span class="math display">\[\begin{aligned}&amp; \textbf{状态方程} \\X_t &amp;= A X_{t-1} + \eta_t \\\text{其中} &amp; \\A &amp;\text{ 是隐变量的自回归系数矩阵} \\\eta_t &amp;\text{ 是状态噪声}\end{aligned}\tag{8.3}\]</span></p><p><strong>马尔可夫模型</strong>：使用 <spanclass="math inline">\(x_{t-1}, \ldots, x_{t-\tau}\)</span> 而不是 <spanclass="math inline">\(x_{t-1}, \ldots, x_1\)</span> 来预测 <spanclass="math inline">\(x_t\)</span> ，只要这种是近似精确的，我们就说序列满足马尔可夫条件。当 <spanclass="math inline">\(\tau = 1\)</span>时，我们称序列是一阶马尔可夫序列。</p><h3 id="文本预处理">8.2 文本预处理</h3><ol type="1"><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> collections<span class="token keyword">import</span> re<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token comment"># 读取数据集</span>d2l<span class="token punctuation">.</span>DATA_HUB<span class="token punctuation">[</span><span class="token string">'time_machine'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>DATA_URL <span class="token operator">+</span> <span class="token string">'timemachine.txt'</span><span class="token punctuation">,</span>                                <span class="token string">'090b5e7e70c295757f55df93cb0a180b9691891a'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">read_time_machine</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""将时间机器数据集加载到文本行的列表中"""</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'time_machine'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        lines <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">'[^A-Za-z]+'</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>lines <span class="token operator">=</span> read_time_machine<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'# 文本总行数: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>lines<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>lines<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 词元化</span><span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>lines<span class="token punctuation">,</span> token<span class="token operator">=</span><span class="token string">'word'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""将文本行拆分为单词或字符词元"""</span>    <span class="token keyword">if</span> token <span class="token operator">==</span> <span class="token string">'word'</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    <span class="token keyword">elif</span> token <span class="token operator">==</span> <span class="token string">'char'</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'错误：未知词元类型：'</span> <span class="token operator">+</span> token<span class="token punctuation">)</span>tokens <span class="token operator">=</span> tokenize<span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 词汇表</span><span class="token comment"># 将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料</span><span class="token keyword">class</span> <span class="token class-name">Vocab</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""文本词表"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> min_freq<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> reserved_tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> reserved_tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            reserved_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment"># 按出现频率排序</span>        counter <span class="token operator">=</span> count_corpus<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_token_freqs <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>counter<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                   reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment"># 未知词元的索引为0</span>        self<span class="token punctuation">.</span>idx_to_token <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'&lt;unk>'</span><span class="token punctuation">]</span> <span class="token operator">+</span> reserved_tokens        self<span class="token punctuation">.</span>token_to_idx <span class="token operator">=</span> <span class="token punctuation">&#123;</span>token<span class="token punctuation">:</span> idx                             <span class="token keyword">for</span> idx<span class="token punctuation">,</span> token <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>        <span class="token keyword">for</span> token<span class="token punctuation">,</span> freq <span class="token keyword">in</span> self<span class="token punctuation">.</span>_token_freqs<span class="token punctuation">:</span>            <span class="token keyword">if</span> freq <span class="token operator">&lt;</span> min_freq<span class="token punctuation">:</span>                <span class="token keyword">break</span>            <span class="token keyword">if</span> token <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">:</span>                self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> self<span class="token punctuation">.</span>unk<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>__getitem__<span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">to_tokens</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> indices<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>indices<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token keyword">for</span> index <span class="token keyword">in</span> indices<span class="token punctuation">]</span>    <span class="token decorator annotation punctuation">@property</span>    <span class="token keyword">def</span> <span class="token function">unk</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 未知词元的索引为0</span>        <span class="token keyword">return</span> <span class="token number">0</span>    <span class="token decorator annotation punctuation">@property</span>    <span class="token keyword">def</span> <span class="token function">token_freqs</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_token_freqs<span class="token keyword">def</span> <span class="token function">count_corpus</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""统计词元的频率"""</span>    <span class="token comment"># 这里的tokens是1D列表或2D列表</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 将词元列表展平成一个列表</span>        tokens <span class="token operator">=</span> <span class="token punctuation">[</span>token <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span>    <span class="token keyword">return</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>vocab <span class="token operator">=</span> Vocab<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'文本:'</span><span class="token punctuation">,</span> tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'索引:'</span><span class="token punctuation">,</span> vocab<span class="token punctuation">[</span>tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 整合所有功能</span><span class="token keyword">def</span> <span class="token function">load_corpus_time_machine</span><span class="token punctuation">(</span>max_tokens<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""返回时光机器数据集的词元索引列表和词表"""</span>    lines <span class="token operator">=</span> read_time_machine<span class="token punctuation">(</span><span class="token punctuation">)</span>    tokens <span class="token operator">=</span> tokenize<span class="token punctuation">(</span>lines<span class="token punctuation">,</span> <span class="token string">'char'</span><span class="token punctuation">)</span>    vocab <span class="token operator">=</span> Vocab<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>    <span class="token comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span>    <span class="token comment"># 所以将所有文本行展平到一个列表中</span>    corpus <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span>    <span class="token keyword">if</span> max_tokens <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        corpus <span class="token operator">=</span> corpus<span class="token punctuation">[</span><span class="token punctuation">:</span>max_tokens<span class="token punctuation">]</span>    <span class="token keyword">return</span> corpus<span class="token punctuation">,</span> vocabcorpus<span class="token punctuation">,</span> vocab <span class="token operator">=</span> load_corpus_time_machine<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span></code></pre><h3 id="语言模型和数据集">8.3 语言模型和数据集</h3>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 7 现代卷积神经网络</title>
      <link href="/2024/06/29/DL/7%20%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/06/29/DL/7%20%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="现代卷积神经网络">7 现代卷积神经网络</h2><h3 id="深度卷积神经网络alexnet">7.1 深度卷积神经网络（AlexNet）</h3><p>从LeNet到AlexNet的变化如下图所示：</p><figure><img src="./images/7.1_lenet-to-alexnet.svg"title="图7.1：LeNet（左）和 AlexNet（右）的设计。"alt="LeNet到AlexNet" /><figcaption aria-hidden="true">LeNet到AlexNet</figcaption></figure><ol type="1"><li>AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li><li>AlexNet使用ReLU而不是sigmoid作为其激活函数。</li><li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li><li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2lnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    <span class="token comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span>    <span class="token comment"># 同时，步幅为4，以减少输出的高度和宽度。</span>    <span class="token comment"># 另外，输出通道的数目远大于LeNet</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 使用三个连续的卷积层和较小的卷积窗口。</span>    <span class="token comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span>    <span class="token comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">6400</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="使用块的网络vgg">7.2 使用块的网络（VGG）</h3><p>与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。从AlexNet到VGG，它们本质上都是块设计，如下图所示：</p><figure><img src="./images/7.2_vgg-block.svg" title="图7.2：VGG块结构"alt="VGG块" /><figcaption aria-hidden="true">VGG块</figcaption></figure><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># VGG块(num_convs个卷积层 + 1个用于空间下采样的最大汇聚层)</span><span class="token keyword">def</span> <span class="token function">vgg_block</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span>                                kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        in_channels <span class="token operator">=</span> out_channels    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span><span class="token comment"># VGG网络</span><span class="token keyword">def</span> <span class="token function">vgg</span><span class="token punctuation">(</span>conv_arch<span class="token punctuation">)</span><span class="token punctuation">:</span>    conv_blks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    in_channels <span class="token operator">=</span> <span class="token number">1</span>    <span class="token comment"># 卷积层部分</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span> <span class="token keyword">in</span> conv_arch<span class="token punctuation">:</span>        conv_blks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>vgg_block<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>        in_channels <span class="token operator">=</span> out_channels    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        <span class="token operator">*</span>conv_blks<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment"># 全连接层部分</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>conv_arch <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 指定卷积层个数和输出通道数</span>net <span class="token operator">=</span> vgg<span class="token punctuation">(</span>conv_arch<span class="token punctuation">)</span></code></pre><h3 id="网络中的网络nin">7.3 网络中的网络（NiN）</h3><p>在每个像素位置（针对每个高度和宽度）应用一个全连接层，NiN块以一个普通卷积层开始，后面是两个<spanclass="math inline">\(1 \times 1\)</span>的卷积层。这两个<spanclass="math inline">\(1 \times1\)</span>卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为<spanclass="math inline">\(1 \times 1\)</span>。如图所示：</p><figure><img src="./images/7.3_nin-block.svg"title="图7.3：对比 VGG 和 NiN 及它们的块之间主要架构差异" alt="NiN块" /><figcaption aria-hidden="true">NiN块</figcaption></figure><ol type="1"><li>NiN使用由一个卷积层和多个<span class="math inline">\(1 \times1\)</span>卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li><li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li><li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li><li>NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># NiN块 (1个普通卷积层 + 2个1x1卷积层)</span><span class="token keyword">def</span> <span class="token function">nin_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># NiN网络</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    nin_block<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nin_block<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nin_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 标签类别数是10</span>    nin_block<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span>    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="含并行连结的网络googlenet">7.4含并行连结的网络（GoogLeNet）</h3><p>GoogLeNet吸收了NiN中串联网络的思想，重点是解决了什么样大小的卷积核最合适的问题。本文的一个观点是，有时使用不同大小的卷积核组合是有利的。Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用<spanclass="math inline">\(1 \times1\)</span>卷积层减少每像素级别上的通道维数从而降低模型复杂度。</p><p>最基本的卷积块叫作Inception块，如下图所示：</p><figure><img src="./images/7.4_inception-block.svg"title="图7.4：Inception块的结构" alt="Inception块" /><figcaption aria-hidden="true">Inception块</figcaption></figure><ol type="1"><li>由四条并行路径组成，前三条路径使用窗口大小为<spanclass="math inline">\(1\times 1\)</span>、<spanclass="math inline">\(3\times 3\)</span>和<spanclass="math inline">\(5\times5\)</span>的卷积层，来抽取不同空间尺寸下的信息，其中中间两条路径会对输入先做<spanclass="math inline">\(1\times1\)</span>卷积来减少输入通道数，以降低模型复杂度。</li><li>第四条路径使用<span class="math inline">\(3\times3\)</span>最大汇聚层，后接<span class="math inline">\(1\times1\)</span>卷积层来改变通道数。</li><li>这四条路径都使用合适的填充来使输入与输出的高和宽一致。</li><li>最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Inception块</span><span class="token keyword">class</span> <span class="token class-name">Inception</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># c1--c4是每条路径的输出通道数</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Inception<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment"># 线路1，单1x1卷积层</span>        self<span class="token punctuation">.</span>p1_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 线路2，1x1卷积层后接3x3卷积层</span>        self<span class="token punctuation">.</span>p2_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>p2_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 线路3，1x1卷积层后接5x5卷积层</span>        self<span class="token punctuation">.</span>p3_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>p3_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span>        self<span class="token punctuation">.</span>p4_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>p4_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        p1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p1_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        p2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        p3 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        p4 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 在通道维度上连结输出</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>从而组合为GoogLeNet，如下图所示：</p><figure><img src="./images/7.5_googlenet.svg" title="图7.5：GoogLeNet网络设计"alt="GoogLeNet" /><figcaption aria-hidden="true">GoogLeNet</figcaption></figure><ol type="1"><li>Inception块之间的最大汇聚层可降低维度</li><li>第一个模块类似于AlexNet和LeNet</li><li>Inception块的组合从VGG继承</li><li>全局平均汇聚层避免了在最后使用全连接层</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># GoogLeNet</span>b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">480</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">208</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">144</span><span class="token punctuation">,</span> <span class="token number">288</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">528</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">832</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   Inception<span class="token punctuation">(</span><span class="token number">832</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">,</span> b5<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="批量规范化">7.5 批量规范化</h3><ol type="1"><li>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</li><li>批量规范化在全连接层和卷积层的使用略有不同。</li><li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。</li></ol><h4 id="原理">7.5.1 原理</h4><p>原理:在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。可以用于加速深度网络的收敛速度，同时避免中间层的变化幅度过于剧烈。</p><p>用<span class="math inline">\(\mathbf{x} \in\mathcal{B}\)</span>表示一个来自小批量<spanclass="math inline">\(\mathcal{B}\)</span>的输入样本，批量规范化<spanclass="math inline">\(\mathrm{BN}\)</span>根据以下等式转换<spanclass="math inline">\(\mathbf{x}\)</span>:</p><p><span class="math display">\[\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} -\hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}}+ \boldsymbol{\beta}\tag{7.1}\]</span></p><p>其中<spanclass="math inline">\(\hat{\boldsymbol{\mu}}_\mathcal{B}\)</span>是小批量<spanclass="math inline">\(\mathcal{B}\)</span>的均值，<spanclass="math inline">\(\hat{\boldsymbol{\sigma}}_\mathcal{B}\)</span>是小批量的标准差。应用标准化后，生成的小批量的平均值为0和单位方差为1。由于单位方差是一个主观的选择，因此我们通常包含<em>拉伸参数（scale）<spanclass="math inline">\(\boldsymbol{\gamma}\)</span></em> 和<em>偏移参数（shift）<spanclass="math inline">\(\boldsymbol{\beta}\)</span></em>，它们的形状与<spanclass="math inline">\(\mathbf{x}\)</span>相同。 请注意，<spanclass="math inline">\(\boldsymbol{\gamma}\)</span>和<spanclass="math inline">\(\boldsymbol{\beta}\)</span>是需要与其他模型参数一起学习的参数。</p><p><span class="math display">\[\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &amp;=\frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &amp;= \frac{1}{|\mathcal{B}|}\sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} -\hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 +\epsilon.\end{aligned}\end{split}\tag{7.2}\]</span></p><h4 id="批量规范化层">7.5.2 批量规范化层</h4><p>批量规范化和其他层之间的一个关键区别是，由于批量规范化在完整的小批量上运行，因此我们不能像以前在引入其他层时那样忽略批量大小</p><p><strong>全连接层</strong>:将批量规范化层置于全连接层中的仿射变换和激活函数之间。<spanclass="math inline">\(\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x}+ \mathbf{b}) )\)</span> <strong>卷积层</strong>:在卷积层之后和非线性激活函数之前应用批量规范化。当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。<strong>预测过程中的批量规范化</strong>: 一种常用的方法是通过<em>移动平均估</em>算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用批量规范化层的 LeNet</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="残差网络resnet">7.6 残差网络（ResNet）</h3><p>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。在残差块中，输入可通过跨层数据线路更快地向前传播。残差网络能够解决深度神经网络中的梯度消失问题，允许构建更深的网络。如图所示：</p><figure><img src="./images/7.6_residual-block.svg"title="图7.6：一个正常块（左图）和一个残差块（右图）" alt="残差块" /><figcaption aria-hidden="true">残差块</figcaption></figure><p>如果想改变通道数，就需要引入一个额外的<span class="math inline">\(1\times1\)</span>卷积层来将输入变换成需要的形状后再做相加运算，如下图所示：</p><figure><img src="./images/7.7_residual-block-channels.svg"title="图7.7：不包含和包含通道变换的残差块" alt="残差块中的通道变换" /><figcaption aria-hidden="true">残差块中的通道变换</figcaption></figure><h3 id="稠密连接网络densenet">7.7 稠密连接网络（DenseNet）</h3><ol type="1"><li>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li><li>DenseNet的主要构建模块是稠密块和过渡层。</li><li>前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</li></ol><p>可以缓解梯度消失问题和强化特征传播，还可以减少参数数量等。</p><p>ResNet（左）与 DenseNet（右）在跨层连接上的主要区别如下图所示：</p><figure><img src="./images/7.8_resnet-densenet.svg"title="图7.8：ResNet与DenseNet在跨层连接上的主要区别: 使用相加和使用连结"alt="ResNet与DenseNet在跨层连接上的主要区别" /><figcaptionaria-hidden="true">ResNet与DenseNet在跨层连接上的主要区别</figcaption></figure><p>将ResNet的 <span class="math inline">\(f(x)=x+g(x)\)</span>(一个简单的线性项和一个复杂的非线性项) 拓展成超过两部分的信息 <spanclass="math inline">\(\mathbf{x} \to \left[\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}),f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right]\)</span></p><p>最后一层与之前的所有层紧密相连，如图所示：</p><figure><img src="./images/7.9_densenet-block.svg" title="图7.9：稠密连接"alt="稠密连接" /><figcaption aria-hidden="true">稠密连接</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 6 卷积神经网络</title>
      <link href="/2024/06/28/DL/6%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/06/28/DL/6%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="卷积神经网络">6 卷积神经网络</h2><p>卷积神经网络（convolutional neuralnetwork，CNN）是一类强大的、为处理图像数据而设计的神经网络，利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型。</p><h3 id="从全连接层到卷积层">6.1 从全连接层到卷积层</h3><ol type="1"><li>平移不变性：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。</li><li>局部性：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系。</li></ol><p>最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p><ol type="1"><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li></ol><p>其实也就是说，卷积层是用来提取<strong>图像的局部特征</strong>，全连接层是用来<strong>整合这些局部特征</strong>:<strong>卷积的本质是有效提取相邻像素间的相关特征</strong>。</p><h3 id="图像卷积">6.2 图像卷积</h3><p>输入<span class="math inline">\(n_h \timesn_w\)</span>的图像，卷积核<span class="math inline">\(k_h \timesk_w\)</span>，输出将是<span class="math inline">\((n_h - k_h + 1) \times(n_w - k_w + 1)\)</span>的图像。</p><h4 id="互相关运算">6.2.1 互相关运算</h4><p>卷积运算其实是互相关运算：</p><figure><img src="./images/6.1_correlation.svg" title="图6.1：二维互相关运算"alt="二维互相关运算" /><figcaption aria-hidden="true">二维互相关运算</figcaption></figure><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">corr2d</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> K<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""计算二维互相关运算"""</span>    h<span class="token punctuation">,</span> w <span class="token operator">=</span> K<span class="token punctuation">.</span>shape    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> h <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> w <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            Y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>X<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> h<span class="token punctuation">,</span> j<span class="token punctuation">:</span>j <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">*</span> K<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> Y</code></pre><h4 id="卷积层">6.2.2 卷积层</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Conv2D</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>kernel_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> corr2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias</code></pre><h4 id="卷积核训练">6.2.3 卷积核训练</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span>conv2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span><span class="token comment"># 其中批量大小和通道数都为1</span>X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">3e-2</span>  <span class="token comment"># 学习率</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    Y_hat <span class="token operator">=</span> conv2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  <span class="token comment"># 通过卷积层 conv2d 计算输出 Y_hat</span>    l <span class="token operator">=</span> <span class="token punctuation">(</span>Y_hat <span class="token operator">-</span> Y<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>  <span class="token comment"># 计算损失 l（均方误差损失）</span>    conv2d<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将卷积层 conv2d 中的所有参数梯度清零</span>    l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 对损失 l 进行反向传播，计算参数的梯度</span>    <span class="token comment"># 迭代卷积核</span>    conv2d<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-=</span> lr <span class="token operator">*</span> conv2d<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>grad  <span class="token comment"># 使用梯度下降法更新卷积核的权重</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>  <span class="token comment"># 每两轮迭代打印一次损失</span></code></pre><h3 id="填充和步幅">6.3 填充和步幅</h3><h4 id="填充">6.3.1 填充</h4><p>在输入图像的边界填充元素（通常填充元素是<spanclass="math inline">\(0\)</span>）。 假设填充<spanclass="math inline">\(p_h\)</span>行和<spanclass="math inline">\(p_w\)</span>列(上下左右各大约一半)，输出形状为：<spanclass="math inline">\((n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)\)</span>所以许多情况下，我们会设置<spanclass="math inline">\(p_h=k_h-1\)</span>和$p_w=k_w-1，这样输入和输出具有相同的高度和宽度。因此卷积核也通常设置为奇数，方便各填充一半。还可以得到对于任何二维张量X，当满足：1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3.输出与输入具有相同高度和宽度 则可以得出：输出Y[i, j]是通过以输入X[i,j]为中心，与卷积核进行互相关计算得到的。</p><h4 id="步幅">6.3.2 步幅</h4><p>卷积窗口可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为步幅（stride）。</p><p>为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列:</p><figure><img src="./images/6.2_strides.svg"title="图6.2：垂直步幅为 3，水平步幅为 2 的二维互相关运算" alt="步幅" /><figcaption aria-hidden="true">步幅</figcaption></figure><p>此时的输出形状为：<spanclass="math inline">\(\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times\lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.\)</span></p><h3 id="多输入通道和多输出通道">6.4 多输入通道和多输出通道</h3><h4 id="多输入通道">6.4.1 多输入通道</h4><p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核:<span class="math inline">\(c_i\times k_h\times k_w\)</span></p><figure><img src="./images/6.3_conv_multi_in.svg"title="图6.3：多输入通道的互相关计算" alt="多输入通道的互相关计算" /><figcaption aria-hidden="true">多输入通道的互相关计算</figcaption></figure><h4 id="多输出通道">6.4.2 多输出通道</h4><p>为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为<spanclass="math inline">\(c_i\times k_h\timesk_w\)</span>的卷积核张量，则<span class="math inline">\(c_o\timesc_i\times k_h\times k_w\)</span>。</p><h4 id="times-1卷积层">6.4.3 <span class="math inline">\(1\times1\)</span>卷积层</h4><p><span class="math inline">\(1\times1\)</span>卷积核的主要作用是调整网络层之间的通道数，而不是识别相邻元素间相互作用。</p><figure><img src="./images/6.4_conv_1x1.svg"title="图6.4：互相关计算使用了具有3个输入通道和2个输出通道的$1\times 1$卷积核"alt="1\times 1卷积核" /><figcaption aria-hidden="true"><span class="math inline">\(1\times1\)</span>卷积核</figcaption></figure><p><strong>本质相当于一个调整通道数的全连接层。</strong></p><h3 id="汇聚层池化层">6.5 汇聚层(池化层)</h3><p><strong>降低卷积层对位置的敏感性</strong>，同时<strong>降低对空间降采样表示的敏感性</strong></p><p>最大汇聚层和平均汇聚层是最常见的汇聚层，分别取池化窗口中输入的最大值和平均值。</p><p>相比卷积层:</p><ul><li>相同：汇聚层也有填充和步幅。</li><li>不同：1. 汇聚层的参数不需要训练，是固定的；2.在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总(这意味着汇聚层的输出通道数与输入通道数相同)。</li></ul><h3 id="卷积神经网络以-lenet-为例">6.6 卷积神经网络(以 LeNet 为例)</h3><p>LeNet（LeNet-5）由两个部分组成：</p><ol type="1"><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ol><figure><img src="./images/6.5_lenet.svg"title="图6.5：LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率"alt="LeNet" /><figcaption aria-hidden="true">LeNet</figcaption></figure><figure><img src="./images/6.6_lenet-vert.svg" title="图6.6：LeNet 的简化版"alt="LeNet 简化版" /><figcaption aria-hidden="true">LeNet 简化版</figcaption></figure><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">def</span> <span class="token function">evaluate_accuracy_gpu</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""使用GPU计算模型在数据集上的精度"""</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 设置为评估模式</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> device<span class="token punctuation">:</span>            device <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>device    <span class="token comment"># 正确预测的数量，总预测的数量</span>    metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment"># BERT微调所需的（之后将介绍）</span>                X <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">]</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            y <span class="token operator">=</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">train_ch6</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用GPU训练模型"""</span>    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear <span class="token keyword">or</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training on'</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>    <span class="token comment"># 模型转移到GPU上</span>    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train loss'</span><span class="token punctuation">,</span> <span class="token string">'train acc'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    timer<span class="token punctuation">,</span> num_batches <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Timer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 训练损失之和，训练准确率之和，样本数</span>        metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>            timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment"># 在每一个批量上计算梯度并更新模型参数，需要转移到GPU</span>            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>            train_l <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>            train_acc <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_batches <span class="token operator">//</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> i <span class="token operator">==</span> num_batches <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>                animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_batches<span class="token punctuation">,</span>                             <span class="token punctuation">(</span>train_l<span class="token punctuation">,</span> train_acc<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        test_acc <span class="token operator">=</span> evaluate_accuracy_gpu<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>train_l<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, train acc </span><span class="token interpolation"><span class="token punctuation">&#123;</span>train_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, '</span></span>          <span class="token string-interpolation"><span class="token string">f'test acc </span><span class="token interpolation"><span class="token punctuation">&#123;</span>test_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> num_epochs <span class="token operator">/</span> timer<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> examples/sec '</span></span>          <span class="token string-interpolation"><span class="token string">f'on </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                           <span class="token comment"># 在全连接层之前要先展平</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 读取数据</span>    batch_size <span class="token operator">=</span> <span class="token number">256</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>    <span class="token comment"># 训练</span>    lr<span class="token punctuation">,</span> num_epochs <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">10</span>    train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 5 深度学习计算</title>
      <link href="/2024/06/27/DL/5%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/"/>
      <url>/2024/06/27/DL/5%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习计算">5 深度学习计算</h2><h3 id="层和块">5.1 层和块</h3><p>块（block）可以描述单个层、由多个层组成的组件或整个模型本身。块由类（class）表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，并且必须存储任何必需的参数。 注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。总体来说，应该具有以下内容：</p><ol type="1"><li>将输入数据作为其前向传播函数的参数。</li><li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。</li><li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li><li>存储和访问前向传播计算所需的参数。</li><li>根据需要初始化模型参数。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token triple-quoted-string string">""" 5.1.1 自定义块 """</span><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span>        <span class="token comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>  <span class="token comment"># 隐藏层</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 输出层</span>    <span class="token comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 5.1.2 顺序块 """</span><span class="token keyword">class</span> <span class="token class-name">MySequential</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员</span>            <span class="token comment"># 变量_modules中。_module的类型是OrderedDict</span>            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>        <span class="token keyword">for</span> block <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            X <span class="token operator">=</span> block<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token keyword">return</span> Xnet <span class="token operator">=</span> MySequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 5.1.3 在前向传播函数中执行代码 """</span><span class="token keyword">class</span> <span class="token class-name">FixedHiddenMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span>        self<span class="token punctuation">.</span>rand_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 使用创建的常量参数以及relu和mm矩阵乘法函数</span>        <span class="token comment"># 此时隐藏层权重始终不会更新</span>        X <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rand_weight<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 复用全连接层。这相当于两个全连接层共享参数</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 控制流</span>        <span class="token keyword">while</span> X<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            X <span class="token operator">/=</span> <span class="token number">2</span>        <span class="token keyword">return</span> X<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 5.1.4 混搭各种组合块 """</span><span class="token keyword">class</span> <span class="token class-name">NestMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>chimera <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>NestMLP<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> FixedHiddenMLP<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="参数管理">5.2 参数管理</h3><ol type="1"><li>访问参数，用于调试、诊断和可视化；</li><li>参数初始化；</li><li>在不同模型组件间共享参数。</li></ol><p>具体操作略。</p><h3 id="延后初始化">5.3 延后初始化</h3><ol type="1"><li>定义了网络架构，但没有指定输入维度。</li><li>添加层时没有指定前一层的输出维度。</li><li>在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。</li></ol><p>例如:<code>net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(),nn.Linear(256,10))</code></p><h3 id="自定义层">5.4 自定义层</h3><h4 id="不含模型参数的自定义层">5.4.1 不含模型参数的自定义层</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CenteredLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> X <span class="token operator">-</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CenteredLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="含模型参数的自定义层">5.4.2 含模型参数的自定义层</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyLinear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>units<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>linear<span class="token punctuation">)</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>MyLinear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> MyLinear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="读写文件">5.5 读写文件</h3><p>定期保存中间结果</p><h4 id="读写张量">5.5.1 读写张量</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 张量</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'x-file'</span><span class="token punctuation">)</span>x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'x-file'</span><span class="token punctuation">)</span><span class="token comment"># 张量列表</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'x-files'</span><span class="token punctuation">)</span>x2<span class="token punctuation">,</span> y2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'x-files'</span><span class="token punctuation">)</span><span class="token comment"># 从字符串映射到张量的字典</span>mydict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'x'</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">:</span> y<span class="token punctuation">&#125;</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>mydict<span class="token punctuation">,</span> <span class="token string">'mydict'</span><span class="token punctuation">)</span>mydict2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'mydict'</span><span class="token punctuation">)</span></code></pre><h4 id="读写模型参数">5.5.2 读写模型参数</h4><p>注意：<strong>仅保存模型参数而不是整个模型</strong>。</p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'mlp.params'</span><span class="token punctuation">)</span>clone <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>clone<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'mlp.params'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="gpu计算">5.6 GPU计算</h3><p>在PyTorch中，每个数组都有一个设备（device），我们通常将其称为环境（context）。</p><ol type="1"><li>默认情况下，所有变量和相关的计算都分配给CPU。</li><li>计算的所有输入数据都必须在同一设备上，无论是CPU还是GPU。</li><li>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPyndarray中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是在GPU内存中为日志分配专门的空间，将小批量的损失累积到较大的日志中，只有在必要时才将整个日志移动到CPU进行打印或记录。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()"""</span>    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">try_all_gpus</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""</span>    devices <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>             <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> devices <span class="token keyword">if</span> devices <span class="token keyword">else</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment"># 张量</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>try_gpu<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># GPU0 上创建张量</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>try_gpu<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># GPU1 上创建张量</span><span class="token comment"># 如果要计算 X + Y，我们需要决定在哪里存储结果，不能简单的使用 X + Y</span>Z <span class="token operator">=</span> X<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X <span class="token operator">+</span> Y<span class="token punctuation">)</span> <span class="token comment"># 异常</span><span class="token keyword">print</span><span class="token punctuation">(</span>Z <span class="token operator">+</span> Y<span class="token punctuation">)</span> <span class="token comment"># 正常</span><span class="token comment"># 模型</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><ol type="1"><li>拷贝操作要格外小心。根据经验，多个小操作比一个大操作糟糕得多，一次执行几个操作比代码中散布的许多单个操作要好得多。</li><li>当打印张量或将张量转换为NumPy格式时，如果数据不在CPU内存中，框架会首先将其复制到内存中，这会导致额外的传输开销。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 4 多层感知机</title>
      <link href="/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2024/06/26/DL/4%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="多层感知机">4 多层感知机</h2><p>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。</p><h3 id="多层感知机-1">4.1 多层感知机</h3><p>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</p><h4 id="隐藏层">4.1.1 隐藏层</h4><h5 id="线性模型的局限性">4.1.1.1 线性模型的局限性</h5><p>线性模型的一个主要局限性是它们只能表示输入和输出之间的简单关系。如果我们想要模拟更复杂的非线性关系，就需要更复杂的模型。</p><h5 id="在网络中加入隐藏层">4.1.1.2 在网络中加入隐藏层</h5><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。我们可以把前<spanclass="math inline">\(L-1\)</span>层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。</p><figure><img src="./images/4.1_multilayer-perceptron.svg"title="图4.1：多层感知机" alt="多层感知机" /><figcaption aria-hidden="true">多层感知机</figcaption></figure><p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p>然而，具有全连接层的多层感知机的参数开销可能会高得令人望而却步。即使在不改变输入或输出大小的情况下，需要在参数节约和模型有效性之间进行权衡。</p><h5 id="从线性到非线性">4.1.1.3 从线性到非线性</h5><p>即使在添加隐藏层后，按照之前的逻辑，我们的输出仍然是输入的线性组合。也就是说，即使我们有无限多的隐藏层，多层感知机仍然等同于线性模型。</p><p>原因如下:</p><p><span class="math display">\[\begin{gathered}\mathbf{H} &amp; = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\end{gathered}\tag{4.1}\]</span></p><p>其中<span class="math inline">\(X\)</span>表示输入，<spanclass="math inline">\(H\)</span>表示隐藏层的输出，<spanclass="math inline">\(O\)</span>表示输出层的输出，<spanclass="math inline">\(\mathbf{W}^{(1)}\)</span>和<spanclass="math inline">\(\mathbf{W}^{(2)}\)</span>是权重参数，<spanclass="math inline">\(\mathbf{b}^{(1)}\)</span>和<spanclass="math inline">\(\mathbf{b}^{(2)}\)</span>是偏置参数。</p><p>上面的隐藏单元由输入<spanclass="math inline">\(X\)</span>的仿射函数给出， 而输出<spanclass="math inline">\(O\)</span>（softmax操作前）只是隐藏单元<spanclass="math inline">\(H\)</span>的仿射函数。仿射函数的仿射函数本身就是仿射函数， 所以这和线性模型没有区别。</p><p><strong>引入关键因素</strong>:在仿射变换之后对每个隐藏单元应用非线性的激活函数<spanclass="math inline">\(\sigma\)</span></p><p><span class="math display">\[\begin{gathered}\mathbf{H} &amp; = \sigma(\mathbf{X} \mathbf{W}^{(1)} +\mathbf{b}^{(1)}), \\\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\\end{gathered}\tag{4.2}\]</span></p><h5 id="通用近似定理">4.1.1.4 通用近似定理</h5><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。</p><h4 id="激活函数">4.1.2 激活函数</h4><p>激活函数通过<strong>计算加权和</strong>并加上<strong>偏置</strong>来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。常用的激活函数包括<strong>ReLU函数、sigmoid函数和tanh函数</strong>。</p><p>其函数和导数如下图所示：</p><figure><img src="./images/4.2_activation-functions.svg" title="图4.2：激活函数"alt="激活函数" /><figcaption aria-hidden="true">激活函数</figcaption></figure><p>绘图代码见<a href="#附录a激活函数绘图代码">附录A</a>。</p><h5 id="relu函数修正线性单元">4.1.2.1 ReLU函数(修正线性单元)</h5><p>ReLU函数仅保留正元素并丢弃所有负元素，给定元素<spanclass="math inline">\(x\)</span>，ReLU函数被定义为该元素与<spanclass="math inline">\(0\)</span>的最大值：</p><p><span class="math display">\[\begin{equation}\begin{gathered}\operatorname{ReLU}(x) = \max(x, 0).\end{gathered}\end{equation}\tag{4.3}\]</span></p><p>使用ReLU的原因是，它求导表现得特别好：<strong>要么让参数消失，要么让参数通过</strong>。这使得优化表现得更好，并且<strong>ReLU减轻了困扰以往神经网络的梯度消失问题</strong>。</p><p>**注意，ReLU函数有许多变体，包括参数化ReLU（pReLU） 函数。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：*</p><p><span class="math display">\[\begin{equation}\begin{gathered}\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).\end{gathered}\end{equation}\tag{4.4}\]</span></p><h5 id="sigmoid函数挤压函数">4.1.2.2 sigmoid函数(挤压函数)</h5><p>sigmoid函数将范围 <span class="math inline">\((-inf, inf)\)</span>中的任意输入压缩到区间 <span class="math inline">\((0, 1)\)</span>中的某个值：</p><p><span class="math display">\[\begin{equation}\begin{gathered}\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.\end{gathered}\end{equation}\tag{4.5}\]</span></p><p>当我们想要将输出视作<strong>二元分类</strong>问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（<strong>sigmoid可以视为softmax的特例</strong>）。然而，sigmoid在<strong>隐藏层中已经较少使用</strong>，它在大部分时候被更简单、更容易训练的<strong>ReLU所取代</strong>。</p><h5 id="tanh函数双曲正切函数">4.1.2.3 tanh函数(双曲正切函数)</h5><p>tanh函数也能将其输入压缩转换到区间<span class="math inline">\((-1,1)\)</span>上:</p><p><span class="math display">\[\begin{equation}\begin{gathered}\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.\end{gathered}\end{equation}\tag{4.6}\]</span></p><h3 id="多层感知机的从零开始实现">4.2 多层感知机的从零开始实现</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token comment"># 4.2.3 激活函数</span><span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>    a <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment"># 4.2.4 模型</span><span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 因为忽略了空间结构，所以需要使用reshape将每个二维图像转换为一个长度为num_inputs的向量</span>    X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>    H <span class="token operator">=</span> relu<span class="token punctuation">(</span>X@W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span>  <span class="token comment"># 这里“@”代表矩阵乘法</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>H@W2 <span class="token operator">+</span> b2<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment"># 4.2 多层感知机的从零开始实现</span>    <span class="token comment"># 4.2.1 读取数据</span>    batch_size <span class="token operator">=</span> <span class="token number">256</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>    <span class="token comment"># 4.2.2 初始化模型参数</span>    num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>    W1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>        num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>    b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    W2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>        num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>    b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    params <span class="token operator">=</span> <span class="token punctuation">[</span>W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span>    <span class="token comment"># 4.2.3 激活函数</span>    <span class="token comment"># 4.2.4 模型</span>    <span class="token comment"># 4.2.5 损失函数</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token comment"># reduction='none'表示直接返回每个样本的损失，其他有sum、mean</span>    <span class="token comment"># 4.2.6 训练</span>    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span>    updater <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> updater<span class="token punctuation">)</span>    <span class="token comment"># 4.2.7 预测</span>    d2l<span class="token punctuation">.</span>predict_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span></code></pre><h3 id="多层感知机的简洁实现">4.3 多层感知机的简洁实现</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment"># 4.3 多层感知机的简洁实现</span>    <span class="token comment"># 4.3.1 定义模型</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token comment"># 将输入的多维张量转换为一维张量</span>                        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 全连接层</span>                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token comment"># 激活函数</span>                        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># 全连接层</span>    <span class="token comment"># 4.3.2 初始化模型参数</span>    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>    <span class="token comment"># 4.3.3 读取数据并训练模型</span>    batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">10</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>    <span class="token comment"># 4.3.4 预测</span>    d2l<span class="token punctuation">.</span>predict_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span></code></pre><h3 id="模型选择欠拟合和过拟合">4.4 模型选择、欠拟合和过拟合</h3><p>如何发现可以<strong>泛化</strong>的模式是机器学习的根本问题。</p><h4 id="训练误差和泛化误差">4.4.1 训练误差和泛化误差</h4><p>训练误差是指， 模型在训练数据集上计算得到的误差。 泛化误差是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。(实际中只能通过测试数据集来近似估计泛化误差)</p><p>几个倾向于影响模型泛化的因素:</p><ol type="1"><li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li><li>训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。</li></ol><h4 id="模型选择">4.4.2 模型选择</h4><p>训练集: 用于训练模型的数据集 验证集: 用于调整模型超参数的数据集测试集: 用于评估模型泛化误差的数据集</p><table><thead><tr><th>数据集</th><th>用途</th><th>参与训练</th><th>用来调参</th><th>用来评估最终性能</th></tr></thead><tbody><tr><td><strong>训练集</strong></td><td>拿来训练模型</td><td>✅ 是</td><td>❌ 否</td><td>❌ 否</td></tr><tr><td><strong>验证集</strong></td><td>拿来选模型/调参数</td><td>❌ 否</td><td>✅ 是</td><td>❌ 否</td></tr><tr><td><strong>测试集</strong></td><td>拿来最终评估</td><td>❌ 否</td><td>❌ 否</td><td>✅ 是</td></tr></tbody></table><p>（但现实是验证数据和测试数据之间的边界模糊得令人担忧。）</p><p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是<strong>采用<spanclass="math inline">\(K\)</span>折交叉验证</strong>。这里，原始训练数据被分成<spanclass="math inline">\(K\)</span>个不重叠的子集。 然后执行<spanclass="math inline">\(K\)</span>次模型训练和验证，每次在<spanclass="math inline">\(K-1\)</span>个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对<spanclass="math inline">\(K\)</span>次实验的结果取平均来估计训练和验证误差。</p><h4 id="欠拟合和过拟合">4.4.3 欠拟合和过拟合</h4><p><strong>欠拟合</strong> 训练误差和验证误差都很严重，但它们之间仅有一点差距。如果<strong>模型不能降低训练误差</strong>，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。</p><p><strong>过拟合</strong> 当我们的训练误差明显低于验证误差时要小心，这表明严重的过拟合。 <strong>注意，过拟合并不总是一件坏事。</strong>特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。<strong>最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</strong></p><p><strong><em>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小</em></strong></p><h5 id="模型复杂性">4.4.3.1 模型复杂性</h5><p>比如一个线性回归模型的拟合(特征是<spanclass="math inline">\(x\)</span>的幂给出的， 模型的权重是<spanclass="math inline">\(w_i\)</span>给出的，偏置是<spanclass="math inline">\(w_0\)</span>给出):</p><p><span class="math display">\[\begin{equation}\begin{gathered}\hat{y}= \sum_{i=0}^d x^i w_i\end{gathered}\end{equation}\tag{4.7}\]</span></p><p>高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。因此在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。事实上，当数据样本包含了<spanclass="math inline">\(x\)</span>的不同值时，函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。</p><p>模型的选择和拟合情况如下图所示：</p><figure><img src="./images/4.3_model-complexity-vs-dataset-size.svg"title="图4.3：模型复杂度对欠拟合和过拟合的影响"alt="模型的选择和拟合情况" /><figcaption aria-hidden="true">模型的选择和拟合情况</figcaption></figure><h5 id="数据集大小">4.4.3.2 数据集大小</h5><p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小。</p><h4 id="多项式回归">4.4.4 多项式回归</h4><p>通过多项式拟合来探索这些概念</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> math<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">def</span> <span class="token function">evaluate_loss</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># @save</span>    <span class="token triple-quoted-string string">"""评估给定数据集上模型的损失"""</span>    metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 损失的总和,样本数量</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>        out <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>out<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> l<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span>          num_epochs<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>    input_shape <span class="token operator">=</span> train_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment"># 不设置偏置，因为我们已经在多项式中实现了它</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    batch_size <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                batch_size<span class="token punctuation">)</span>    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                               batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1e-3</span><span class="token punctuation">,</span> <span class="token number">1e2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        d2l<span class="token punctuation">.</span>train_epoch_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span>        <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">20</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>                                     evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'weight:'</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token comment"># 1. 生成数据集</span>    max_degree <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># 多项式的最大阶数</span>    n_train<span class="token punctuation">,</span> n_test <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span>  <span class="token comment"># 训练和测试数据集大小</span>    true_w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span>  <span class="token comment"># 分配大量的空间</span>    true_w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">,</span> <span class="token number">5.6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    features <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_train <span class="token operator">+</span> n_test<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 噪声项</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>features<span class="token punctuation">)</span> <span class="token comment"># 打乱顺序</span>    poly_features <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>features<span class="token punctuation">,</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_degree<span class="token punctuation">)</span><span class="token punctuation">:</span>        poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/=</span> math<span class="token punctuation">.</span>gamma<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># gamma(n)=(n-1)!</span>    <span class="token comment"># labels的维度:(n_train+n_test,)</span>    labels <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>poly_features<span class="token punctuation">,</span> true_w<span class="token punctuation">)</span>    labels <span class="token operator">+=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token comment"># NumPy ndarray转换为tensor</span>    true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>        x<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span>true_w<span class="token punctuation">,</span> features<span class="token punctuation">,</span> poly_features<span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token comment"># 2. 定义、训练和测试模型</span>    <span class="token comment"># 在外部函数中定义</span>    <span class="token comment"># 3.1  三阶多项式函数拟合(正常)</span>    <span class="token comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span>    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 3.2 线性函数拟合(欠拟合)</span>    <span class="token comment"># 从多项式特征中选择前2个维度，即1和x</span>    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 减少该模型的训练损失相对困难，在最后一个迭代周期完成后，训练损失仍然很高。</span>    <span class="token comment"># 3.3 高阶多项式函数拟合(过拟合)</span>    <span class="token comment"># 从多项式特征中选取所有维度，即1,x,x^2/2!,...,x^19/19!</span>    train<span class="token punctuation">(</span>poly_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> poly_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          labels<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">1500</span><span class="token punctuation">)</span>    <span class="token comment"># 训练误差迅速降低，但测试损失仍然很高</span></code></pre><p>数据集的生成：</p><p><span class="math display">\[\begin{equation}\begin{gathered}y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{where }\epsilon \sim \mathcal{N}(0, 0.1^2)\end{gathered}\end{equation}\tag{4.8}\]</span></p><p>在优化的过程中，我们通常希望避免非常大的梯度值或损失值。这就是我们将特征从 <span class="math inline">\(x^i\)</span> 调整为 <spanclass="math inline">\(\frac{x^i}{i!}\)</span> 的原因，这样可以避免很大的 <span class="math inline">\(i\)</span>带来的特别大的指数值。</p><h5 id="使用线性函数拟合欠拟合">4.4.4.1 使用线性函数拟合(欠拟合)</h5><p>减少该模型的训练损失相对困难。在最后一个迭代周期完成后，训练损失仍然很高。</p><figure><img src="./images/4.4_linear-function-fit.svg"title="图4.4：使用线性函数拟合" alt="使用线性函数拟合" /><figcaption aria-hidden="true">使用线性函数拟合</figcaption></figure><h5 id="使用三阶多项式函数拟合合适">4.4.4.2使用三阶多项式函数拟合(合适)</h5><p>该模型能有效降低训练损失和测试损失。学习到的模型参数也接近真实值。</p><figure><img src="./images/4.5_cubic-function-fit.svg"title="图4.5：使用三阶多项式函数拟合" alt="使用三阶多项式函数拟合" /><figcaption aria-hidden="true">使用三阶多项式函数拟合</figcaption></figure><h5 id="使用高阶多项式函数拟合过拟合">4.4.4.3使用高阶多项式函数拟合(过拟合)</h5><p>在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。虽然训练损失可以有效地降低，但测试损失仍然很高。</p><figure><img src="./images/4.6_high-order-function-fit.svg"title="图4.6：使用高阶多项式函数拟合" alt="使用高阶多项式函数拟合" /><figcaption aria-hidden="true">使用高阶多项式函数拟合</figcaption></figure><h3 id="权重衰减">4.5 权重衰减</h3><p>虽然可以通过去收集更多的训练数据来缓解<strong>过拟合</strong>，但是……并不太可能假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在<strong>正则化技术</strong>上。正则化是处理<strong>过拟合</strong>的常用方法：<strong>在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度</strong>。<strong>权重衰减（weight decay）是最广泛使用的正则化的技术之一，它通常也被称为<spanclass="math inline">\(L_2\)</span>正则化。</strong></p><p>原来的损失函数:</p><p><span class="math display">\[\begin{equation}\begin{gathered}L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n\frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2\end{gathered}\end{equation}\tag{4.9}\]</span></p><p>现在的损失函数(通过正则化常数<spanclass="math inline">\(\lambda\)</span>来描述这种权衡，这是一个非负超参数):</p><p><span class="math display">\[\begin{equation}\begin{gathered}L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2\end{gathered}\end{equation}\tag{4.10}\]</span></p><h4 id="高维线性回归">4.5.1 高维线性回归</h4><p>使用的数据集由下面公式生成:</p><p><span class="math display">\[\begin{equation}\begin{gathered}y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }\epsilon \sim \mathcal{N}(0, 0.01^2)\end{gathered}\end{equation}\tag{4.11}\]</span></p><p>为了使过拟合的效果更加明显，我们可以将问题的维数增加到 <spanclass="math inline">\(d=200\)</span>，并使用一个只包含20个样本的小训练集。</p><h4 id="从零开始实现">4.5.2 从零开始实现</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> torch<span class="token comment"># 随机初始化模型参数</span><span class="token keyword">def</span> <span class="token function">init_params</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    w <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token comment"># L2范数惩罚</span><span class="token keyword">def</span> <span class="token function">l2_penalty</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token comment"># 定义训练</span><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>lambd<span class="token punctuation">)</span><span class="token punctuation">:</span>    w<span class="token punctuation">,</span> b <span class="token operator">=</span> init_params<span class="token punctuation">(</span><span class="token punctuation">)</span>    net<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token keyword">lambda</span> X<span class="token punctuation">:</span> d2l<span class="token punctuation">.</span>linreg<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>squared_loss    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0.003</span>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epochs'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            <span class="token comment"># 增加了L2范数惩罚项，</span>            <span class="token comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">+</span> lambd <span class="token operator">*</span> l2_penalty<span class="token punctuation">(</span>w<span class="token punctuation">)</span>            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>                                     d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w的L2范数是:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    n_train<span class="token punctuation">,</span> n_test<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">5</span>    true_w<span class="token punctuation">,</span> true_b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.05</span>    train_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_train<span class="token punctuation">)</span>    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    test_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_test<span class="token punctuation">)</span>    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token comment"># 无正则化</span>    train<span class="token punctuation">(</span>lambd<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># 使用权重衰减</span>    train<span class="token punctuation">(</span>lambd<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre><h5 id="不使用正则化">4.5.2.1 不使用正则化</h5><p>这里训练误差有了减少，但测试误差没有减少，这意味着出现了严重的过拟合</p><figure><img src="./images/4.7_no-weight-decay.svg" title="图4.7：不使用正则化"alt="不使用正则化" /><figcaption aria-hidden="true">不使用正则化</figcaption></figure><h5 id="使用权重衰减">4.5.2.2 使用权重衰减</h5><p>在这里训练误差增大，但测试误差减小。这正是我们期望从正则化中得到的效果。</p><figure><img src="./images/4.8_weight-decay.svg" title="图4.8：使用权重衰减"alt="使用权重衰减" /><figcaption aria-hidden="true">使用权重衰减</figcaption></figure><h4 id="简洁实现">4.5.3 简洁实现</h4><p>深度学习框架为了便于我们使用权重衰减，将权重衰减集成到优化算法中，以便与任何损失函数结合使用。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> torch<span class="token keyword">def</span> <span class="token function">train_concise</span><span class="token punctuation">(</span>wd<span class="token punctuation">)</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0.003</span>    <span class="token comment"># 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了weight_decay，所以偏置参数b不会衰减。</span>    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> wd<span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">&#123;</span><span class="token string">"params"</span><span class="token punctuation">:</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epochs'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span>                            xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>            l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span>                         <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">,</span>                          d2l<span class="token punctuation">.</span>evaluate_loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w的L2范数:'</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    n_train<span class="token punctuation">,</span> n_test<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">5</span>    true_w<span class="token punctuation">,</span> true_b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.05</span>    train_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_train<span class="token punctuation">)</span>    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    test_data <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> n_test<span class="token punctuation">)</span>    test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token comment"># 无正则化</span>    train_concise<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># 使用权重衰减</span>    train_concise<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre><h5 id="不使用正则化-1">4.5.3.1 不使用正则化</h5><figure><img src="./images/4.9_no-weight-decay.svg" title="图4.9：不使用正则化"alt="不使用正则化" /><figcaption aria-hidden="true">不使用正则化</figcaption></figure><h5 id="使用权重衰减-1">4.5.3.2 使用权重衰减</h5><figure><img src="./images/4.10_weight-decay.svg" title="图4.10：使用权重衰减"alt="使用权重衰减" /><figcaption aria-hidden="true">使用权重衰减</figcaption></figure><h3 id="暂退法">4.6 暂退法</h3><p><strong>暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。</strong>这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（dropout）一些神经元。<strong>在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</strong></p><p>那么关键的挑战就是如何注入这种噪声。一种想法是以一种无偏向（unbiased）的方式注入噪声。这样在固定住其他层时，每一层的期望值等于没有噪音时的值。在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差:</p><p><span class="math display">\[\begin{split}\begin{aligned}h&#39; =\begin{cases}    0 &amp; \text{ 概率为 } p \\    \frac{h}{1-p} &amp; \text{ 其他情况}\end{cases}\end{aligned}\end{split}\tag{4.12}\]</span></p><p>期望值保持不变，即 <span class="math inline">\(E[h&#39;] =h\)</span>。</p><h4 id="实践中的暂退法">4.6.1 实践中的暂退法</h4><p>当我们将暂退法应用到隐藏层，以<spanclass="math inline">\(p\)</span>的概率将隐藏单元置为零时，结果可以看作一个只包含原始神经元子集的网络。 如图4.11， 删除了<spanclass="math inline">\(h_2\)</span>和<spanclass="math inline">\(h_5\)</span>， 因此输出的计算不再依赖于<spanclass="math inline">\(h_2\)</span>或<spanclass="math inline">\(h_5\)</span>，并且它们各自的梯度在执行反向传播时也会消失。这样，输出层的计算不能过度依赖于<spanclass="math inline">\(h_1,...,h_5\)</span>的任何一个元素。</p><figure><img src="./images/4.11_dropout_MLP.svg"title="图4.11：dropout前后的多层感知机" alt="dropout前后的多层感知机" /><figcaption aria-hidden="true">dropout前后的多层感知机</figcaption></figure><p>通常，我们在测试时不用暂退法。给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。然而也有一些例外：一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p><h4 id="从零开始实现-1">4.6.2 从零开始实现</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token comment"># 定义一个dropout函数</span><span class="token keyword">def</span> <span class="token function">dropout_layer</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> dropout <span class="token operator">&lt;=</span> <span class="token number">1</span>    <span class="token comment"># 在本情况中，所有元素都被丢弃</span>    <span class="token keyword">if</span> dropout <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token comment"># 在本情况中，所有元素都被保留</span>    <span class="token keyword">if</span> dropout <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> X    mask <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">></span> dropout<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 生成一个mask</span>    <span class="token keyword">return</span> mask <span class="token operator">*</span> X <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> dropout<span class="token punctuation">)</span> <span class="token comment"># 按位mask</span><span class="token comment"># 测试dropout函数</span><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">,</span>                 is_training <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_inputs <span class="token operator">=</span> num_inputs        self<span class="token punctuation">.</span>training <span class="token operator">=</span> is_training        self<span class="token punctuation">.</span>lin1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lin2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lin3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        H1 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lin1<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 只有在训练模型时才使用dropout</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>            <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>            H1 <span class="token operator">=</span> dropout_layer<span class="token punctuation">(</span>H1<span class="token punctuation">,</span> dropout1<span class="token punctuation">)</span>        H2 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lin2<span class="token punctuation">(</span>H1<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>            <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>            H2 <span class="token operator">=</span> dropout_layer<span class="token punctuation">(</span>H2<span class="token punctuation">,</span> dropout2<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>lin3<span class="token punctuation">(</span>H2<span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span>    <span class="token comment"># 定义模型</span>    <span class="token comment"># 我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 </span>    <span class="token comment"># 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。</span>    dropout1<span class="token punctuation">,</span> dropout2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span>    net <span class="token operator">=</span> Net<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>    <span class="token comment"># 训练和测试模型</span>    num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">256</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></code></pre><h4 id="简洁实现-1">4.6.3 简洁实现</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2lnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>                    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout1<span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>                    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout2<span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></code></pre><h3 id="前向传播反向传播和计算图">4.7 前向传播、反向传播和计算图</h3><h4 id="前向传播">4.7.1 前向传播</h4><p>按顺序（从输入层到输出层）<strong>计算</strong>和<strong>存储</strong>神经网络中每层的结果。</p><p><span class="math display">\[\begin{equation}\begin{gathered}\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x}\\\mathbf{h}= \phi(\mathbf{z})\\\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}\\\end{gathered}\end{equation}\tag{4.13}\]</span></p><p>损失函数<span class="math inline">\(l\)</span>，样本标签<spanclass="math inline">\(y\)</span>，则损失项，正则项，正则化损失:</p><p><span class="math display">\[\begin{gathered}L = l(\mathbf{o}, y)\\s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 +\|\mathbf{W}^{(2)}\|_F^2\right)\\J = L + s\end{gathered}\tag{4.14}\]</span></p><p>目标函数即为<span class="math inline">\(J\)</span>。</p><h4 id="前向传播计算图">4.7.2 前向传播计算图</h4><p>正方形表示变量，圆圈表示操作符，左下角表示输入，右上角表示输出。</p><figure><img src="./images/4.12_forward.svg" title="图4.12：前向传播计算图"alt="前向传播计算图" /><figcaption aria-hidden="true">前向传播计算图</figcaption></figure><h4 id="反向传播">4.7.3 反向传播</h4><p>反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的<strong>链式规则</strong>，按相反的顺序从输出层到输入层遍历网络。<strong>该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）</strong>。</p><p>在此例子中，即通过链式法则计算<span class="math inline">\(\partialJ/\partial \mathbf{W}^{(1)}\)</span>和<spanclass="math inline">\(\partial J/\partial\mathbf{W}^{(2)}\)</span>。</p><p><span class="math display">\[\begin{gathered}  \frac{\partial J}{\partial \mathbf{W}^{(2)}}=\text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial\mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) +\text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partials}{\partial \mathbf{W}^{(2)}}\right)\\  \frac{\partial J}{\partial \mathbf{W}^{(1)}}  = \text{prod}\left(\frac{\partial J}{\partial \mathbf{z}},\frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) +\text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partials}{\partial \mathbf{W}^{(1)}}\right)\end{gathered}\tag{4.15}\]</span></p><h4 id="内存需求">4.7.4 内存需求</h4><p>因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，<strong>反向传播重复利用前向传播中存储的中间值</strong>，以避免重复计算。带来的影响之一是我们<strong>需要保留中间值</strong>，直到反向传播完成。这也是<strong>训练比单纯的预测需要更多的内存（显存）</strong>的原因之一。此外，这些<strong>中间值的大小与网络层的数量和批量的大小大致成正比</strong>。因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out ofmemory）错误。</p><h3 id="数值稳定性和模型初始化">4.8 数值稳定性和模型初始化</h3><p>初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</p><ol type="1"><li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li></ol><h4 id="梯度爆炸梯度消失打破对称性">4.8.1梯度爆炸、梯度消失、打破对称性</h4><p><strong>梯度消失（gradient vanishing）</strong>：<strong>参数更新过小，在每次更新时几乎不会移动</strong>，导致模型无法学习。根据链式求导法则，梯度的计算是由不同因子的连乘结果，只要其中某个因子的数值小于1那么随着网络的加深，后续的梯度一定是逐渐降低的（假设其他因子设置合理）。如果因子的数值够低，后续梯度甚至会出现消失现象，导致网络难以训练和收敛，这就是梯度消失的现象。例如:当sigmoid函数的输入很大或是很小时，它的梯度都会消失。<strong>梯度爆炸（gradient exploding）</strong>：<strong>参数更新过大，破坏了模型的稳定收敛</strong>。同梯度消失的原理一样，梯度爆炸也是因为因子的数值大于1，在经过网络的不断加深，后续梯度出现爆炸的现象。<strong>打破对称性</strong>：神经网络设计中的另一个问题是其参数化所固有的对称性。如果我们<strong>将隐藏层的所有参数初始化为同一个常量</strong>，会发生什么？在这种情况下，在前向传播期间，隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数对输出单元进行微分，得到一个梯度，其元素都取相同的值。 因此，在基于梯度的迭代之后，隐藏单元的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。<strong>隐藏层的行为就好像只有一个单元</strong>。请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p><h4 id="初始化参数">4.8.2 初始化参数</h4><ol type="1"><li>Pytorch的默认初始化</li><li>Xavier初始化: 从均值为零，方差 <span class="math inline">\(\sigma^2= \frac{2}{n_\mathrm{in} + n_\mathrm{out}}\)</span>的高斯分布中采样权重。</li></ol><h3 id="环境与分布偏移">4.9 环境与分布偏移</h3><p><strong>分布偏移（DistributionShift）是指模型在训练和测试数据集之间的数据分布不匹配的情况。</strong>这种不匹配可能导致模型在测试集上的表现下降，因为模型在训练时学习到的特征在测试时可能不再适用。</p><h4 id="分布偏移的类型">4.9.1 分布偏移的类型</h4><ol type="1"><li><strong>协变量（特征）偏移</strong>：虽然输入的分布改变，但标签没有改变。例如猫狗识别，训练数据主要来自家养，而测试数据主要来自野外拍摄，此时输入数据的分布发生了变化，但标签（猫或狗）没有变化。</li><li><strong>标签偏移</strong>：虽然输入的分布保持不变，但标签的分布改变。假设你训练了一个模型来预测某城市的天气，训练数据中晴天和雨天的比例是9:1，但测试数据中这个比例变成了1:1。输入的天气特征分布保持不变，但标签的分布发生了变化。</li><li><strong>概念偏移</strong>：输入和输出之间的映射关系发生了变化，即特征和标签之间的关系变了。例如，有一个垃圾邮件分类器，训练数据中垃圾邮件的特征是某些关键词（如“免费”、“优惠”等），但随着时间推移，垃圾邮件发送者改变了策略，使用了新的关键词（如“促销”、“折扣”等）。</li></ol><h4 id="分布偏移纠正">4.9.2 分布偏移纠正</h4><p>可以采用<strong>加权经验风险最小化</strong>等方法。</p><p>注：真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。</p><h3 id="实战kaggle比赛房价预测">4.10 实战Kaggle比赛：房价预测</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> hashlib<span class="token keyword">import</span> os<span class="token keyword">import</span> tarfile<span class="token keyword">import</span> zipfile<span class="token keyword">import</span> requests<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token keyword">def</span> <span class="token function">download</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'..'</span><span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""下载一个DATA_HUB中的文件，返回本地文件名"""</span>    <span class="token keyword">assert</span> name <span class="token keyword">in</span> DATA_HUB<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string"> 不存在于 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>DATA_HUB<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>    url<span class="token punctuation">,</span> sha1_hash <span class="token operator">=</span> DATA_HUB<span class="token punctuation">[</span>name<span class="token punctuation">]</span>    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>cache_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    fname <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cache_dir<span class="token punctuation">,</span> url<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">:</span>        sha1 <span class="token operator">=</span> hashlib<span class="token punctuation">.</span>sha1<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 初始化一个sha1对象</span>        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>            <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>                data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token number">1048576</span><span class="token punctuation">)</span>  <span class="token comment"># 逐块读取文件内容，每次读取 1 MB</span>                <span class="token keyword">if</span> <span class="token keyword">not</span> data<span class="token punctuation">:</span>                    <span class="token keyword">break</span>                sha1<span class="token punctuation">.</span>update<span class="token punctuation">(</span>data<span class="token punctuation">)</span>  <span class="token comment"># sha1可以在历史哈希值基础上用当前文件内容更新哈希值</span>        <span class="token keyword">if</span> sha1<span class="token punctuation">.</span>hexdigest<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> sha1_hash<span class="token punctuation">:</span>            <span class="token keyword">return</span> fname  <span class="token comment"># 命中缓存</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'正在从</span><span class="token interpolation"><span class="token punctuation">&#123;</span>url<span class="token punctuation">&#125;</span></span><span class="token string">下载</span><span class="token interpolation"><span class="token punctuation">&#123;</span>fname<span class="token punctuation">&#125;</span></span><span class="token string">...'</span></span><span class="token punctuation">)</span>    r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> stream<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> verify<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>r<span class="token punctuation">.</span>content<span class="token punctuation">)</span>    <span class="token keyword">return</span> fname<span class="token keyword">def</span> <span class="token function">download_extract</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> folder<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""下载并解压zip/tar文件"""</span>    fname <span class="token operator">=</span> download<span class="token punctuation">(</span>name<span class="token punctuation">)</span>    base_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>    data_dir<span class="token punctuation">,</span> ext <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>    <span class="token keyword">if</span> ext <span class="token operator">==</span> <span class="token string">'.zip'</span><span class="token punctuation">:</span>        fp <span class="token operator">=</span> zipfile<span class="token punctuation">.</span>ZipFile<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> ext <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'.tar'</span><span class="token punctuation">,</span> <span class="token string">'.gz'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        fp <span class="token operator">=</span> tarfile<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'只有zip/tar文件可以被解压缩'</span>    fp<span class="token punctuation">.</span>extractall<span class="token punctuation">(</span>base_dir<span class="token punctuation">)</span>    <span class="token keyword">return</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>base_dir<span class="token punctuation">,</span> folder<span class="token punctuation">)</span> <span class="token keyword">if</span> folder <span class="token keyword">else</span> data_dir<span class="token keyword">def</span> <span class="token function">download_all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""下载DATA_HUB中的所有文件"""</span>    <span class="token keyword">for</span> name <span class="token keyword">in</span> DATA_HUB<span class="token punctuation">:</span>        download<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_net</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> net<span class="token comment"># 采用相对误差而不是绝对误差来衡量误差</span><span class="token keyword">def</span> <span class="token function">log_rmse</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 将预测值中小于1的部分设置为1，以避免在取对数时出现负无穷大的情况</span>    clipped_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>clipped_preds<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> rmse<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span>          num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    train_ls<span class="token punctuation">,</span> test_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    train_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    <span class="token comment"># 这里使用的是Adam优化算法</span>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                 lr<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span>                                 weight_decay<span class="token operator">=</span>weight_decay<span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> test_labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> train_ls<span class="token punctuation">,</span> test_ls<span class="token comment"># 用K折交叉验证来评估模型</span><span class="token keyword">def</span> <span class="token function">get_k_fold_data</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> i<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> k <span class="token operator">></span> <span class="token number">1</span>    fold_size <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> k    X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>        idx <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>j <span class="token operator">*</span> fold_size<span class="token punctuation">,</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> fold_size<span class="token punctuation">)</span>        X_part<span class="token punctuation">,</span> y_part <span class="token operator">=</span> X<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        <span class="token keyword">if</span> j <span class="token operator">==</span> i<span class="token punctuation">:</span>            X_valid<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> X_part<span class="token punctuation">,</span> y_part        <span class="token keyword">elif</span> X_train <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            X_train<span class="token punctuation">,</span> y_train <span class="token operator">=</span> X_part<span class="token punctuation">,</span> y_part        <span class="token keyword">else</span><span class="token punctuation">:</span>            X_train <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>X_train<span class="token punctuation">,</span> X_part<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>            y_train <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>y_train<span class="token punctuation">,</span> y_part<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> X_valid<span class="token punctuation">,</span> y_valid<span class="token keyword">def</span> <span class="token function">k_fold</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    train_l_sum<span class="token punctuation">,</span> valid_l_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> get_k_fold_data<span class="token punctuation">(</span>k<span class="token punctuation">,</span> i<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>        net <span class="token operator">=</span> get_net<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_ls<span class="token punctuation">,</span> valid_ls <span class="token operator">=</span> train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token operator">*</span>data<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span>                                   weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>        train_l_sum <span class="token operator">+=</span> train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        valid_l_sum <span class="token operator">+=</span> valid_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            d2l<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>train_ls<span class="token punctuation">,</span> valid_ls<span class="token punctuation">]</span><span class="token punctuation">,</span>                     xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'rmse'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>                     legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'valid'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'折</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">，训练log rmse</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">, '</span></span>              <span class="token string-interpolation"><span class="token string">f'验证log rmse</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>valid_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token keyword">return</span> train_l_sum <span class="token operator">/</span> k<span class="token punctuation">,</span> valid_l_sum <span class="token operator">/</span> k<span class="token keyword">def</span> <span class="token function">train_and_pred</span><span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_data<span class="token punctuation">,</span>                   num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> get_net<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_ls<span class="token punctuation">,</span> _ <span class="token operator">=</span> train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span>                        num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>train_ls<span class="token punctuation">]</span><span class="token punctuation">,</span> xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span>             ylabel<span class="token operator">=</span><span class="token string">'log rmse'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'训练log rmse：</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_ls<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token comment"># 将网络应用于测试集。</span>    preds <span class="token operator">=</span> net<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 将其重新格式化以导出到Kaggle</span>    test_data<span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span> <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>preds<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    submission <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>test_data<span class="token punctuation">[</span><span class="token string">'Id'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">[</span><span class="token string">'SalePrice'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    submission<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">'submission.csv'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    DATA_HUB <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    DATA_URL <span class="token operator">=</span> <span class="token string">'http://d2l-data.s3-accelerate.amazonaws.com/'</span>    DATA_HUB<span class="token punctuation">[</span><span class="token string">'kaggle_house_train'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>        DATA_URL <span class="token operator">+</span> <span class="token string">'kaggle_house_pred_train.csv'</span><span class="token punctuation">,</span> <span class="token string">'585e9cc93e70b39160e7921475f9bcd7d31219ce'</span><span class="token punctuation">)</span>    DATA_HUB<span class="token punctuation">[</span><span class="token string">'kaggle_house_test'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>        DATA_URL <span class="token operator">+</span> <span class="token string">'kaggle_house_pred_test.csv'</span><span class="token punctuation">,</span> <span class="token string">'fa19780a7b011d9b009e8bff8e99922a8ee2eb90'</span><span class="token punctuation">)</span>    <span class="token comment">#  划分训练集以创建验证集</span>    train_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>download<span class="token punctuation">(</span><span class="token string">'kaggle_house_train'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    test_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>download<span class="token punctuation">(</span><span class="token string">'kaggle_house_test'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 删除ID列以及标签列</span>    all_features <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 数据预处理</span>    <span class="token comment"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span>    numeric_features <span class="token operator">=</span> all_features<span class="token punctuation">.</span>dtypes<span class="token punctuation">[</span>all_features<span class="token punctuation">.</span>dtypes <span class="token operator">!=</span>                                           <span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index    all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span> <span class="token operator">=</span> all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>        <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 标准化数据有两个原因： 方便优化；不知道哪些特征是相关的，避免让惩罚分配给一个特征的系数比分配给其他任何特征的系数更大。</span>    <span class="token comment"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span>    all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span> <span class="token operator">=</span> all_features<span class="token punctuation">[</span>numeric_features<span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># 将离散数值转换为独热编码(get_dummies函数将分类变量转换为虚拟变量)</span>    <span class="token comment"># “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征</span>    all_features <span class="token operator">=</span> pd<span class="token punctuation">.</span>get_dummies<span class="token punctuation">(</span>all_features<span class="token punctuation">,</span> dummy_na<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># 通过values属性得到NumPy格式的数据，并转换成张量</span>    n_train <span class="token operator">=</span> train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    train_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>        all_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    test_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>        all_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    train_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>        train_data<span class="token punctuation">.</span>SalePrice<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token comment"># 训练模型</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    in_features <span class="token operator">=</span> train_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    k<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">64</span>    <span class="token comment"># 将训练集划分为K份，然后使用第K份作为验证集，其余作为训练集</span>    train_l<span class="token punctuation">,</span> valid_l <span class="token operator">=</span> k_fold<span class="token punctuation">(</span>        k<span class="token punctuation">,</span> train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>k<span class="token punctuation">&#125;</span></span><span class="token string">-折验证: 平均训练log rmse: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_l<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">, '</span></span><span class="token string-interpolation"><span class="token string">f'平均验证log rmse: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>valid_l<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token comment"># 使用所有数据对其进行训练，然后预测并在Kaggle提交结果</span>    train_and_pred<span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span>                   test_data<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> weight_decay<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span></code></pre><h3 id="附录">附录</h3><h4 id="附录a激活函数绘图代码">附录A：激活函数绘图代码</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib <span class="token keyword">as</span> mpl<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># 设置中文字体</span>mpl<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'SimHei'</span><span class="token punctuation">]</span>mpl<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token comment"># 定义激活函数</span><span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tanh</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">prelu</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> alpha <span class="token operator">*</span> np<span class="token punctuation">.</span>minimum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token comment"># 定义导数</span><span class="token keyword">def</span> <span class="token function">relu_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sigmoid_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    s <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> s <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> s<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tanh_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token keyword">def</span> <span class="token function">prelu_derivative</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>x <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> alpha<span class="token punctuation">)</span><span class="token comment"># 创建数据点</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token comment"># 创建图形和子图 (2行4列，共8个子图)</span>fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 设置图表标题</span>fig<span class="token punctuation">.</span>suptitle<span class="token punctuation">(</span><span class="token string">'激活函数及其导数对比'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token comment"># 第一行：激活函数</span><span class="token comment"># ReLU 激活函数</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'ReLU 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'ReLU(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># Sigmoid 激活函数</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'g-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Sigmoid 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Sigmoid(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># Tanh 激活函数</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'b-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Tanh 激活函数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Tanh(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># pReLU 激活函数</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prelu<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'m-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'pReLU 激活函数 (α=0.2)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'pReLU(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># 第二行：导数</span><span class="token comment"># ReLU 导数</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> relu_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'ReLU 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'ReLU\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># Sigmoid 导数</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sigmoid_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'g-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Sigmoid 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Sigmoid\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># Tanh 导数</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tanh_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'b-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Tanh 导数'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Tanh\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># pReLU 导数</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prelu_derivative<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'m-'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'pReLU 导数 (α=0.2)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'pReLU\'(x)'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axhline<span class="token punctuation">(</span>y<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token comment"># 调整布局</span>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 保存图像</span>save_path <span class="token operator">=</span> <span class="token string">'./source/_posts/DL/activation_functions.svg'</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span>save_path<span class="token punctuation">,</span> <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">'svg'</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> bbox_inches<span class="token operator">=</span><span class="token string">'tight'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"图像已保存至: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>save_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token comment"># 显示图像</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4id="附录b神经网络反向传播通俗解释">附录B：神经网络反向传播通俗解释</h4><p>神经网络使用反向传播算法进行训练，核心是链式法则（链式求导），训练的目标是最小化损失函数。我们要解决的就是怎么调整模型的参数<spanclass="math inline">\(\omega\)</span>才能让损失函数最小。</p><p>这就类似于<spanclass="math inline">\(y=f(x)\)</span>的函数，我们想要找到一个最优的<spanclass="math inline">\(x\)</span>使得<spanclass="math inline">\(y\)</span>最小。这里的<spanclass="math inline">\(x\)</span>就是神经网络的参数<spanclass="math inline">\(\omega\)</span>，而<spanclass="math inline">\(y\)</span>就是损失函数<spanclass="math inline">\(L(\omega)\)</span>。自然而然的引出了导数的概念，也就是梯度<spanclass="math inline">\(\frac{\partial L}{\partial\omega}\)</span>，它告诉我们在当前参数位置，损失函数的变化率。</p><p>由此引出以下几个问题：</p><p><strong>1. 导数和梯度的关系是什么？</strong></p><p>情况一：一元函数（只有一个输入）<span class="math inline">\(f(w) =w^2\)</span></p><ul><li>它的导数是：<span class="math inline">\(\frac{df}{dw} =2w\)</span></li><li>它的梯度也是：<span class="math inline">\(\nabla f(w) =(2w)\)</span></li></ul><p>在这种情况下，<strong>梯度就等于导数本身</strong>，只是写成了一个一维向量。</p><p>情况二：多元函数（多个输入）<span class="math inline">\(f(w_1, w_2) =w_1^2 + w_2^2\)</span></p><p><strong>没法用「导数」描述它的全部方向的变化</strong>。要用梯度：<spanclass="math inline">\(\nabla f = \left( \frac{\partial f}{\partial w_1},\frac{\partial f}{\partial w_2} \right) = (2w_1, 2w_2)\)</span></p><table><colgroup><col style="width: 46%" /><col style="width: 12%" /><col style="width: 40%" /></colgroup><thead><tr><th>情况</th><th>使用的概念</th><th>是否等价</th></tr></thead><tbody><tr><td>一维函数 <span class="math inline">\(f(w)\)</span></td><td>导数 &amp; 梯度</td><td>✅ 等价（梯度是 1 维向量）</td></tr><tr><td>多维函数 <span class="math inline">\(f(w_1, w_2,\dots)\)</span></td><td>梯度（向量）</td><td>❌ 不等价，导数只看一个方向，梯度是全部方向</td></tr></tbody></table><p><strong>2. 为什么不直接让梯度为0就可以了？</strong></p><table><colgroup><col style="width: 31%" /><col style="width: 68%" /></colgroup><thead><tr><th>问题</th><th>回答</th></tr></thead><tbody><tr><td>为什么不直接让梯度为 0？</td><td>因为求不出来（无解析解），而且梯度为 0 不一定是最小值</td></tr><tr><td>那我们怎么做？</td><td>使用梯度下降，沿着负梯度方向一点点走，逐步逼近最小值</td></tr><tr><td>好处？</td><td>不需要显式解方程，适合高维、大规模、非线性优化问题</td></tr></tbody></table><p><strong>3. 有了梯度之后怎么更新参数？为什么这么更新？</strong></p><p>有了梯度之后，<strong>我们用梯度的反方向来更新参数</strong>，因为梯度指出了损失函数增加最快的方向，所以反方向是<strong>下降最快的方向</strong>。</p><p>参数更新的公式（梯度下降法）。设：</p><ul><li><spanclass="math inline">\(\mathbf{w}\)</span>：模型参数（可以是权重、偏置等）</li><li><spanclass="math inline">\(\mathcal{L}(\mathbf{w})\)</span>：损失函数</li><li><span class="math inline">\(\nabla\mathcal{L}(\mathbf{w})\)</span>：当前参数处的梯度</li><li><span class="math inline">\(\eta\)</span>：学习率（learningrate，控制步子大小）</li></ul><p>那么参数更新公式为：</p><p><span class="math display">\[\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \cdot \nabla\mathcal{L}(\mathbf{w}_{\text{old}})\tag{B.1}\]</span></p><p>为什么是这个公式？</p><blockquote><p>1️⃣ 梯度告诉我们什么？</p></blockquote><p>梯度 <span class="math inline">\(\nabla\mathcal{L}(\mathbf{w})\)</span> 是一个向量，表示在每一个维度上：</p><ul><li>增加参数 → 损失函数会上升多少</li></ul><p>所以梯度的方向，就是“损失函数上升最快的方向”</p><blockquote><p>2️⃣ 那我们想做什么？</p></blockquote><p>我们想让损失函数 <strong>变小</strong>，而不是变大！</p><p>因此，我们要朝<strong>损失下降最快的方向</strong>移动，那就是梯度的<strong>负方向</strong>：<spanclass="math inline">\(-\nabla \mathcal{L}(\mathbf{w})\)</span></p><blockquote><p>3️⃣ 为什么还要乘一个 <span class="math inline">\(\eta\)</span>学习率？</p></blockquote><p>梯度只是一个方向，我们还需要控制“走多远”：</p><ul><li>步子太大 → 可能错过最小值，甚至震荡或发散</li><li>步子太小 → 学得太慢，收敛极慢</li></ul><p>所以我们加一个系数 <span class="math inline">\(\eta\)</span>，就是<strong>学习率</strong>，来控制更新的幅度：<spanclass="math inline">\(\text{更新量} = -\eta \cdot \nabla\mathcal{L}(\mathbf{w})\)</span></p><table><colgroup><col style="width: 8%" /><col style="width: 91%" /></colgroup><thead><tr><th>步骤</th><th>说明</th></tr></thead><tbody><tr><td>1. 计算梯度</td><td>得到损失函数对参数的导数（每个方向的变化率）</td></tr><tr><td>2. 取反方向</td><td>因为梯度是上升方向，下降要走反方向</td></tr><tr><td>3. 乘以学习率</td><td>控制更新的幅度，避免步子太大或太小</td></tr><tr><td>4. 更新参数</td><td><span class="math inline">\(\mathbf{w}_{\text{new}} =\mathbf{w}_{\text{old}} - \eta \cdot \nabla \mathcal{L}\)</span></td></tr></tbody></table><h4id="附录c为什么关注激活函数的导数">附录C：为什么关注激活函数的导数？</h4><blockquote><p><strong>举例：ReLU 的导数</strong></p></blockquote><ul><li><p>ReLU(x) = max(0, x)</p></li><li><p>它的导数为：</p><p><span class="math display">\[\text{ReLU}&#39;(x) = \begin{cases}1, &amp; x &gt; 0 \\0, &amp; x \leq 0\end{cases}\tag{C.1}\]</span></p></li></ul><p>这意味着只有当输入为正时，ReLU 才会让梯度流动——否则梯度为0，神经元不再学习。</p><blockquote><p><strong>了解梯度消失或爆炸的问题</strong></p></blockquote><p>某些激活函数的导数可能导致梯度消失（如 Sigmoid）或爆炸（如 Softplus的大输入）。分析导数可以帮助理解和选择<strong>更合适的激活函数</strong>，避免这些问题。</p><p><strong>例子：Sigmoid 的导数</strong></p><ul><li>最大值是 0.25，输入过大或过小时导数接近 0，导致梯度消失。</li></ul><blockquote><p><strong>设计新激活函数</strong></p></blockquote><p>很多改进型激活函数（如 LeakyReLU、ELU、Swish、Mish）都是基于对导数行为的深入理解后提出的。例如：</p><ul><li><p><strong>Leaky ReLU</strong>：解决 ReLU的“神经元死亡”问题（即一旦输出为0，就无法恢复），通过在负区间给一个小斜率（导数非零）：</p><p><span class="math display">\[\text{Leaky ReLU}(x) = \begin{cases}x, &amp; x &gt; 0 \\0.01x, &amp; x \leq 0\end{cases}\tag{C.2}\]</span></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 3 线性神经网络</title>
      <link href="/2024/06/25/DL/3%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/06/25/DL/3%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="线性神经网络">3 线性神经网络</h2><h3 id="线性回归">3.1 线性回归</h3><p>回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。</p><h4 id="随机梯度下降">3.1.1 随机梯度下降</h4><p><strong>梯度下降通过不断地在损失函数递减的方向上更新参数来降低误差。</strong></p><p><span class="math display">\[w \leftarrow w - \eta \frac{\partial(J)}{\partial(w)}\tag{3.1}\]</span></p><p>梯度下降最简单的用法是计算<strong>损失函数</strong>（数据集中所有样本的损失均值）<strong>关于模型参数的导数</strong>（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降。</p><p>在每次迭代中，我们首先随机抽样一个小批量<spanclass="math inline">\(B\)</span>， 它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数<spanclass="math inline">\(\eta\)</span>，并从当前参数的值中减掉。</p><p>更具体的 <spanclass="math inline">\(B\)</span>表示小批量中的样本数，也被称为批量大小（batchsize） <span class="math inline">\(\eta\)</span>称为学习率（learningrate），是一个正数，用来调节每次更新的幅度。 当<spanclass="math inline">\(\eta\)</span>较小时，更新的幅度较小。 当<spanclass="math inline">\(\eta\)</span>较大时，更新的幅度较大。 当<spanclass="math inline">\(\eta\)</span>过大时，可能会使模型在优化过程中发散，甚至无法收敛到最优解。当<spanclass="math inline">\(\eta\)</span>过小时，模型优化的速度会过慢。</p><p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。</p><h4 id="矢量化加速">3.1.2 矢量化加速</h4><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。为了实现这一点，我们需要利用线性代数库，而不是在Python中编写开销较大的for循环。</p><p>例如 <code>for i in range(n): c[i] = a[i] + b[i]</code> 可以被替换为<code>c = a + b</code>。</p><h4 id="正态分布与平方损失">3.1.3 正态分布与平方损失</h4><p>根据极大似然估计法，参数<span class="math inline">\(w\)</span>和<spanclass="math inline">\(b\)</span>的最优值是使整个数据集的似然最大的值。由于平方损失函数，这等价于最小化平方损失。</p><h4 id="从线性回归到深度网络">3.1.4 从线性回归到深度网络</h4><p>输入层的特征维度为<spanclass="math inline">\(d\)</span>，输出层的输出维度为1，所表示的神经网络可以被认为是一个单层神经网络(通常我们计算层数时不考虑输入层)，如图3.1所示。</p><figure><img src="./images/3.1_linear-regression-single-layer-network.svg"title="图3.1：线性回归是一个单层神经网络"alt="线性回归是一个单层神经网络" /><figcaption aria-hidden="true">线性回归是一个单层神经网络</figcaption></figure><h3 id="线性回归的从零开始实现">3.2 线性回归的从零开始实现</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 3.2.1 生成数据集</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> random<span class="token keyword">def</span> <span class="token function">synthetic_data</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> num_examples<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""生成y=Xw+b+噪声"""</span>    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 生成一个1000行2列的矩阵</span>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b <span class="token comment"># y = Xw+b</span>    y <span class="token operator">+=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># 添加噪声，标准差为0.01的正态分布</span>    <span class="token keyword">return</span> X<span class="token punctuation">,</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># y变为列向量</span>true_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>true_b <span class="token operator">=</span> <span class="token number">4.2</span><span class="token comment"># 生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征</span>features<span class="token punctuation">,</span> labels <span class="token operator">=</span> synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token comment"># 3.2.2 读取数据</span><span class="token comment"># 训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。</span><span class="token keyword">def</span> <span class="token function">data_iter</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    num_examples <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>features<span class="token punctuation">)</span>    indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">)</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>indices<span class="token punctuation">)</span> <span class="token comment"># 打乱数据集中的样本并以小批量方式获取数据</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_examples<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>indices<span class="token punctuation">[</span>i<span class="token punctuation">:</span> <span class="token builtin">min</span><span class="token punctuation">(</span>i <span class="token operator">+</span> batch_size<span class="token punctuation">,</span> num_examples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># yield关键字来构造一个生成器，逐批返回数据和标签，用于高效地处理大数据集或在训练机器学习模型时进行小批量训练</span>        <span class="token keyword">yield</span> features<span class="token punctuation">[</span>batch_indices<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>batch_indices<span class="token punctuation">]</span>batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token comment"># 生成第一个小批量数据样本</span><span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token string">'\n'</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token keyword">break</span><span class="token comment"># 3.2.3 初始化模型参数</span><span class="token comment"># 使用随机数来初始化权重，偏置则初始化为0，同时启用张量的自动微分功能</span>w <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 3.2.4 定义模型</span><span class="token keyword">def</span> <span class="token function">linreg</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""线性回归模型"""</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token comment"># 3.2.5 定义损失函数</span><span class="token comment"># 因为需要计算损失函数的梯度，所以我们应该先定义损失函数</span><span class="token keyword">def</span> <span class="token function">squared_loss</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""均方损失"""</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>y_hat <span class="token operator">-</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">/</span> <span class="token number">2</span> <span class="token comment"># 需要将真实值y的形状转换为和预测值y_hat的形状相同</span><span class="token comment"># 3.2.6 定义优化算法(小批量随机梯度下降更新)</span><span class="token comment"># 在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。</span><span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 模型参数，学习速率，批量大小</span>    <span class="token triple-quoted-string string">"""小批量随机梯度下降"""</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 开启一个上下文，在该上下文中禁用梯度计算。参数更新和反向传播是不同的步骤</span>        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>            <span class="token comment"># 计算的损失是一个批量样本的总和，所以用批量大小（batch_size） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</span>            param <span class="token operator">-=</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size <span class="token comment"># 每一步更新的大小由学习速率lr决定。 </span>            param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零</span><span class="token comment"># 3.2.7 训练</span>lr <span class="token operator">=</span> <span class="token number">0.03</span> <span class="token comment"># 学习率</span>num_epochs <span class="token operator">=</span> <span class="token number">3</span> <span class="token comment"># 训练轮次</span>net <span class="token operator">=</span> linreg <span class="token comment"># 自定义模型</span>loss <span class="token operator">=</span> squared_loss <span class="token comment"># 自定义损失函数</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>  <span class="token comment"># X和y的小批量损失</span>        <span class="token comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span>        <span class="token comment"># 并以此计算关于[w,b]的梯度</span>        l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播</span>        sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>  <span class="token comment"># 使用参数的梯度更新参数</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 模型评估不需要反向传播</span>        train_l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token comment"># 打印当前模型在整个训练集上的损失</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token comment"># 3.2.8 对比真实值</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'w的估计误差: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>true_w <span class="token operator">-</span> w<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>true_w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'b的估计误差: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>true_b <span class="token operator">-</span> b<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span></code></pre><h3 id="线性回归的简洁实现">3.3 线性回归的简洁实现</h3><p>使用深度学习框架的高级API简洁实现线性回归</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 3.3.1 生成数据集</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2ltrue_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>true_b <span class="token operator">=</span> <span class="token number">4.2</span>features<span class="token punctuation">,</span> labels <span class="token operator">=</span> d2l<span class="token punctuation">.</span>synthetic_data<span class="token punctuation">(</span>true_w<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token comment"># 3.3.2 读取数据集</span><span class="token comment"># 调用data函数，将features和labels作为参数传入，得到一个data.TensorDataset实例</span><span class="token keyword">def</span> <span class="token function">load_array</span><span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># *data_arrays表示传入的是一个list，*表示解包</span>    dataset <span class="token operator">=</span> data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span><span class="token operator">*</span>data_arrays<span class="token punctuation">)</span>    <span class="token comment"># shuffle表示是否打乱数据</span>    <span class="token keyword">return</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span>is_train<span class="token punctuation">)</span>batch_size <span class="token operator">=</span> <span class="token number">10</span>data_iter <span class="token operator">=</span> load_array<span class="token punctuation">(</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token comment"># 读取并打印第一个小批量样本</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 使用iter函数生成迭代器，使用next函数得到第一个元素</span><span class="token comment"># 3.3.3 定义模型</span><span class="token comment"># Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。</span><span class="token comment"># 全连接层在Linear类中定义。 该层的输出数量为1。</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 直接指定输入和输出尺寸。2表示输入特征数，1表示输出特征数</span><span class="token comment"># 3.3.4 初始化模型参数</span><span class="token comment"># 通过net[0]访问Sequential实例中的第一层，然后使用weight.data和bias.data方法访问参数。</span><span class="token comment"># 使用替换方法normal_和fill_来重写参数值</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>  <span class="token comment"># 均值为0，标准差为0.01的正态分布</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 使用fill_方法将偏置参数设置为0</span><span class="token comment"># 3.3.5 定义损失函数</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 均方误差损失函数</span><span class="token comment"># 3.3.6 定义优化算法</span><span class="token comment"># 指定优化的参数 （可通过net.parameters()从模型中获得）以及优化算法所需的超参数字典</span>trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">)</span>  <span class="token comment"># 使用SGD优化算法，学习率为0.03</span><span class="token comment"># 3.3.7 训练</span>num_epochs <span class="token operator">=</span> <span class="token number">3</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>        trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>l<span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token comment"># 比较学到的模型参数和真实l的模型参数</span>w <span class="token operator">=</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'w的估计误差:'</span><span class="token punctuation">,</span> true_w <span class="token operator">-</span> w<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>true_w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b的估计误差:'</span><span class="token punctuation">,</span> true_b <span class="token operator">-</span> b<span class="token punctuation">)</span></code></pre><h3 id="softmax回归">3.4 softmax回归</h3><p>softmax运算获取一个向量并将其映射为概率，适用于分类问题，它使用了softmax运算中输出类别的概率分布。</p><h4 id="分类问题">3.4.1 分类问题</h4><p>从一个图像分类问题开始。 假设每次输入是一个 2X2 的灰度图像。我们可以用一个标量表示每个像素值，每个图像对应四个特征<spanclass="math inline">\(x_1,x_2,x_3,x_4\)</span>。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。(独热编码:独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。)</p><h4 id="网络架构">3.4.2 网络架构</h4><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affinefunction）。</p><p><span class="math display">\[\begin{gathered}o_1 &amp;= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\o_2 &amp;= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\o_3 &amp;= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\end{gathered}\tag{3.2}\]</span></p><p>表达为向量形式<span class="math inline">\(\mathbf{o} = \mathbf{W}\mathbf{x} + \mathbf{b}\)</span></p><p>因此可以构建如下的单层神经网络(同样的，也是全连接层)：</p><figure><img src="./images/3.2_softmax-regression-single-layer-network.svg"title="图3.2：softmax回归是一个单层神经网络"alt="softmax回归是一个单层神经网络" /><figcaptionaria-hidden="true">softmax回归是一个单层神经网络</figcaption></figure><h4 id="全连接层的参数开销">3.4.3 全连接层的参数开销</h4><p>全连接层是“完全”连接的，可能有很多可学习的参数。具体来说，对于任何具有d个输入和p个输出的全连接层， 参数开销为<spanclass="math inline">\(\mathcal{O}(dq)\)</span>，这个数字在实践中可能高得令人望而却步。幸运的是，将d个输入转换为q个输出的成本可以减少到<spanclass="math inline">\(\mathcal{O}(\frac{dq}{n})\)</span>。其中，其中超参数n可以由我们灵活指定</p><h4 id="softmax运算">3.4.4. softmax运算</h4><p>现在我们将优化参数以最大化观测数据的概率。为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。</p><p>然而我们能否将未规范化的预测<spanclass="math inline">\(o\)</span>直接视作我们感兴趣的输出呢？答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题：一方面，我们没有限制这些输出数字的总和为1。另一方面，根据输入的不同，它们可以为负值。要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。</p><p>由此引出<strong>softmax函数</strong>:</p><p><span class="math display">\[\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad\hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}\tag{3.3}\]</span></p><p><spanclass="math inline">\(\hat{\mathbf{y}}\)</span>可以视为一个正确的概率分布。softmax运算不会改变未规范化的预测<spanclass="math inline">\(o\)</span>之间的大小次序，从而依旧是:</p><p><span class="math display">\[\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.\tag{3.4}\]</span></p><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，<strong>softmax回归是一个线性模型</strong>（linear model）。</p><h4 id="小批量样本的矢量化">3.4.5. 小批量样本的矢量化</h4><p>为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。假设我们读取了一个批量的样本<spanclass="math inline">\(X\)</span>， 其中特征维度（输入数量）为<spanclass="math inline">\(d\)</span>，批量大小为<spanclass="math inline">\(n\)</span>。 此外，假设我们在输出中有<spanclass="math inline">\(q\)</span>个类别。 那么小批量样本的特征为<spanclass="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span>，权重为<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \timesq}\)</span>， 偏置为<span class="math inline">\(\mathbf{b} \in\mathbb{R}^{1\times q}\)</span>。 softmax回归的矢量计算表达式为：</p><p><span class="math display">\[\begin{gathered}\mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\\hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}).\end{gathered}\tag{3.5}\]</span></p><p>相对于一次处理一个样本， 小批量样本的矢量化加快了<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(W\)</span>的矩阵-向量乘法。</p><h4 id="损失函数">3.4.6 损失函数</h4><p>同线性回归一样，使用最大似然估计法。</p><p><strong>交叉熵损失</strong>(是分类问题最常用的损失之一，是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数):</p><p><span class="math display">\[l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.\tag{3.6}\]</span></p><h3 id="图像分类数据集">3.5 图像分类数据集</h3><p>MNIST数据集是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。我们将使用类似但更复杂的Fashion-MNIST数据集。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2ld2l<span class="token punctuation">.</span>use_svg_display<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 3.5.1 读取数据集</span><span class="token comment"># 3.5.1.1 通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中</span>trans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，并除以255使得所有像素的数值均在0～1之间</span>mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># Fashion-MNIST由10个类别的图像组成， 每个类别由训练数据集（train dataset）中的6000张图像 和测试数据集（test dataset）中的1000张图像组成。 </span><span class="token comment"># 因此，训练集和测试集分别包含60000和10000张图像。 测试数据集不会用于训练，只用于评估模型性能。</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>mnist_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>mnist_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 每个输入图像的高度和宽度均为28像素。 数据集由灰度图像组成，其通道数为1</span><span class="token keyword">print</span><span class="token punctuation">(</span>mnist_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([1, 28, 28])</span><span class="token comment"># 3.5.1.2 在数字标签索引及其文本名称之间进行转换</span><span class="token keyword">def</span> <span class="token function">get_fashion_mnist_labels</span><span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""返回Fashion-MNIST数据集的文本标签"""</span>    text_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'t-shirt'</span><span class="token punctuation">,</span> <span class="token string">'trouser'</span><span class="token punctuation">,</span> <span class="token string">'pullover'</span><span class="token punctuation">,</span> <span class="token string">'dress'</span><span class="token punctuation">,</span> <span class="token string">'coat'</span><span class="token punctuation">,</span>                   <span class="token string">'sandal'</span><span class="token punctuation">,</span> <span class="token string">'shirt'</span><span class="token punctuation">,</span> <span class="token string">'sneaker'</span><span class="token punctuation">,</span> <span class="token string">'bag'</span><span class="token punctuation">,</span> <span class="token string">'ankle boot'</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>text_labels<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> labels<span class="token punctuation">]</span><span class="token comment"># 3.5.1.3 可视化样本</span><span class="token keyword">def</span> <span class="token function">show_images</span><span class="token punctuation">(</span>imgs<span class="token punctuation">,</span> num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> titles<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""绘制图像列表"""</span>    figsize <span class="token operator">=</span> <span class="token punctuation">(</span>num_cols <span class="token operator">*</span> scale<span class="token punctuation">,</span> num_rows <span class="token operator">*</span> scale<span class="token punctuation">)</span>    _<span class="token punctuation">,</span> axes <span class="token operator">=</span> d2l<span class="token punctuation">.</span>plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> figsize<span class="token operator">=</span>figsize<span class="token punctuation">)</span>    axes <span class="token operator">=</span> axes<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax<span class="token punctuation">,</span> img<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> imgs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>is_tensor<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 图片张量</span>            ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># PIL图片</span>            ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>axes<span class="token punctuation">.</span>get_xaxis<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>set_visible<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>axes<span class="token punctuation">.</span>get_yaxis<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>set_visible<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> titles<span class="token punctuation">:</span>            ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>titles<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> axes<span class="token comment"># 展示前几个样本</span>X<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>show_images<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> titles<span class="token operator">=</span>get_fashion_mnist_labels<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment"># 3.5.2 小批量读取</span><span class="token comment"># 使用内置的数据迭代器，每次都会读取一小批量数据，大小为batch_size。同时随机打乱所有样本，从而无偏见地读取小批量</span>batch_size <span class="token operator">=</span> <span class="token number">256</span><span class="token keyword">def</span> <span class="token function">get_dataloader_workers</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""使用4个进程来读取数据"""</span>    <span class="token keyword">return</span> <span class="token number">4</span><span class="token comment"># 3.5.3 整合所有组件</span><span class="token comment"># 获取和读取Fashion-MNIST数据集，返回训练集和验证集的数据迭代器。</span><span class="token keyword">def</span> <span class="token function">load_data_fashion_mnist</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""下载Fashion-MNIST数据集，然后将其加载到内存中"""</span>    trans <span class="token operator">=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> resize<span class="token punctuation">:</span> <span class="token comment"># 可选参数resize，用来将图像大小调整为另一种形状</span>        trans<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span>resize<span class="token punctuation">)</span><span class="token punctuation">)</span>    trans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>trans<span class="token punctuation">)</span>    mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>        root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>        root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>get_dataloader_workers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_test<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>get_dataloader_workers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 得到训练集和测试集的数据迭代器(测试load_data_fashion_mnist函数的图像大小调整功能)</span>train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> load_data_fashion_mnist<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>    <span class="token comment"># torch.Size([32, 1, 64, 64])   torch.float32   torch.Size([32])    torch.int64 </span>    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> X<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> y<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>    <span class="token keyword">break</span></code></pre><h3 id="softmax回归的从零开始实现">3.6 softmax回归的从零开始实现</h3><p>既然我们已经引入了Fashion-MNIST数据集，让我们使用softmax回归从零开始实现它。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> IPython <span class="token keyword">import</span> display<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token comment"># 3.6.1 初始化模型参数(放入main函数中)</span><span class="token comment"># 原始输入图像大小是28*28，这里将其转换为长度为28*28=784的向量</span><span class="token comment"># 由于有10个类别，所以输出层的输出个数为10</span><span class="token comment"># 因此，权重将构成一个784*10的矩阵</span><span class="token comment"># 偏置将构成一个10维的向量</span><span class="token comment"># 3.6.2 实现softmax运算</span><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 1. 对每个项求幂（使用exp）；</span>    X_exp <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token comment"># 2. 对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数；</span>    partition <span class="token operator">=</span> X_exp<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 调用sum时，可以指定保持在原始张量的轴数，而不折叠求和的维度。其中1表示沿行求和，keepdim=True保持列向量的形状</span>    <span class="token comment"># 3. 将每一行除以其规范化常数，确保结果的和为1。</span>    <span class="token keyword">return</span> X_exp <span class="token operator">/</span> partition  <span class="token comment"># 这里应用了广播机制</span><span class="token comment"># 3.6.3 定义模型</span><span class="token comment"># 输入通过网络映射到输出</span><span class="token keyword">def</span> <span class="token function">net</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 通过reshape函数将每张原始图像改为长度为num_inputs的向量，即reshape((-1, 784))。</span>    <span class="token keyword">return</span> softmax<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> W<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token comment"># 3.6.4 定义损失函数</span><span class="token comment"># 交叉熵损失函数</span><span class="token keyword">def</span> <span class="token function">cross_entropy</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># y_hat是预测概率，y是标签</span>    <span class="token keyword">return</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y_hat<span class="token punctuation">[</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 3.6.5 计算分类准确率</span><span class="token comment"># 计算准确率</span><span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""计算预测正确的数量"""</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span> <span class="token keyword">and</span> y_hat<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>        y_hat <span class="token operator">=</span> y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 返回沿轴axis最大值的索引</span>    <span class="token builtin">cmp</span> <span class="token operator">=</span> y_hat<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">==</span> y <span class="token comment"># 匹配数据类型并比较，返回布尔值0或1</span>    <span class="token keyword">return</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">cmp</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 评估模型net的精度</span><span class="token keyword">def</span> <span class="token function">evaluate_accuracy</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""计算在指定数据集上模型的精度"""</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 如果是torch.nn.Module实例，使用isinstance函数返回True</span>        net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将模型设置为评估模式</span>    metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 正确预测数、预测总数(自定义的累加器类)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 不计算梯度</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>            metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>accuracy<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 累加每个batch_size个样本的准确率和样本数</span>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment"># 3.6.6 训练模型</span><span class="token comment"># 定义一个函数来训练一个迭代周期</span><span class="token keyword">def</span> <span class="token function">train_epoch_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""训练模型一个迭代周期"""</span>    <span class="token comment"># 将模型设置为训练模式</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 训练损失总和、训练准确度总和、样本数</span>    metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>        <span class="token comment"># 计算梯度并更新参数</span>        y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>updater<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 使用PyTorch内置的优化器和损失函数</span>            updater<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            updater<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># 使用定制的优化器和损失函数</span>            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            updater<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 返回训练损失和训练精度</span>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token comment"># 训练函数</span><span class="token keyword">def</span> <span class="token function">train_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> updater<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""训练模型"""</span>    animator <span class="token operator">=</span> Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                        legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train loss'</span><span class="token punctuation">,</span> <span class="token string">'train acc'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        train_metrics <span class="token operator">=</span> train_epoch_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">)</span>        test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_metrics <span class="token operator">+</span> <span class="token punctuation">(</span>test_acc<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train_metrics    <span class="token keyword">assert</span> train_loss <span class="token operator">&lt;</span> <span class="token number">0.5</span><span class="token punctuation">,</span> train_loss    <span class="token keyword">assert</span> train_acc <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">and</span> train_acc <span class="token operator">></span> <span class="token number">0.7</span><span class="token punctuation">,</span> train_acc    <span class="token keyword">assert</span> test_acc <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">and</span> test_acc <span class="token operator">></span> <span class="token number">0.7</span><span class="token punctuation">,</span> test_acc<span class="token comment"># 小批量随机梯度下降来优化模型的损失函数</span><span class="token keyword">def</span> <span class="token function">updater</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>W<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token comment"># 3.6.7 预测</span><span class="token keyword">def</span> <span class="token function">predict_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""预测标签"""</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> test_iter<span class="token punctuation">:</span> <span class="token comment"># 取一个batch_size的数据</span>        <span class="token keyword">break</span>    trues <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token comment"># 真实标签</span>    preds <span class="token operator">=</span> d2l<span class="token punctuation">.</span>get_fashion_mnist_labels<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 预测标签</span>    titles <span class="token operator">=</span> <span class="token punctuation">[</span>true <span class="token operator">+</span><span class="token string">'\n'</span> <span class="token operator">+</span> pred <span class="token keyword">for</span> true<span class="token punctuation">,</span> pred <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>trues<span class="token punctuation">,</span> preds<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 标题</span>    d2l<span class="token punctuation">.</span>show_images<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> titles<span class="token operator">=</span>titles<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 显示图片</span><span class="token comment"># Helper class</span><span class="token comment"># 累加器类</span><span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token comment"># 数据可视化</span><span class="token keyword">class</span> <span class="token class-name">Animator</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""在动画中绘制数据"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xlabel<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 ylim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> xscale<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span> yscale<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span>                 fmts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'-'</span><span class="token punctuation">,</span> <span class="token string">'m--'</span><span class="token punctuation">,</span> <span class="token string">'g-.'</span><span class="token punctuation">,</span> <span class="token string">'r:'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nrows<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ncols<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3.5</span><span class="token punctuation">,</span> <span class="token number">2.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 增量地绘制多条线</span>        <span class="token keyword">if</span> legend <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            legend <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        d2l<span class="token punctuation">.</span>use_svg_display<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fig<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes <span class="token operator">=</span> d2l<span class="token punctuation">.</span>plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token punctuation">,</span> ncols<span class="token punctuation">,</span> figsize<span class="token operator">=</span>figsize<span class="token punctuation">)</span>        <span class="token keyword">if</span> nrows <span class="token operator">*</span> ncols <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>axes <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> <span class="token punctuation">]</span>        <span class="token comment"># 使用lambda函数捕获参数</span>        self<span class="token punctuation">.</span>config_axes <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> d2l<span class="token punctuation">.</span>set_axes<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> xlabel<span class="token punctuation">,</span> ylabel<span class="token punctuation">,</span> xlim<span class="token punctuation">,</span> ylim<span class="token punctuation">,</span> xscale<span class="token punctuation">,</span> yscale<span class="token punctuation">,</span> legend<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>fmts <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> fmts    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 向图表中添加多个数据点</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token string">"__len__"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            y <span class="token operator">=</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span>        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">"__len__"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">*</span> n        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>X<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>Y<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>Y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> a <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> b <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>X<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>a<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>Y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>b<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cla<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> fmt <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>fmts<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> fmt<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>config_axes<span class="token punctuation">(</span><span class="token punctuation">)</span>        display<span class="token punctuation">.</span>display<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fig<span class="token punctuation">)</span>        display<span class="token punctuation">.</span>clear_output<span class="token punctuation">(</span>wait<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token comment"># 读取数据</span>    batch_size <span class="token operator">=</span> <span class="token number">256</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span> <span class="token comment"># 加载数据集</span>    <span class="token comment"># 初始化模型参数</span>    <span class="token comment"># 原始输入图像大小是28*28，这里将其转换为长度为28*28=784的向量</span>    num_inputs <span class="token operator">=</span> <span class="token number">784</span>    <span class="token comment"># 由于有10个类别，所以输出层的输出个数为10</span>    num_outputs <span class="token operator">=</span> <span class="token number">10</span>    <span class="token comment"># 因此，权重将构成一个784*10的矩阵</span>    W <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># 偏置将构成一个10维的向量</span>    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># 训练模型</span>    lr <span class="token operator">=</span> <span class="token number">0.1</span>    num_epochs <span class="token operator">=</span> <span class="token number">10</span>    updater <span class="token operator">=</span> <span class="token keyword">lambda</span> batch_size<span class="token punctuation">:</span> d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>W<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> cross_entropy<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> updater<span class="token punctuation">)</span>    <span class="token comment"># 预测</span>    predict_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span></code></pre><h3 id="softmax回归的简洁实现">3.7 softmax回归的简洁实现</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l<span class="token comment"># 3.7.1 获取和读取数据</span><span class="token comment"># 3.7.2 初始化模型参数</span><span class="token comment"># softmax回归的输出层是一个全连接层。 因此只需在Sequential中添加一个带有10个输出的全连接层</span><span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment"># 3.7.3 重新审视Softmax的实现</span><span class="token comment"># 在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数</span><span class="token comment"># PyTorch中的CrossEntropyLoss类将softmax运算和交叉熵损失计算结合在一起，同时避免了数值不稳定性</span><span class="token comment"># 3.7.4 优化算法</span><span class="token comment"># 与线性回归中的相同，softmax回归也使用小批量随机梯度下降作为优化算法。</span><span class="token comment"># 3.7.5 训练</span><span class="token comment"># 通过调用优化算法的step函数来迭代模型参数</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token comment"># 3.7.1 获取和读取数据</span>    batch_size <span class="token operator">=</span> <span class="token number">256</span>    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>    <span class="token comment"># 3.7.2 初始化模型参数</span>    <span class="token comment"># PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment"># 3.7.3 重新审视Softmax的实现</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>    <span class="token comment"># 3.7.4 优化算法</span>    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>    <span class="token comment"># 3.7.5 训练(3.6.6节中定义的训练函数)</span>    num_epochs <span class="token operator">=</span> <span class="token number">10</span>    d2l<span class="token punctuation">.</span>train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> trainer<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 2 预备知识</title>
      <link href="/2024/06/25/DL/2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"/>
      <url>/2024/06/25/DL/2%20%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h2 id="预备知识">2 预备知识</h2><h3 id="数据操作">2.1 数据操作</h3><p>n 维数组，也称为张量（tensor）。无论使用哪个深度学习框架，它的张量类都与Numpy的ndarray类似。</p><p>但深度学习框架又比Numpy的ndarray多一些重要功能：</p><ul><li>GPU很好地支持加速计算，而NumPy仅支持CPU计算</li><li>张量类支持自动微分。</li></ul><h3 id="数据预处理">2.2 数据预处理</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 2.2.1 读取数据集</span><span class="token comment"># 读取数据到CSV文件</span><span class="token keyword">import</span> osos<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'..'</span><span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 确保文件夹存在</span>data_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'..'</span><span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token string">'house_tiny.csv'</span><span class="token punctuation">)</span> <span class="token comment"># 数据文件名</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>data_file<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span> <span class="token comment"># 创建并写入一个CSV文件</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'NumRooms,Alley,Price\n'</span><span class="token punctuation">)</span>  <span class="token comment"># 列名</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'NA,Pave,127500\n'</span><span class="token punctuation">)</span>  <span class="token comment"># 每行表示一个数据样本</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'2,NA,106000\n'</span><span class="token punctuation">)</span>     f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'4,NA,178100\n'</span><span class="token punctuation">)</span>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'NA,NA,140000\n'</span><span class="token punctuation">)</span><span class="token comment"># 读取数据</span><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pddata <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_file<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token comment"># 2.2.2 处理缺失值</span>inputs<span class="token punctuation">,</span> outputs <span class="token operator">=</span> data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment"># iloc索引器，用于分离输入数据和输出数据</span>inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项</span><span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span><span class="token comment"># 2.2.3 转换为张量格式</span><span class="token keyword">import</span> torchX <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>to_numpy<span class="token punctuation">(</span>dtype<span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>to_numpy<span class="token punctuation">(</span>dtype<span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span></code></pre><h4 id="读取数据集">2.2.1 读取数据集</h4><p>首先将数据存入CSV文件，然后使用Pandas库读取数据。</p><h4 id="处理缺失值">2.2.2 处理缺失值</h4><p>典型的方法包括<strong>插值法</strong>和<strong>删除法</strong>，插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。</p><h4 id="转换为张量格式">2.2.3 转换为张量格式</h4><p><code>X=torch.tensor(X.values)</code></p><h3 id="线性代数">2.3 线性代数</h3><h4 id="矩阵的广播机制">2.3.1 矩阵的广播机制</h4><p>当对两个形状不同的张量按元素运算时，可能会触发广播机制：首先，将元素逐元素地复制到一个合适的形状，以使两个张量具有相同的形状，然后再按元素运算。</p><p>广播的规则如下：</p><ol type="1"><li>如果张量的维数不同，将维数较小的张量进行扩展，直到两个张量的维数都一样。例如，将张量的形状(2,3, 4)和(4,)广播为(2, 3, 4)和(1, 1, 4)。</li><li>如果两个张量在某个维度上的长度是相同的，<strong><em>或其中一个张量在该维度上的长度为1</em></strong>，那么我们就说这两个张量在该维度上是相容的。例如，(2,3, 1)和(1, 1, 4)是相容的。</li></ol><p><strong>注意:两个张量的形状在任何一个维度上要么相等，要么其中一个为1。</strong></p><h3 id="微积分">2.4 微积分</h3><h3 id="自动微分">2.5 自动微分</h3><p>深度学习框架通过自动计算导数，即自动微分（automaticdifferentiation）来加快求导。实际中，根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p><p>我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</p><h3 id="概率">2.6 概率</h3><p>简单地说，机器学习就是做出预测。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习—— 1 引言</title>
      <link href="/2024/06/25/DL/1%20%E5%BC%95%E8%A8%80/"/>
      <url>/2024/06/25/DL/1%20%E5%BC%95%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<p>基于<a href="https://zh-v2.d2l.ai/">动手学深度学习Pytorch版</a></p><h2 id="引言">1 引言</h2><h3 id="机器学习中的关键组件">1.1 机器学习中的关键组件</h3><ol type="1"><li>可以用来学习的数据（data）；</li><li>如何转换数据的模型（model）；</li><li>⼀个目标函数（objective function），用来量化模型的有效性；</li><li>调整模型参数以优化目标函数的算法（algorithm）。</li></ol><h3 id="各种机器学习问题">1.2 各种机器学习问题</h3><h4 id="监督学习">1.2.1 监督学习</h4><p>监督学习（supervised learning）擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都称为一个样本（example）。有时，即使标签是未知的，样本也可以指代输入特征。我们的目标是生成一个模型，能够将任何输入特征映射到标签（即预测）。</p><p>监督学习的学习过程一般可以分为三大步骤：</p><ol type="1"><li>从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。这些输入和相应的标签一起构成了训练数据集；</li><li>选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；</li><li>将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。</li></ol><p>具体如图1.1所示:</p><figure><img src="./images/1.1_supervised-learning.svg#60x60"title="图1.1：监督学习" alt="监督学习" /><figcaption aria-hidden="true">监督学习</figcaption></figure><h5 id="回归">1.2.1.1 回归</h5><p>当标签取任意数值时，我们称之为回归问题，即训练一个回归函数来输出一个数值。例如，给定一组房屋的特征，我们可以训练一个模型来预测房屋的价格。回归问题的常见损失函数为<strong>平方误差</strong>。</p><h5 id="分类">1.2.1.2 分类</h5><p>分类问题希望模型能够预测样本属于哪个类别。例如，给定一组猫和狗的图片，我们可以训练一个模型来预测图片中是猫还是狗。分类问题的常见损失函数为<strong>交叉熵</strong>。</p><h5 id="标记问题">1.2.1.3 标记问题</h5><p>类似于多标签分类，比如识别一张图中所有的物体并给出标记。</p><h5 id="搜索">1.2.1.4 搜索</h5><p>在信息检索领域，我们希望对一组项目进行排序。以网络搜索为例，目标不是简单的“查询（query）-网页（page）”分类，而是在海量搜索结果中找到用户最需要的那部分。可能的解决方案是首先为集合中的每个元素分配相应的相关性分数，然后检索评级最高的元素。</p><h5 id="推荐系统">1.2.1.5 推荐系统</h5><p>目标是向特定用户进行“个性化”推荐</p><h5 id="序列学习">1.2.1.6 序列学习</h5><p>如果输入是连续的，模型可能就需要拥有“记忆”功能。因为预测后者需要前者的信息，比如语音识别，机器翻译等。</p><h4 id="无监督学习">1.2.2 无监督学习</h4><ol type="1"><li><strong>聚类</strong>（clustering）问题：没有标签的情况下，给数据分类。</li><li><strong>主成分分析</strong>（principal componentanalysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。</li><li><strong>因果关系</strong>（causality）和<strong>概率图</strong>模型（probabilisticgraphicalmodels）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</li><li><strong>生成对抗性网络</strong>（generative adversarialnetworks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试。</li></ol><h4 id="与环境互动">1.2.3 与环境互动</h4><p>到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。<strong>这里所有学习都是在算法与环境断开后进行的，被称为离线学习</strong>。</p><h4 id="强化学习">1.2.4 强化学习</h4><p>智能体（agent）在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制将其传输回环境，最后智能体从环境中获得奖励（reward）。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。强化学习的过程在图1.2中进行了说明。请注意，强化学习的目标是产生一个好的策略（policy）。强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。</p><figure><img src="./images/1.2_reinforcement-learning.svg#70x70"title="图1.2：强化学习" alt="强化学习" /><figcaption aria-hidden="true">强化学习</figcaption></figure><ul><li><strong>当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程</strong>（markovdecision process）。</li><li>当状态不依赖于之前的操作时，我们称该问题为上下文赌博机（contextualbandit problem）。</li><li>当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的多臂赌博机（multi-armedbandit problem）。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Win应用清单文件</title>
      <link href="/2024/06/20/TechnicalNotes/win%E5%BA%94%E7%94%A8%E6%B8%85%E5%8D%95/"/>
      <url>/2024/06/20/TechnicalNotes/win%E5%BA%94%E7%94%A8%E6%B8%85%E5%8D%95/</url>
      
        <content type="html"><![CDATA[<h2 id="package.appxmanifest-文件概述">1. Package.appxmanifest文件概述</h2><p>Package.appxmanifest 文件是 Windows应用程序的清单文件，包含有关应用程序的元数据和配置信息。它定义了应用程序的名称、版本、描述、图标、权限等信息。Package.appxmanifest文件是 Windows 应用程序打包和部署的核心部分。</p><p>在VS2022中创建 WinUI3 项目时，Package.appxmanifest文件会自动生成。可以在项目的根目录下找到该文件。 该文件是 XML格式的，可以使用文本编辑器或 Visual Studio 的图形化界面进行编辑。</p><h2 id="package.appxmanifest-文件的结构">2. Package.appxmanifest文件的结构</h2><h3 id="应用资产">2.1 应用资产</h3><ol type="1"><li>显示名称：应用程序的名称，在 Windows 开始菜单和任务栏中显示。</li><li>入口点：指定应用启动时要执行的类或文件。默认为<code>$targetentrypoint$</code>，通常在 WinUI 3 应用中指向 App.xaml.cs文件中的 OnLaunched 方法。</li><li>默认语言：指定应用程序的默认语言。可以使用语言代码（如<code>en-US</code>、<code>zh-CN</code> 等）来表示不同的语言。</li><li>说明：应用程序的简短描述，通常在 Windows应用商店中显示。它应该清晰地传达应用程序的功能和特点。</li><li>信任级别：指定应用的安全上下文。<ol type="1"><li><code>AppContainer</code>：沙盒环境，限制应用访问系统资源，提供更高的安全性。</li><li><code>MediumIL</code>：较高权限，允许访问更多系统资源。</li></ol></li><li>运行时行为：<ol type="1"><li><code>WindowsApp</code>：传统 UWP 应用程序的运行时行为，遵循 Windows应用生命周期管理。</li><li><code>PackagedClassicApp</code>：打包的传统应用程序，通常是通过Desktop Bridge 打包的 Win32 应用，保留 Win32 行为但享受现代包装。</li><li><code>Win32App</code>：标准 Win32应用程序行为，适用于传统桌面应用程序。</li><li><code>AppSilo</code>：提供隔离环境，允许应用在隔离的上下文中执行代码，增强安全性和稳定性。</li></ol></li><li>支持的旋转：指示应用程序首选的屏幕方向。</li><li>锁定屏幕通知：<ol type="1"><li>徽章：小图标通知，可在锁屏界面显示数字或图标，通常用于显示未读消息数量或应用状态。</li><li>徽章和磁贴文本：除徽章外，还可显示文字通知，提供更丰富的信息内容。</li></ol></li><li>资源组：用于组织和管理应用资源（如图标、语言文件）。</li><li>磁贴更新：配置动态磁贴的更新方式。<ul><li>重复(时间)：定义磁贴内容更新的频率（如每30分钟、每小时）。</li><li>URI模板：指定获取磁贴更新内容的网络地址，用于动态更新磁贴内容。</li></ul></li></ol><h2 id="视觉对象资产">2.2 视觉对象资产</h2><p>视觉对象资产定义了应用的图形标识元素。</p><h2 id="功能">2.3 功能</h2><p>功能定义应用可以访问的系统资源和API权限。</p><h2 id="声明">2.4 声明</h2><p>声明定义应用可以与Windows操作系统和其他应用进行深度集成的方式。</p><h2 id="内容uri">2.5 内容URI</h2><p>内容URI规则定义了应用内WebView控件可以导航到的网站。</p><h2 id="打包">2.6 打包</h2><ol type="1"><li>包名: 应用程序的唯一标识符，通常是一个反向域名格式的字符串（如<code>com.example.app</code>）。</li><li>包显示名称：应用程序在 Windows 系统中的显示名称。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习介绍</title>
      <link href="/2024/06/05/ML/introduction/"/>
      <url>/2024/06/05/ML/introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习介绍">机器学习介绍</h1><h2 id="机器学习的定义">1 机器学习的定义</h2><p>机器学习是人工智能的一个子领域，研究如何让计算机系统利用数据和经验，来不断改善和优化自身的性能。其核心思想是通过算法和模型让计算机从数据中学习，而不是通过明确的编程规则来执行任务。</p><h2 id="机器学习的分类">2 机器学习的分类</h2><p>机器学习的主要分支有监督学习、无监督学习、半监督学习、强化学习、深度学习等。</p><ol type="1"><li>根据学习方式：<ol type="1"><li>监督学习(Supervised Learning)</li><li>无监督学习(Unsupervised Learning)</li><li>半监督学习(Semi-supervised Learning)</li><li>强化学习(Reinforcement Learning)</li></ol></li><li>根据学习任务：<ol type="1"><li>分类(Classification)</li><li>回归(Regression)</li><li>聚类(Clustering)</li><li>降维(Dimensionality Reduction)</li></ol></li></ol><p>它们之间的关系如图所示：</p><div class="mermaid-wrap"><pre class="mermaid-src" hidden>  graph LR    A[机器学习] --&gt; B[监督学习]    A --&gt; C[无监督学习]    B --&gt; D[半监督学习]    C --&gt; D[半监督学习]    A --&gt; E[强化学习]    A --&gt; F[深度学习]    A --&gt; G[迁移学习]    E --&gt; H[深度强化学习]    F --&gt; H  </pre></div><figure><img src="./images/ML_Categories.webp" alt="机器学习分类" /><figcaption aria-hidden="true">机器学习分类</figcaption></figure><h3 id="监督学习supervised-learning">2.1 监督学习(SupervisedLearning)</h3><p>在监督学习中，模型通过使用带标签的训练数据进行学习，即每个输入数据都有一个对应的输出标签。目标是学习从输入到输出的映射关系，能够对新数据进行预测。</p><ol type="1"><li>主要类别/任务：<ol type="1"><li>分类(Classification): 将输入数据分类到预定义的类别中。</li><li>回归(Regression): 预测连续的数值。</li></ol></li><li>主要算法<ol type="1"><li>逻辑回归(Logistic Regression)</li><li>线性回归(Linear Regression)</li><li>支持向量机(Support Vector Machine, SVM)</li><li>决策树(Decision Tree)</li><li>随机森林(Random Forest)</li><li>深度神经网络(Deep Neural Networks, DNN)</li></ol></li></ol><h3 id="无监督学习unsupervised-learning">2.2 无监督学习(UnsupervisedLearning)</h3><p>在无监督学习中，模型使用未标记的数据进行学习，试图从数据中发现隐藏的结构或模式。无监督学习不依赖于输出标签。</p><ol type="1"><li>主要类别/任务：<ol type="1"><li>聚类(Clustering):将数据分组到不同的类别中，使得同一类别内的数据相似度较高，不同类别之间的数据相似度较低。</li><li>降维(Dimensionality Reduction):减少数据的特征数量，同时尽可能保留重要信息。</li></ol></li><li>主要算法<ol type="1"><li>K均值聚类(K-means Clustering)</li><li>层次聚类(Hierarchical Clustering)</li><li>主成分分析(Principal Component Analysis, PCA)</li><li>独立成分分析(Independent Component Analysis, ICA)</li><li>t-分布邻域嵌入(t-Distributed Stochastic Neighbor Embedding,t-SNE)</li><li>自编码器(Autoencoder)</li><li>生成对抗网络(Generative Adversarial Networks, GAN)</li></ol></li></ol><h3 id="半监督学习semi-supervised-learning">2.3半监督学习(Semi-supervised Learning)</h3><p>半监督学习结合了监督学习和无监督学习的特点，使用少量标记数据和大量未标记数据进行训练。这种方法在标记数据难以获取但未标记数据丰富的情况下非常有用。</p><ol type="1"><li>图半监督学习(Graph-based Semi-supervised Learning)</li><li>半监督支持向量机(Semi-supervised Support Vector Machine)</li><li>半监督深度学习(Semi-supervised Deep Learning)</li></ol><h3 id="强化学习reinforcement-learning">2.4 强化学习(ReinforcementLearning)</h3><p>通过智能体/机器人/代理（Agent）与环境（Environment）进行交互学习的方法。在强化学习中，代理根据环境的状态（State）选择动作（Action），并通过观察环境的反馈（奖励（Reward）或惩罚）来调整自己的行为策略，以达到最大化长期累积奖励的目标。</p><ol type="1"><li>主要类别/任务：<ol type="1"><li>基于值函数的强化学习(Value-based Reinforcement Learning):通过学习值函数（ValueFunction）来指导代理的决策，以获得最大的长期累积奖励。</li><li>基于策略的强化学习(Policy-based Reinforcement Learning):直接学习最优策略（Policy），以获得最大的长期累积奖励。</li><li>基于模型的强化学习(Model-based Reinforcement Learning):通过学习环境的模型来指导代理的决策，以获得最大的长期累积奖励。</li></ol></li><li>主要算法<ol type="1"><li>Q学习(Q-Learning)</li><li>深度Q网络(Deep Q-Network, DQN)</li><li>深度确定性策略梯度(Deep Deterministic Policy Gradient, DDPG)</li><li>近端策略优化(Proximal Policy Optimization, PPO)</li></ol></li></ol><h3 id="深度学习deep-learning">2.5 深度学习(Deep Learning)</h3><p>使用人工神经网络模型来学习数据的表征。与传统机器学习算法相比，深度学习模型可以自动地从数据中学习更加复杂、高阶的特征表示，因此在处理大规模、高维度数据时具有很强的表达能力。</p><ol type="1"><li>主要类别/任务：<ol type="1"><li>图像识别(Image Recognition)</li><li>语音识别(Speech Recognition)</li><li>自然语言处理(Natural Language Processing, NLP)</li></ol></li><li>主要算法<ol type="1"><li>卷积神经网络(Convolutional Neural Networks, CNN)</li><li>循环神经网络(Recurrent Neural Networks, RNN)</li><li>长短时记忆网络(Long Short-Term Memory, LSTM)</li><li>生成对抗网络(Generative Adversarial Networks, GAN)</li></ol></li></ol><h3 id="深度强化学习deep-reinforcement-learning">2.6 深度强化学习(DeepReinforcement Learning)</h3><p>将深度学习和强化学习结合起来，使用深度神经网络模型来学习代理的策略或值函数，以获得最大的长期累积奖励。</p><ol type="1"><li>主要类别/任务：<ol type="1"><li>游戏玩法(Game Playing)</li><li>机器人控制(Robot Control)</li><li>自动驾驶(Autonomous Driving)</li></ol></li><li>主要算法<ol type="1"><li>深度Q网络(Deep Q-Network, DQN)</li><li>深度确定性策略梯度(Deep Deterministic Policy Gradient, DDPG)</li><li>近端策略优化(Proximal Policy Optimization, PPO)</li></ol></li></ol><h3 id="迁移学习transfer-learning">2.7 迁移学习(Transfer Learning)</h3><p>利用一个任务的学习经验来加速另一个相关任务学习的方法。在迁移学习中，模型通过学习一个任务的特征表示，可以更快地适应新的任务，特别是当新任务的数据较少或者新任务与原任务有一定的相关性时。</p><ol type="1"><li>微调(Fine-tuning):在预训练模型的基础上，使用新任务的数据进行微调，以适应新任务。</li><li>特征提取(Feature Extraction):使用预训练模型提取特征，然后在新任务上训练一个简单的分类器。</li><li>领域适应(Domain Adaptation):在源领域和目标领域之间进行迁移，以提高模型在目标领域的性能。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python系列之datetime</title>
      <link href="/2024/03/05/Python/datetime/"/>
      <url>/2024/03/05/Python/datetime/</url>
      
        <content type="html"><![CDATA[<h1 id="datetime">datetime</h1><p><code>datetime</code>模块提供了处理日期和时间的类。它支持日期、时间、时区等操作。</p><h2 id="获取日期和时间">1 获取日期和时间</h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> datetime<span class="token keyword">def</span> <span class="token function">get_time</span><span class="token punctuation">(</span>delta<span class="token punctuation">,</span> unit<span class="token punctuation">,</span> hours_offset<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    :param delta: delta time    :param unit: options: 'minutes', 'hours', 'days' or 'seconds' or 'none'    :param hours_offset: East 8 time zone offset (Default value = 8)    """</span>    <span class="token comment"># 创建东八区时区对象，偏移量为8小时</span>    tz_east_8 <span class="token operator">=</span> datetime<span class="token punctuation">.</span>timezone<span class="token punctuation">(</span>datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>hours<span class="token operator">=</span>hours_offset<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 获取当前时间</span>    now <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span>tz_east_8<span class="token punctuation">)</span>    <span class="token comment"># 计算未来时间</span>    <span class="token keyword">if</span> unit <span class="token operator">==</span> <span class="token string">"none"</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> now    <span class="token keyword">elif</span> unit <span class="token operator">==</span> <span class="token string">"seconds"</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> now <span class="token operator">+</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>seconds<span class="token operator">=</span>delta<span class="token punctuation">)</span>    <span class="token keyword">elif</span> unit <span class="token operator">==</span> <span class="token string">"minutes"</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> now <span class="token operator">+</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>minutes<span class="token operator">=</span>delta<span class="token punctuation">)</span>    <span class="token keyword">elif</span> unit <span class="token operator">==</span> <span class="token string">"hours"</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> now <span class="token operator">+</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>hours<span class="token operator">=</span>delta<span class="token punctuation">)</span>    <span class="token keyword">elif</span> unit <span class="token operator">==</span> <span class="token string">"days"</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> now <span class="token operator">+</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span>delta<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported unit. Please use 'seconds', 'minutes', 'hours', 'days' or 'none'."</span><span class="token punctuation">)</span><span class="token comment"># 示例</span><span class="token comment"># 获取当前时间</span>current_time <span class="token operator">=</span> get_time<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"none"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Current time:"</span><span class="token punctuation">,</span> current_time<span class="token punctuation">)</span><span class="token comment"># 获取未来时间</span>future_time <span class="token operator">=</span> get_time<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">"minutes"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Future time:"</span><span class="token punctuation">,</span> future_time<span class="token punctuation">)</span></code></pre><h2 id="当前时间转为字符串">2 当前时间转为字符串</h2><p>将 datetime.datetime 或 datetime.date 类型转化为字符串格式</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> datetime<span class="token keyword">def</span> <span class="token function">datetime_to_string</span><span class="token punctuation">(</span>dt<span class="token punctuation">,</span> <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">"%Y-%m-%d %H:%M:%S"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    :param dt: datetime.datetime or datetime.date    :param format: format string    :return: formatted string    """</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>dt<span class="token punctuation">,</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> dt<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token builtin">format</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>dt<span class="token punctuation">,</span> datetime<span class="token punctuation">.</span>date<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> dt<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token builtin">format</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported type. Please use datetime.datetime or datetime.date."</span><span class="token punctuation">)</span><span class="token comment"># 示例</span><span class="token comment"># 获取当前时间</span>current_time <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 转换为字符串</span>current_time_str <span class="token operator">=</span> datetime_to_string<span class="token punctuation">(</span>current_time<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Current time as string:"</span><span class="token punctuation">,</span> current_time_str<span class="token punctuation">)</span><span class="token comment"># 转换为日期</span>current_date <span class="token operator">=</span> datetime<span class="token punctuation">.</span>date<span class="token punctuation">.</span>today<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 转换为字符串</span>current_date_str <span class="token operator">=</span> datetime_to_string<span class="token punctuation">(</span>current_date<span class="token punctuation">,</span> <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">"%Y-%m-%d"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Current date as string:"</span><span class="token punctuation">,</span> current_date_str<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python系列之qrcode</title>
      <link href="/2024/03/03/Python/qrcode/"/>
      <url>/2024/03/03/Python/qrcode/</url>
      
        <content type="html"><![CDATA[<p><code>qrcode</code> 是一个用于生成二维码的 Python库。它可以轻松地将文本、URL 或其他数据编码为二维码图像。</p><h2 id="安装">1 安装</h2><pre class="language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> qrcode</code></pre><h2 id="使用">2 使用</h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> qrcode<span class="token comment"># 数据</span>download_link <span class="token operator">=</span> <span class="token string">"https://example.com/download"</span><span class="token comment"># 创建二维码对象</span>qr <span class="token operator">=</span> qrcode<span class="token punctuation">.</span>QRCode<span class="token punctuation">(</span>    version<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    error_correction<span class="token operator">=</span>qrcode<span class="token punctuation">.</span>constants<span class="token punctuation">.</span>ERROR_CORRECT_L<span class="token punctuation">,</span>    box_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>    border<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token comment"># 添加数据到二维码</span>qr<span class="token punctuation">.</span>add_data<span class="token punctuation">(</span>download_link<span class="token punctuation">)</span>qr<span class="token punctuation">.</span>make<span class="token punctuation">(</span>fit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 自动调整二维码大小以适应数据</span><span class="token comment"># 创建二维码图片</span>qr_image <span class="token operator">=</span> qr<span class="token punctuation">.</span>make_image<span class="token punctuation">(</span>fill_color<span class="token operator">=</span><span class="token string">"black"</span><span class="token punctuation">,</span> back_color<span class="token operator">=</span><span class="token string">"white"</span><span class="token punctuation">)</span><span class="token comment"># 保存二维码图片</span><span class="token comment"># qr_image.save("qr_code.png")</span><span class="token comment"># 保存为webp格式</span>qr_image<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"qr_code.webp"</span><span class="token punctuation">,</span> <span class="token string">"WEBP"</span><span class="token punctuation">)</span></code></pre><p><strong>参数说明：</strong></p><ol type="1"><li>version:二维码的版本，版本号越高，二维码可以存储的信息量就越大，但二维码的尺寸也会增加<ul><li>版本号从1到40，1代表最小尺寸，40代表最大尺寸</li><li>每个版本的二维码都是一个正方形，版本号为1时，二维码的尺寸为21x21个小格子，每增加一个版本，二维码的尺寸就增加4个小格子</li></ul></li><li>error_correction: 二维码的纠错级别，用于提高二维码的容错能力<ul><li>分别代表了从低到高的纠错能力，但二维码的尺寸也会增加<ul><li>ERROR_CORRECT_L: 7% 的数据可以被纠正</li><li>ERROR_CORRECT_M: 15% 的数据可以被纠正</li><li>ERROR_CORRECT_Q: 25% 的数据可以被纠正</li><li>ERROR_CORRECT_H: 30% 的数据可以被纠正</li></ul></li></ul></li><li>box_size:每个小格子（或称为像素）的尺寸。值越大，生成的二维码图像越大。box_size被设置为 10，表示每个小格子的尺寸为 10 个像素</li><li>border:二维码图像的边框宽度，即围绕二维码内容的空白边缘的宽度。border 被设置为4，表示二维码图像的边框宽度为 4 个像素</li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>数据库</title>
      <link href="/2024/03/03/Web/database/"/>
      <url>/2024/03/03/Web/database/</url>
      
        <content type="html"><![CDATA[<h1 id="数据库">数据库</h1><h2 id="mysql">1. MySQL</h2><p>用于存储网站数据</p><h3 id="数据库操作">1.1 数据库操作</h3><p>查看所有数据库</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">SHOW</span> <span class="token keyword">DATABASES</span><span class="token punctuation">;</span></code></pre><p>创建数据库</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">DATABASE</span> xxxxx <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span> utf8 <span class="token keyword">COLLATE</span> utf8_general_ci<span class="token punctuation">;</span></code></pre><p>删除数据库</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">DROP</span> <span class="token keyword">DATABASE</span> xxxxx<span class="token punctuation">;</span></code></pre><p>进入数据库</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">USE</span> xxxxx<span class="token punctuation">;</span></code></pre><p>查看数据库下的数据表</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">show</span> <span class="token keyword">tables</span><span class="token punctuation">;</span></code></pre><h3 id="数据表操作">1.2 数据表操作</h3><p>创建数据表</p><p>例auth</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> auth<span class="token punctuation">(</span>     id <span class="token keyword">int</span> <span class="token keyword">primary</span> <span class="token keyword">key</span> <span class="token keyword">auto_increment</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>     username <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>     password <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>     email <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>     user_auth_token <span class="token keyword">varchar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     expires <span class="token keyword">datetime</span> <span class="token punctuation">)</span><span class="token keyword">default</span> <span class="token keyword">charset</span><span class="token operator">=</span>utf8<span class="token punctuation">;</span></code></pre><p>查看数据表结构</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">desc</span> xxxx<span class="token punctuation">;</span></code></pre><p>查看数据表数据</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> xxxx<span class="token punctuation">;</span></code></pre><p>删除数据表</p><pre class="language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">table</span> xxxx<span class="token punctuation">;</span></code></pre><h2 id="sqlite">2. SQLite</h2><p>用于存储本地app数据</p><h2 id="数据库转换">3. 数据库转换</h2><p>SQLite转MySQL</p><ol type="1"><li>MT管理器导出数据库文件xxx.db或者xxx.database(需要重命名为xxx.db)</li><li>DB Browser for SQLite将xxx.db导出为xxx.csv或者xxx.sql</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flask建站教程</title>
      <link href="/2024/03/03/Web/flasksite/"/>
      <url>/2024/03/03/Web/flasksite/</url>
      
        <content type="html"><![CDATA[<h1 id="flask建站教程">Flask建站教程</h1><p>仅针对宝塔面板的建站教程。</p><h2 id="建站类型">建站类型</h2><ol type="1"><li>静态资源站选择HTML项目</li><li>有可执行文件的选择对应的项目，如PHP项目、Node项目、Python项目、Java项目、Go项目、其他项目</li><li>非静态资源站，也没有可执行文件的选择纯静态PHP项目，可以配置反向代理等</li></ol><h2 id="建站通用配置">建站通用配置</h2><h3 id="ssl">SSL</h3><ol type="1"><li>Let’s Encrypt 证书</li><li>DNS 验证-手动解析-自动组合泛域名</li><li>按照提示修改对应的 CloudFlare 的 DNS 解析</li><li>验证即可</li></ol><h3 id="nginx">Nginx</h3><p>Nginx 包含反向代理等配置，可以在宝塔面板中配置，仅说明server部分。Nginxserver通常包含基本配置、日志配置、Location块、反向代理、HTTPS配置、重定向、缓存配置等</p><p>关键结构如下：</p><pre class="language-nginx" data-language="nginx"><code class="language-nginx"><span class="token directive"><span class="token keyword">server</span></span> <span class="token punctuation">&#123;</span>    基础配置    Location块-用于匹配请求的路径，并定义其处理方式-例如自定义可以访问的目录    反向代理-其实可以看做特殊的Location块<span class="token punctuation">&#125;</span></code></pre><h3 id="具体步骤">具体步骤</h3><h4 id="环境准备">环境准备</h4><ol type="1"><li>安装Python</li><li>准备requirements.txt</li><li>域名需要先在CloudFlare中解析</li></ol><h4 id="配置">配置</h4><ol type="1"><li>初始化配置，启动方式gunicorn，通信协议wsgi，环境变量无，启动用户www</li><li>等待安装requirements.txt中的依赖</li><li>进入配置界面</li><li>项目信息加入开机启动</li><li>域名管理-添加域名</li><li>外网映射打开-可能需要提前配置SSL</li><li>添加Nginx配置</li><li>配置SSL证书，打开强制HTTPS</li><li>重定向，例如非www重定向到www</li><li>在<strong>自定义的下方</strong>添加gunicon配置</li><li>测试访问</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Mailgun用户手册</title>
      <link href="/2024/03/03/Web/mailgun/"/>
      <url>/2024/03/03/Web/mailgun/</url>
      
        <content type="html"><![CDATA[<h1 id="mailgun用户手册">Mailgun用户手册</h1><p>个人信息界面请参考<ahref="https://app.mailgun.com/mg/dashboard">Mailgun面板</a>仅介绍http接口的使用，不涉及smtp协议。 更多信息请参考<ahref="https://documentation.mailgun.com/">官方文档</a></p><h2 id="发送邮件">2. 发送邮件</h2><h3 id="发送基本文本">2.1 发送基本文本</h3><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient-1@example.com \    -F to&#x3D;recipient-2@example.com \    -F subject&#x3D;&#39;Hello there!&#39; \    -F text&#x3D;&#39;Testing some Mailgun awesomeness!&#39;</code></pre><h3 id="以文本和-html-版本发送">2.2 以文本和 HTML 版本发送</h3><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F text&#x3D;&#39;This will be the text-only version&#39; \    --form-string html&#x3D;&#39;&lt;html&gt;&lt;body&gt;&lt;p&gt;This is the HTML version&lt;&#x2F;p&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;&#39;</code></pre><h3 id="发送附件">2.3 发送附件</h3><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F text&#x3D;&#39;Testing some Mailgun awesomeness!&#39; \    -F attachment&#x3D;@tps-report.txt \    -F attachment&#x3D;@cover-letter.txt</code></pre><h3 id="发送内联文件">2.4 发送内联文件</h3><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F inline&#x3D;@logo.jpg \    --form-string html&#x3D;&#39;&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello from &lt;img src&#x3D;&quot;cid:email.webp&quot;&#x2F;&gt;&lt;&#x2F;p&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;&#39;</code></pre><h3 id="自定义功能">2.5 自定义功能</h3><h4 id="仅对特定消息启用跟踪">2.5.1 仅对特定消息启用跟踪</h4><p>虽然可以为仪表板中的所有消息启用跟踪，但你也可以有选择地针对每条消息启用跟踪。要启用所有跟踪类型，请使用“o:tracking=“yes””参数。否则，你可以仅启用对打开（‘o:tracking-opens’）或点击（‘o:tracking-clicks’）的特定跟踪：</p><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F text&#x3D;&#39;Testing some Mailgun awesomeness!&#39; \    -F o:tracking-opens&#x3D;&quot;yes&quot;</code></pre><h4 id="特定时间发送">2.5.2 特定时间发送</h4><p>“o:deliverytime”选项允许你指定何时发送电子邮件。 它使用 RFC822日期格式，并且不能超过未来 3 天（除非计费计划支持 7天或更多天的存储能力）：</p><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F text&#x3D;&#39;Testing some Mailgun awesomeness!&#39; \    -F o:deliverytime&#x3D;&#39;Fri, 14 Oct 2011 23:10:10 -0000&#39;</code></pre><h4 id="标记电子邮件">2.5.3 标记电子邮件</h4><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; \    https:&#x2F;&#x2F;api.mailgun.net&#x2F;v3&#x2F;YOUR_DOMAIN_NAME&#x2F;messages \    -F from&#x3D;&#39;Excited User &lt;postmaster@YOUR_DOMAIN_NAME&gt;&#39; \    -F to&#x3D;recipient@example.com \    -F subject&#x3D;&quot;Hello there!&quot; \    -F text&#x3D;&#39;Testing some Mailgun awesomeness!&#39; \    -F o:tag&#x3D;&#39;September newsletter&#39; \    -F o:tag&#x3D;&#39;newsletters&#39;</code></pre><h4 id="重新发送之前发送的电子邮件">2.5.4重新发送之前发送的电子邮件</h4><pre class="language-MiME" data-language="MiME"><code class="language-MiME">curl -s --user &#39;api:YOUR_API_KEY&#39; &#123;&#123;STORAGE.URL&#125;&#125; \    -F to&#x3D;&#39;bob@example.com, john@example.com&#39;</code></pre><h4 id="批量发送">2.5.5 批量发送</h4><p>Mailgun 支持通过单个 API 调用或 SMTP会话发送给一组收件人的功能。这是通过以下方式实现的：</p><ol type="1"><li>通过指定多个收件人电子邮件地址作为 to参数并使用收件人变量来使用批量发送。</li><li>将邮件列表与模板变量结合使用</li></ol><p>更多信息请参考<ahref="https://documentation.mailgun.com/docs/mailgun/user-manual/sending-messages/#batch-sending">批量发送</a></p><h4 id="邮件列表">2.5.6 邮件列表</h4><p>邮件列表是使用电子邮件别名向多个收件人发送邮件的好方法。当你使用邮件列表时，Mailgun将使用电子邮件别名向每个订阅成员发送邮件副本。你可以使用 API或控制面板创建和维护订阅者列表。此外，你可以使用模板变量为邮件列表的每个成员创建唯一的消息。</p><p>更多信息请参考<ahref="https://documentation.mailgun.com/docs/mailgun/user-manual/sending-messages/#mailing-lists">邮件列表</a></p><h2 id="追踪消息">3. 追踪消息</h2><h3 id="启用跟踪">3.1 启用跟踪</h3><p>事件跟踪会自动启用，但取消订阅、打开和点击除外。</p><p>你可以通过控制面板的“域”选项卡为你的域启用取消订阅跟踪。你还可以使用取消订阅变量来管理每条消息的取消订阅。</p><p>你可以在两个级别启用打开和点击跟踪 - 每个发送域和每个消息。</p><ul><li>你可以在特定域设置页面的 “跟踪设置”部分下针对每个域启用打开和点击跟踪 。</li><li>发送消息时，还可以通过设置 o:tracking、o:tracking-clicks 和o:tracking-opens 参数来切换跟踪。这将覆盖域级别设置。</li></ul><p>你需要将 CNAME 记录指向 mailgun.org，以便 Mailgun重写链接并跟踪打开情况。此外，邮件中需要有一个 HTML 部分，以便 Mailgun跟踪打开情况。</p><h3 id="事件跟踪">3.2 事件跟踪</h3><p>Mailgun 会自动跟踪以下事件：</p><ol type="1"><li>accepted：Mailgun接受了发送/转发电子邮件的请求，并且邮件已放入队列中。</li><li>rejected：Mailgun 拒绝了发送/转发电子邮件的请求。</li><li>delivered：Mailgun发送了电子邮件，并被收件人电子邮件服务器接受。</li><li>failed：Mailgun 无法将电子邮件传送到收件人电子邮件服务器。</li><li>opened：电子邮件收件人打开电子邮件并启用图像查看。必须在 Mailgun控制面板中启用开放式跟踪，并且 CNAME 记录必须指向 mailgun.org。</li><li>clicked：电子邮件收件人单击了电子邮件中的链接。必须在 Mailgun控制面板中启用点击跟踪，并且 CNAME 记录必须指向 mailgun.org。</li><li>unsubscribed：电子邮件收件人单击取消订阅链接。必须在 Mailgun控制面板中启用取消订阅跟踪。</li><li>complained：电子邮件收件人单击其电子邮件客户端中的垃圾邮件投诉按钮。反馈循环使Mailgun 能够接收通知。</li><li>stored：Mail 已存储传入消息。</li><li>list_member_uploaded：此事件在成功将成员添加到邮件列表后发生。</li><li>list_member_upload_error：如果将成员添加到邮件列表时发生错误，甚至会发生这种情况。</li><li>list_uploaded：此事件在成功将大量成员上传到邮件列表后发生。</li></ol><p>你可以通过几个接口访问事件：</p><ol type="1"><li>Webhooks（我们将数据发布到你的 URL）</li><li>事件 API（你通过 API 获取数据）</li><li>控制面板 (GUI) 的 日志选项 卡</li></ol><h3 id="网络钩子">3.3 网络钩子</h3><p>当你的邮件发生事件时，Mailgun 可以向你的 URL 发出 HTTP/HTTPSPOST。如果你希望 Mailgun甚至可以发布通知，你需要在控制面板的Webhooks选项卡中提供回调 URL。Webhook 位于域级别，它允许你使用域下拉选择器为每个域提供唯一的URL。</p><ul><li>注意：如果要包含 HTTPS 端点，则必须使用受信任的CA（证书颁发机构）签名的 SSL 证书（而不是自签名证书）对其进行配置。</li></ul><p>你可以阅读有关下面相应部分中发布的数据的更多信息（跟踪打开、跟踪点击、跟踪退订、跟踪垃圾邮件投诉、跟踪失败、跟踪递送）。我们建议使用<ahref="http://bin.mailgun.net/" class="uri">http://bin.mailgun.net/</a>创建临时 URL 来测试和调试你的 Webhooks。</p><p>对于 Webhook POST，Mailgun会侦听来自服务器的以下代码并做出相应反应：</p><ol type="1"><li>如果 Mailgun 收到 200（成功）代码，它将确定 Webhook POST成功并且不会重试。</li><li>如果 Mailgun 收到 406（不可接受）代码，Mailgun 将确定 POST被拒绝并且不会重试。</li><li>对于任何其他代码，Mailgun 将根据以下 Webhooks 的时间表重试POSTing（送达通知除外）。</li></ol><p>如果你的应用程序无法处理 Webhook 请求，但你没有返回 406错误代码，Mailgun 将在 8小时内按以下时间间隔重试（送达通知除外），然后停止尝试：5 分钟、10分钟、15 分钟分钟、1小时、2小时、4小时。</p><h3 id="检测机器人">3.4 检测机器人</h3><p>Mailgun 使用跟踪像素和 URL重定向来跟踪用户何时打开邮件并单击电子邮件中的链接。但是，有各种第三方自动化系统会自动打开并发送消息并跟踪链接以进行病毒扫描和用户活动混淆，例如AppleMail Privacy Protection。</p><p>由于自动化系统可能会影响打开和点击跟踪的准确性，因此 Mailgun将尝试检测其中一个系统何时检索跟踪像素或单击链接。当检测到机器人打开或单击电子邮件中的链接时，Mailgun将通过打开/单击事件中的 client-info.bot 字段进行指示。</p><pre class="language-MIME" data-language="MIME"><code class="language-MIME">&#123;    &quot;client-info&quot;: &#123;      &quot;client-name&quot;: &quot;unknown&quot;,      &quot;client-type&quot;: &quot;unknown&quot;,      &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0&quot;,      &quot;device-type&quot;: &quot;unknown&quot;,      &quot;client-os&quot;: &quot;unknown&quot;,      &quot;bot&quot;: &quot;apple&quot;    &#125;,    &quot;tags&quot;: [],    &quot;timestamp&quot;: 1652883435.279025,    &quot;recipient&quot;: &quot;bot@apple.com&quot;,    &quot;geolocation&quot;: &#123;      &quot;region&quot;: &quot;Unknown&quot;,      &quot;country&quot;: &quot;US&quot;,      &quot;city&quot;: &quot;Unknown&quot;    &#125;,    &quot;event&quot;: &quot;opened&quot;,&#125;</code></pre><p>bot 字段可以具有以下可能值之一：</p><ol type="1"><li>apple:表示 Apple MPP 机器人</li><li>gmail:表示 Gmail 机器人</li><li>generic:表示未知机器人（很可能是防火墙或防病毒扫描）</li><li>(empty):如果机器人字段为空，则表示未检测到机器人。</li></ol><h2 id="接收转发和存储消息">4. 接收、转发和存储消息</h2><p>Mailgun 将允许你通过路由接收电子邮件，该路由将执行以下操作：</p><ul><li>将电子邮件转发到不同的电子邮件地址</li><li>将电子邮件中的数据发布到 URL</li><li>暂时存储电子邮件以供后续通过 GET 请求检索</li></ul><p>更多信息请参考<ahref="https://documentation.mailgun.com/docs/mailgun/user-manual/receive-forward-store/">接收、转发和存储消息</a></p><h2 id="测试脚本">测试脚本</h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> json<span class="token keyword">import</span> os<span class="token keyword">import</span> random<span class="token keyword">import</span> re<span class="token keyword">import</span> requests<span class="token keyword">import</span> threading<span class="token keyword">from</span> jinja2 <span class="token keyword">import</span> Environment<span class="token keyword">from</span> jinja2 <span class="token keyword">import</span> FileSystemLoader<span class="token keyword">from</span> jinja2 <span class="token keyword">import</span> select_autoescape<span class="token keyword">def</span> <span class="token function">send_email</span><span class="token punctuation">(</span>        to_addresses<span class="token punctuation">,</span>        subject<span class="token punctuation">,</span>        text<span class="token punctuation">,</span>        tag<span class="token punctuation">,</span>        html_template<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        attachments<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        tracking_opens<span class="token operator">=</span><span class="token string">"no"</span><span class="token punctuation">,</span>        tracking_clicks<span class="token operator">=</span><span class="token string">"no"</span><span class="token punctuation">,</span>        from_address<span class="token operator">=</span>os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"MAILGUN_FROM_ADDRESS"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"MAILGUN_API_KEY"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>    domain <span class="token operator">=</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"MAILGUN_DOMAIN"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> api_key <span class="token keyword">or</span> <span class="token keyword">not</span> domain <span class="token keyword">or</span> <span class="token keyword">not</span> from_address<span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">None</span>    url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"https://api.mailgun.net/v3/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>domain<span class="token punctuation">&#125;</span></span><span class="token string">/messages"</span></span>    auth <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"api"</span><span class="token punctuation">,</span> api_key<span class="token punctuation">)</span>    data <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">"from"</span><span class="token punctuation">:</span> from_address<span class="token punctuation">,</span>        <span class="token string">"to"</span><span class="token punctuation">:</span> to_addresses<span class="token punctuation">,</span>        <span class="token string">"subject"</span><span class="token punctuation">:</span> subject<span class="token punctuation">,</span>        <span class="token string">"text"</span><span class="token punctuation">:</span> text<span class="token punctuation">,</span>        <span class="token string">"o:tracking-opens"</span><span class="token punctuation">:</span> tracking_opens<span class="token punctuation">,</span>        <span class="token string">"o:tracking-clicks"</span><span class="token punctuation">:</span> tracking_clicks<span class="token punctuation">,</span>        <span class="token string">"o:tag"</span><span class="token punctuation">:</span> tag<span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span>    <span class="token comment"># 如果有HTML内容，添加到data中</span>    <span class="token keyword">if</span> html_template<span class="token punctuation">:</span>        data<span class="token punctuation">[</span><span class="token string">"html"</span><span class="token punctuation">]</span> <span class="token operator">=</span> html_template    <span class="token comment"># 处理附件</span>    files <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> attachments<span class="token punctuation">:</span>        <span class="token comment"># 使用上下文管理器打开文件</span>        files <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"attachment"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>attachment<span class="token punctuation">,</span> <span class="token builtin">open</span><span class="token punctuation">(</span>attachment<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                 <span class="token keyword">for</span> attachment <span class="token keyword">in</span> attachments<span class="token punctuation">]</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        <span class="token comment"># 发送请求并检查错误</span>        response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> auth<span class="token operator">=</span>auth<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">,</span> files<span class="token operator">=</span>files<span class="token punctuation">)</span>        response<span class="token punctuation">.</span>raise_for_status<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 这将抛出异常，如果响应状态码表明一个错误</span>    <span class="token keyword">except</span> requests<span class="token punctuation">.</span>exceptions<span class="token punctuation">.</span>RequestException <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token comment"># 对于所有请求相关的异常</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Error sending email: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">None</span>    <span class="token keyword">finally</span><span class="token punctuation">:</span>        <span class="token comment"># 无论请求成功还是抛出异常，确保所有打开的文件都被关闭</span>        <span class="token keyword">for</span> _<span class="token punctuation">,</span> file_tuple <span class="token keyword">in</span> files<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>file_tuple<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                file_tuple<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token keyword">def</span> <span class="token function">get_email_id</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> response <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">None</span>    response_dict <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>    <span class="token comment"># 如果存在id，返回id</span>    <span class="token keyword">if</span> <span class="token string">"id"</span> <span class="token keyword">in</span> response_dict<span class="token punctuation">:</span>        email_id <span class="token operator">=</span> response_dict<span class="token punctuation">[</span><span class="token string">"id"</span><span class="token punctuation">]</span>        email_id_cleaned <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r"[&lt;>]"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">,</span> email_id<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        email_id_cleaned <span class="token operator">=</span> <span class="token boolean">None</span>    <span class="token keyword">return</span> email_id_cleaned<span class="token comment"># 发送注册验证码</span><span class="token keyword">def</span> <span class="token function">send_email_register_code</span><span class="token punctuation">(</span>to_addresses<span class="token punctuation">,</span> code<span class="token punctuation">,</span> created_at<span class="token punctuation">)</span><span class="token punctuation">:</span>    subject <span class="token operator">=</span> <span class="token string">"注册验证码"</span>    text <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"你的注册验证码为：\n"</span></span>            <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>code<span class="token punctuation">&#125;</span></span><span class="token string">\n"</span></span>            <span class="token string-interpolation"><span class="token string">f"有效期为5分钟\n"</span></span>            <span class="token string-interpolation"><span class="token string">f"请注意，如果这不是你本人的操作，请忽略并关闭此邮件。\n"</span></span><span class="token punctuation">)</span>    tag <span class="token operator">=</span> <span class="token string">"register"</span>  <span class="token comment"># 标签</span>    html_template <span class="token operator">=</span> render_template_direct<span class="token punctuation">(</span><span class="token string">"email/register.html"</span><span class="token punctuation">,</span> code<span class="token operator">=</span>code<span class="token punctuation">)</span>    <span class="token comment"># 发送邮件</span>    response <span class="token operator">=</span> send_email<span class="token punctuation">(</span>to_addresses<span class="token punctuation">,</span> subject<span class="token punctuation">,</span> text<span class="token punctuation">,</span> tag<span class="token punctuation">,</span> html_template<span class="token punctuation">)</span>    email_id <span class="token operator">=</span> get_email_id<span class="token punctuation">(</span>response<span class="token punctuation">)</span>    <span class="token keyword">return</span> email_id<span class="token comment"># jinja2模板渲染</span><span class="token keyword">def</span> <span class="token function">render_template_direct</span><span class="token punctuation">(</span>template_name<span class="token punctuation">,</span> <span class="token operator">**</span>context<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 设置模板环境</span>    env <span class="token operator">=</span> Environment<span class="token punctuation">(</span>        loader<span class="token operator">=</span>FileSystemLoader<span class="token punctuation">(</span><span class="token string">"templates"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        autoescape<span class="token operator">=</span>select_autoescape<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"html"</span><span class="token punctuation">,</span> <span class="token string">"xml"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    template <span class="token operator">=</span> env<span class="token punctuation">.</span>get_template<span class="token punctuation">(</span>template_name<span class="token punctuation">)</span>    <span class="token keyword">return</span> template<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token operator">**</span>context<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    code <span class="token operator">=</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    send_email_register_code<span class="token punctuation">(</span><span class="token string">"xx@xx"</span><span class="token punctuation">,</span> code<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>网站搭建中一些记录</title>
      <link href="/2024/03/03/Web/tracks/"/>
      <url>/2024/03/03/Web/tracks/</url>
      
        <content type="html"><![CDATA[<h2 id="前端">1 前端</h2><h3 id="id和name的区别">1.1 id和name的区别</h3><p><strong>id</strong></p><ol type="1"><li>在一个HTML文档中，每个元素的id属性必须是唯一的。这意味着同一个页面上不能有两个元素具有相同的id值。</li><li>id属性主要用于JavaScript和CSS。在JavaScript中，你可以使用document.getElementById()来获取具有特定id的元素。在CSS中，你可以使用#后跟id值来指定样式规则应用于哪个元素。</li><li>id属性还可用于创建页面内的链接锚点。通过在URL后添加#和元素的id，可以直接跳转到页面上具有该id的元素。</li></ol><p><strong>name</strong></p><p>name属性主要用于HTML表单元素。当表单提交时，name属性的值作为提交数据的键，元素的值作为提交数据的值。</p><h2 id="后端">2 后端</h2><h3 id="获取post数据">2.1 获取post数据</h3><ol type="1"><li>使用request.json.get(‘time’) 如果发送的是JSON数据</li><li>使用request.form.get(‘time’) 如果发送的是表单数据</li><li>使用request.args.get(‘time’) 如果发送的是URL参数</li></ol><h2 id="综合">3 综合</h2><h3 id="目录问题">3.1 目录问题</h3><p><strong>在后端使用的是python</strong></p><ol type="1"><li>“/”： 指的是系统根目录，即磁盘根目录</li><li>“./”： 指的是当前文件目录，同不加任何符号(./xxx和xxx是一样的)</li><li>“../”：指的是当前文件的上一级目录</li></ol><p><strong>在前端</strong></p><ol type="1"><li>“/”： 从域的根开始查找资源</li><li>“./”：从当前页面的相对位置查找资源</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网站搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python系列之requests</title>
      <link href="/2024/02/28/Python/requests/"/>
      <url>/2024/02/28/Python/requests/</url>
      
        <content type="html"><![CDATA[<h1 id="requests">requests</h1><p><code>requests</code> 是一个用于发送 HTTP 请求的 Python库。它提供了简单易用的 API，可以轻松地与 Web 服务进行交互。 它支持GET、POST、PUT、DELETE 等多种 HTTP 方法，并且可以处理Cookies、会话、文件上传等常见的 Web 操作。</p><h2 id="安装">1 安装</h2><pre class="language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> requests</code></pre><h2 id="使用">2 使用</h2><h3 id="get-请求">2.1 GET 请求</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api.example.com/data"</span>  <span class="token comment"># 替换为实际的 API URL</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>response_time <span class="token operator">=</span> response<span class="token punctuation">.</span>elapsed<span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取响应时间</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    data <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将响应内容解析为 JSON 格式</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"请求失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"响应时间：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response_time<span class="token punctuation">&#125;</span></span><span class="token string"> 秒"</span></span><span class="token punctuation">)</span></code></pre><h3 id="post-请求">2.2 POST 请求</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api.example.com/data"</span>  <span class="token comment"># 替换为实际的 API URL</span>data <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"key1"</span><span class="token punctuation">:</span> <span class="token string">"value1"</span><span class="token punctuation">,</span>    <span class="token string">"key2"</span><span class="token punctuation">:</span> <span class="token string">"value2"</span><span class="token punctuation">&#125;</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> json<span class="token operator">=</span>data<span class="token punctuation">)</span>  <span class="token comment"># 使用 json 参数自动将数据转换为 JSON 格式</span>response_time <span class="token operator">=</span> response<span class="token punctuation">.</span>elapsed<span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取响应时间</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">201</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"数据已成功发送"</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"请求失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"响应内容：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>text<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"响应时间：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response_time<span class="token punctuation">&#125;</span></span><span class="token string"> 秒"</span></span><span class="token punctuation">)</span></code></pre><h3 id="处理-cookies">2.3 处理 Cookies</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api.example.com/login"</span>  <span class="token comment"># 替换为实际的登录 URL</span>data <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"username"</span><span class="token punctuation">:</span> <span class="token string">"your_username"</span><span class="token punctuation">,</span>    <span class="token string">"password"</span><span class="token punctuation">:</span> <span class="token string">"your_password"</span><span class="token punctuation">&#125;</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span>response_time <span class="token operator">=</span> response<span class="token punctuation">.</span>elapsed<span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取响应时间</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    <span class="token comment"># 登录成功，获取 Cookies</span>    cookies <span class="token operator">=</span> response<span class="token punctuation">.</span>cookies    <span class="token comment"># 使用 Cookies 发送后续请求</span>    url <span class="token operator">=</span> <span class="token string">"https://api.example.com/user_data"</span>    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> cookies<span class="token operator">=</span>cookies<span class="token punctuation">)</span>    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>        user_data <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>user_data<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"请求失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"登录失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></code></pre><h3 id="上传文件">2.4 上传文件</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api.example.com/upload"</span>  <span class="token comment"># 替换为实际的文件上传 URL</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"example.txt"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token builtin">file</span><span class="token punctuation">:</span>    files <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">"file"</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">"filename.txt"</span><span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">,</span> <span class="token string">"text/plain"</span><span class="token punctuation">)</span>  <span class="token comment"># (文件名, 文件对象, MIME类型)</span>    <span class="token punctuation">&#125;</span>    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> files<span class="token operator">=</span>files<span class="token punctuation">)</span>    response_time <span class="token operator">=</span> response<span class="token punctuation">.</span>elapsed<span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取响应时间</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"文件上传成功"</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"请求失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"响应时间：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response_time<span class="token punctuation">&#125;</span></span><span class="token string"> 秒"</span></span><span class="token punctuation">)</span></code></pre><h3 id="设置请求头">2.5 设置请求头</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api.example.com/data"</span>  <span class="token comment"># 替换为实际的 API URL</span>headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"Authorization"</span><span class="token punctuation">:</span> <span class="token string">"Bearer your_token_here"</span><span class="token punctuation">,</span>  <span class="token comment"># 替换为实际的认证令牌</span>    <span class="token string">"Content-Type"</span><span class="token punctuation">:</span> <span class="token string">"application/json"</span><span class="token punctuation">,</span>    <span class="token string">"User-Agent"</span><span class="token punctuation">:</span> <span class="token string">"My Python App"</span><span class="token punctuation">&#125;</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>response_time <span class="token operator">=</span> response<span class="token punctuation">.</span>elapsed<span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取响应时间</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    data <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"请求失败，状态码：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"响应时间：</span><span class="token interpolation"><span class="token punctuation">&#123;</span>response_time<span class="token punctuation">&#125;</span></span><span class="token string"> 秒"</span></span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu桌面图标</title>
      <link href="/2023/11/21/Linux/DesktopIcon/"/>
      <url>/2023/11/21/Linux/DesktopIcon/</url>
      
        <content type="html"><![CDATA[<h1 id="创建一个启动器.desktop文件">创建一个启动器（.desktop文件）</h1><p>将此文件保存到 ~/.local/share/applications/ 目录下</p><h2 id="空白文件">空白文件</h2><pre class="language-conf" data-language="conf"><code class="language-conf">[Desktop Entry]Version&#x3D;1.0Type&#x3D;ApplicationName&#x3D;Icon&#x3D;Exec&#x3D;&quot; &quot; %fComment&#x3D;Keywords&#x3D;;Categories&#x3D;;Terminal&#x3D;falseStartupWMClass&#x3D;StartupNotify&#x3D;true</code></pre><h3 id="示例">示例</h3><pre class="language-conf" data-language="conf"><code class="language-conf">[Desktop Entry]Version&#x3D;1.0Type&#x3D;ApplicationName&#x3D;PyCharm Professional EditionIcon&#x3D;&#x2F;home&#x2F;xx&#x2F;software&#x2F;pycharm-2023.2.4&#x2F;bin&#x2F;pycharm.svgExec&#x3D;&quot;&#x2F;home&#x2F;xx&#x2F;software&#x2F;pycharm-2023.2.4&#x2F;bin&#x2F;pycharm.sh&quot; %fComment&#x3D;Python IDE for Professional DevelopersKeywords&#x3D;python;ide;development;coding;programming;Categories&#x3D;Development;IDE;Terminal&#x3D;falseStartupWMClass&#x3D;jetbrains-pycharmStartupNotify&#x3D;true</code></pre><h4 id="词条说明">词条说明</h4><ol type="1"><li>Version：桌面条目规范的版本。</li><li>Type：指定这个桌面条目的类型。</li><li>Name：指定应用程序的名称，即用户在菜单或桌面上看到的名称。</li><li>Icon：指定应用程序的图标路径。</li><li>Exec：指定启动应用程序的命令。</li><li>Comment：提供有关应用程序的额外描述。</li><li>Keywords：提供一组关键词，帮助在桌面环境的应用程序菜单中更容易地搜索到这个应用。</li><li>Categories：定义了这个应用程序属于哪些分类。 AudioVideo -音频和视频相关的应用程序。 Audio - 只和音频相关的应用程序。 Video -只和视频相关的应用程序。 Development - 开发工具，如IDEs、编译器等。 IDE- 集成开发环境 Education - 教育相关的软件。 Game - 游戏。 Graphics -图形处理软件，如图像编辑器、查看器等。 Network -网络相关的应用程序，如浏览器、聊天客户端等。 Office -办公软件，如文字处理、表格制作等。 Science -科学相关软件，如数学、天文学等。 Settings - 系统设置或配置工具。 System- 系统工具或实用程序。 Utility - 小工具或辅助应用程序。</li><li>Terminal：指定启动应用程序时是否需要打开一个终端窗口。</li><li>StartupWMClass：指定窗口管理器类（WMClass），用于将应用程序窗口与这个 .desktop 文件关联。任务栏中显示</li><li>StartupNotify：启动这个应用程序时桌面环境会显示一个通知，通常是一种视觉反馈，表明应用正在启动。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Android-主线程（UI线程）</title>
      <link href="/2023/10/05/Android/MainThread/"/>
      <url>/2023/10/05/Android/MainThread/</url>
      
        <content type="html"><![CDATA[<h1 id="android-主线程ui线程">Android 主线程（UI线程）</h1><p>本文介绍了Android主线程（UI线程）的概念，以及如何在主线程上执行操作。</p><h2 id="主线程ui线程">主线程（UI线程）</h2><p>在Android应用中，主线程是负责管理用户界面的线程。所有的UI更新（如设置文本视图的文本、更改视图的可见性等）都必须在这个线程上执行。这是因为Android的UI工具包不是线程安全的，所以从其他线程直接操作UI可能导致应用崩溃或不可预见的行为。</p><h2 id="后台线程非ui线程">后台线程（非UI线程）</h2><p>用于执行耗时操作，如网络请求、数据库操作等，以避免阻塞主线程。在主线程上执行这些耗时操作会导致应用无响应。</p><ol type="1"><li>数据处理和转换</li><li>写入或读取文件（非UI操作）</li><li>发送网络请求（如果有特殊的实现，需要在特定线程上执行）</li><li>记录或分析数据</li></ol><h2id="为什么需要在主线程上执行ui操作">为什么需要在主线程上执行UI操作？</h2><p>如果在后台线程上直接更新UI，可能会遇到以下问题：</p><ol type="1"><li>应用崩溃：由于AndroidUI组件不支持从非UI线程访问，直接在后台线程上操作UI可能会导致应用崩溃。</li><li>不一致和不可预测的行为：UI可能不会按照预期更新，或者完全不更新。</li></ol><h2 id="如何在主线程上执行操作">如何在主线程上执行操作？</h2><p>在Android中，有几种方法可以确保代码在主线程上执行</p><h3 id="runonuithread方法">1 runOnUiThread方法</h3><h4 id="activity-中">1.1 Activity 中</h4><pre class="language-java" data-language="java"><code class="language-java"><span class="token function">runOnUiThread</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="fragment-中">2.2 Fragment 中</h4><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">isAdded</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token function">getActivity</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token function">getActivity</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">runOnUiThread</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>        <span class="token comment">// UI操作</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span></code></pre><h3 id="handler">2 Handler</h3><p>Handler 绑定到主线程的 Looper 可以用来在主线程上执行代码。 这种方法在Activity, Fragment, 或任何其他上下文中都很有用。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">new</span> <span class="token class-name">Handler</span><span class="token punctuation">(</span><span class="token class-name">Looper</span><span class="token punctuation">.</span><span class="token function">getMainLooper</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">// 加入延迟</span><span class="token keyword">new</span> <span class="token class-name">Handler</span><span class="token punctuation">(</span><span class="token class-name">Looper</span><span class="token punctuation">.</span><span class="token function">getMainLooper</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">postDelayed</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// 非UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>另外如果非主线程使用 Handler</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">new</span> <span class="token class-name">Handler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">// 加入延迟</span><span class="token keyword">new</span> <span class="token class-name">Handler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">postDelayed</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// 非UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="view的-post-方法">3 View的 post 方法</h3><p>任何继承自 View 的类，如 Button 或 TextView，都有一个 post(Runnableaction) 方法，可以用来在主线程上执行代码。</p><pre class="language-java" data-language="java"><code class="language-java">myView<span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="viewmodel的-livedata">4 ViewModel的 LiveData</h3><p>当使用 LiveData 与 ViewModel 结合时，观察 LiveData的代码默认在主线程上执行。因此，更新 LiveData通常会导致在主线程上执行UI操作。</p><pre class="language-java" data-language="java"><code class="language-java">myLiveData<span class="token punctuation">.</span><span class="token function">observe</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">,</span> data <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// UI操作</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 安卓 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python系列之pyinstaller</title>
      <link href="/2023/04/20/Python/py2exe/"/>
      <url>/2023/04/20/Python/py2exe/</url>
      
        <content type="html"><![CDATA[<h1 id="pyinstaller">PyInstaller</h1><p>PyInstaller 是一个将 Python 程序打包成独立可执行文件的工具，支持Windows、Linux 和 macOS 等平台。它可以将Python解释器和所有依赖的库打包到一个单独的可执行文件中，使得用户无需安装Python 环境即可运行程序。</p><p>在 Python 中打包成 EXE 且要求无弹窗、体积小，推荐使用 PyInstaller +UPX 压缩。</p><h2 id="安装必要工具">1. 安装必要工具</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 安装 PyInstaller</span>pip <span class="token function">install</span> pyinstaller</code></pre><p>下载 <a href="https://github.com/upx/upx/releases">UPX</a>并解压到任意目录</p><h2 id="打包命令">2. 打包命令</h2><pre class="language-bash" data-language="bash"><code class="language-bash">pyinstaller <span class="token parameter variable">--onefile</span> <span class="token parameter variable">--noconsole</span> <span class="token parameter variable">--name</span> app <span class="token parameter variable">--icon</span><span class="token operator">=</span>logo.ico --add-data<span class="token operator">=</span><span class="token string">"xxx;."</span> --upx-dir<span class="token operator">=</span><span class="token string">"D:\upx-5.0.0-win64"</span> <span class="token parameter variable">--clean</span> --exclude-module<span class="token operator">=</span>tests app.py</code></pre><p><strong>参数说明</strong>：</p><ul><li><code>--onefile</code>：生成单个可执行文件</li><li><code>--noconsole</code>：禁用控制台窗口（GUI程序必选）</li><li><code>--name</code>：指定生成的可执行文件名称</li><li><code>--icon</code>：指定图标文件路径</li><li><code>--add-data</code>：添加额外数据文件（如图标等）</li><li><code>--upx-dir</code>：指定 UPX 路径进行二进制压缩（体积减少30-50%）</li><li><code>--clean</code>：清理临时文件</li><li><code>--exclude-module</code>：排除不需要的模块</li></ul><p><strong>注意事项</strong>:</p><ol type="1"><li>若图标仅作为EXE文件的显示图标使用，无需<code>--add-data</code>参数。</li><li><code>--add-data</code>参数在Windows和Linux下的分隔符不同：<ul><li>Windows：使用分号 <code>;</code> 作为路径分隔符</li><li>Linux：使用冒号 <code>:</code> 作为路径分隔符</li></ul></li><li>可使用更小的 Python 基础环境（推荐 Python3.11+，自带更小的嵌入包）</li></ol><h2 id="打包示例">3. 打包示例</h2><h3 id="打包带图标的-exe-文件">3.1. 打包带图标的 EXE 文件</h3><pre class="language-bash" data-language="bash"><code class="language-bash">pyinstaller <span class="token parameter variable">--onefile</span> <span class="token parameter variable">--noconsole</span> <span class="token parameter variable">--name</span> app <span class="token parameter variable">--icon</span><span class="token operator">=</span>logo.ico --upx-dir<span class="token operator">=</span><span class="token string">"D:\upx-5.0.0-win64"</span> <span class="token parameter variable">--clean</span> app.py</code></pre><h3 id="打包带数据文件的-exe-文件">3.2. 打包带数据文件的 EXE 文件</h3><pre class="language-bash" data-language="bash"><code class="language-bash">pyinstaller <span class="token parameter variable">--onefile</span> <span class="token parameter variable">--noconsole</span> <span class="token parameter variable">--name</span> app <span class="token parameter variable">--icon</span><span class="token operator">=</span>logo.ico --add-data<span class="token operator">=</span><span class="token string">"logo.ico;."</span> --upx-dir<span class="token operator">=</span><span class="token string">"D:\upx-5.0.0-win64"</span> <span class="token parameter variable">--clean</span> app.py</code></pre><p>然后在代码中使用以下方式获取资源文件路径：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 获取当前可执行文件的目录</span><span class="token keyword">import</span> sys<span class="token keyword">import</span> os<span class="token comment"># 当用 PyInstaller 打包时，如果程序被“冻结”（也就是被打包成一个文件），sys.frozen 的值为 True，否则为 False。</span><span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>sys<span class="token punctuation">,</span> <span class="token string">'frozen'</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 当前目录为当前可执行文件所在目录</span>    application_path <span class="token operator">=</span> sys<span class="token punctuation">.</span>_MEIPASS<span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token comment"># 当前目录为当前脚本所在目录</span>    application_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token comment"># 获取资源文件的路径</span>icon_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>application_path<span class="token punctuation">,</span> <span class="token string">'example.ico'</span><span class="token punctuation">)</span></code></pre><h2 id="替代方案nuitka">4. 替代方案Nuitka</h2><p><a href="https://github.com/Nuitka/Nuitka">Nuitka</a> 是一个 Python到 C++ 的编译器，可以将 Python代码编译成独立的可执行文件。它的优点是可以生成更高效的代码，打包后的程序运行速度更快。但是Nuitka 的使用相对复杂，需要安装 C++ 编译器，并且编译速度较慢。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Conda</title>
      <link href="/2023/03/22/TechnicalNotes/Conda/"/>
      <url>/2023/03/22/TechnicalNotes/Conda/</url>
      
        <content type="html"><![CDATA[<h2 id="conda安装">1 Conda安装</h2><p>推荐使用<ahref="https://www.anaconda.com/download">Miniconda</a>。</p><h2 id="conda常用命令">2 Conda常用命令</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 创建环境</span>conda create <span class="token parameter variable">-n</span> myenv <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span><span class="token comment"># 激活环境</span>conda activate myenv<span class="token comment"># 关闭环境</span>conda deactivate<span class="token comment"># 删除环境</span>conda remove <span class="token parameter variable">-n</span> myenv <span class="token parameter variable">--all</span></code></pre><h2 id="conda更新">3 Conda更新</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 更新conda</span>conda update conda<span class="token comment"># 搜索Python版本</span>conda search python<span class="token comment"># 安装指定版本的Python</span>conda <span class="token function">install</span> <span class="token assign-left variable">python</span><span class="token operator">=</span>x.x.x<span class="token comment"># 更新Python到最新版本</span>conda update python</code></pre><h2 id="conda安装包">4 Conda安装包</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 搜索包</span>conda search numpy<span class="token comment"># 安装包</span>conda <span class="token function">install</span> numpy<span class="token comment"># 卸载包</span>conda remove numpy</code></pre><h2 id="conda查看信息">5 Conda查看信息</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 查看已安装的包</span>conda list<span class="token comment"># 查看所有环境</span>conda <span class="token function">env</span> list<span class="token comment"># 查看当前环境信息</span>conda info</code></pre><h2 id="conda导出环境">6 Conda导出环境</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 导出环境到文件</span>conda <span class="token function">env</span> <span class="token builtin class-name">export</span> <span class="token operator">></span> environment.yml<span class="token comment"># 从文件创建环境</span>conda <span class="token function">env</span> create <span class="token parameter variable">-f</span> environment.yml</code></pre><h2 id="conda清理">7 Conda清理</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 清理未使用的包和缓存</span>conda clean <span class="token parameter variable">-a</span><span class="token comment"># 清理索引缓存</span>conda clean <span class="token parameter variable">-i</span><span class="token comment"># 清理包缓存</span>conda clean <span class="token parameter variable">-p</span><span class="token comment"># 清理tar包缓存</span>conda clean <span class="token parameter variable">-t</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PowerShell命令</title>
      <link href="/2023/03/01/TechnicalNotes/Commands/"/>
      <url>/2023/03/01/TechnicalNotes/Commands/</url>
      
        <content type="html"><![CDATA[<h2 id="cdn下载文件">1 CDN下载文件</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-o</span> jquery.min.js https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js<span class="token comment"># -o: 指定输出文件地址和名称</span></code></pre><h2 id="windows-powershell-更新">2 Windows PowerShell 更新</h2><h3 id="显示当前的-powershell-版本号">显示当前的 PowerShell 版本号</h3><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$PSVersionTable</span>.PSVersion</code></pre><h3id="查看最新版本官方正版的是-id-带有-microsoft-前缀">查看最新版本（官方正版的是ID 带有 Microsoft 前缀）</h3><pre class="language-bash" data-language="bash"><code class="language-bash">winget search powershell</code></pre><h3 id="使用-winget-安装-powershell">使用 Winget 安装 PowerShell</h3><pre class="language-bash" data-language="bash"><code class="language-bash">winget <span class="token function">install</span> <span class="token parameter variable">--id</span> Microsoft.Powershell <span class="token parameter variable">--source</span> winget</code></pre>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Android-知识合集</title>
      <link href="/2021/11/26/Android/AndroidKnowledgeCollection/"/>
      <url>/2021/11/26/Android/AndroidKnowledgeCollection/</url>
      
        <content type="html"><![CDATA[<h1 id="android知识合集">Android知识合集</h1><p>这里是Android知识合集，包含了Android开发的一些基础知识。</p><h2 id="关于组件">关于组件</h2><h3 id="关于fragment">关于Fragment</h3><ol type="1"><li><p>获取上下文</p><p><pre class="language-java" data-language="java"><code class="language-java"><span class="token function">getContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 获取的是 <span class="token class-name">Fragment</span> 的上下文，需要判断是否为空<span class="token function">requireContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 获取的是 <span class="token class-name">Fragment</span> 的上下文，必须有内容，为空则报错。</code></pre></p></li><li><p>获取依附的 Activity</p><p><pre class="language-java" data-language="java"><code class="language-java"><span class="token function">getActivity</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 获取的是 <span class="token class-name">Fragment</span> 依附的 <span class="token class-name">Activity</span>，需要判断是否为空<span class="token function">requireActivity</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 获取的是 <span class="token class-name">Fragment</span> 依附的 <span class="token class-name">Activity</span>，必须有内容，为空则报错。</code></pre></p></li></ol><h2 id="关于控件">关于控件</h2><h3 id="关于checkbox">关于CheckBox</h3><p>CheckBox按钮响应事件</p><h4 id="使用-oncheckedchangelistener">1. 使用OnCheckedChangeListener</h4><p>使用OnCheckedChangeListener 可以知道 CheckBox 的选中状态何时更改。这是最常用的方法，尤其是关心 CheckBox 是否被选中时。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token class-name">CheckBox</span> checkBox <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">CheckBox</span><span class="token punctuation">)</span> <span class="token function">findViewById</span><span class="token punctuation">(</span><span class="token class-name">R</span><span class="token punctuation">.</span>id<span class="token punctuation">.</span>your_checkbox_id<span class="token punctuation">)</span><span class="token punctuation">;</span>checkBox<span class="token punctuation">.</span><span class="token function">setOnCheckedChangeListener</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">CompoundButton<span class="token punctuation">.</span>OnCheckedChangeListener</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onCheckedChanged</span><span class="token punctuation">(</span><span class="token class-name">CompoundButton</span> buttonView<span class="token punctuation">,</span> <span class="token keyword">boolean</span> isChecked<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>isChecked<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token comment">// 处理选中状态</span>        <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>            <span class="token comment">// 处理非选中状态</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="使用-onclicklistener">2. 使用 OnClickListener</h4><p>也可以使用 OnClickListener，在用户点击 CheckBox时触发事件，而不仅仅是在其状态改变时。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token class-name">CheckBox</span> checkBox <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">CheckBox</span><span class="token punctuation">)</span> <span class="token function">findViewById</span><span class="token punctuation">(</span><span class="token class-name">R</span><span class="token punctuation">.</span>id<span class="token punctuation">.</span>your_checkbox_id<span class="token punctuation">)</span><span class="token punctuation">;</span>checkBox<span class="token punctuation">.</span><span class="token function">setOnClickListener</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">View<span class="token punctuation">.</span>OnClickListener</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onClick</span><span class="token punctuation">(</span><span class="token class-name">View</span> v<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token comment">// 检查是否选中</span>        <span class="token keyword">boolean</span> checked <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token class-name">CheckBox</span><span class="token punctuation">)</span> v<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">isChecked</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>checked<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token comment">// 处理选中状态</span>        <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>            <span class="token comment">// 处理非选中状态</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h2 id="关于androidmanifest.xml">关于AndroidManifest.xml</h2><ol type="1"><li>android:exported=“true”: 允许其他应用访问该组件</li></ol>]]></content>
      
      
      <categories>
          
          <category> 安卓 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Android-ROOM组件</title>
      <link href="/2021/11/05/Android/Room/"/>
      <url>/2021/11/05/Android/Room/</url>
      
        <content type="html"><![CDATA[<h1 id="room">ROOM</h1><p>本文介绍了 ROOM 数据库的基本概念和使用方法。</p><h2 id="room-数据库">ROOM 数据库</h2><p>一个database可以同时存在： N个@DAO, N个@Entity, 1个@Database,1个DBEngine</p><h2 id="查询回调的两种方式">查询回调的两种方式</h2><p>Java 8或更高版本，推荐使用 Consumer 接口的方法。</p><h3 id="使用-consumer-接口">使用 Consumer 接口</h3><p>Consumer<AccountData> 接口是 Java 8 中引入的函数式接口的一部分。这种方式允许直接在调用方法时定义对结果的处理逻辑，通常是通过一个 lambda表达式或方法引用。</p><p>特点:</p><p>灵活性: 直接在调用点定义如何处理数据。 简洁性: 使用 lambda表达式可以使代码更加简洁和直观。 依赖于 Java 8: 依赖于 Java 8的特性，如函数式接口和 lambda 表达式。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">queryAllDatas</span><span class="token punctuation">(</span><span class="token class-name">DataQueryCallback</span> callback<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        databaseExecutor<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>            <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Data</span><span class="token punctuation">></span></span> allDatas <span class="token operator">=</span> dataDao<span class="token punctuation">.</span><span class="token function">getAllDatas</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment">// 确保回调在主线程上执行（如果涉及UI操作）</span>            <span class="token keyword">new</span> <span class="token class-name">Handler</span><span class="token punctuation">(</span><span class="token class-name">Looper</span><span class="token punctuation">.</span><span class="token function">getMainLooper</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> callback<span class="token punctuation">.</span><span class="token function">onDataRetrieved</span><span class="token punctuation">(</span>allDatas<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment">// 直接在后台线程处理数据</span>            callback<span class="token punctuation">.</span><span class="token function">onDataRetrieved</span><span class="token punctuation">(</span>allDatas<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token comment">// 另外的接口文件</span><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">DataQueryCallback</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">void</span> <span class="token function">onDataRetrieved</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Data</span><span class="token punctuation">></span></span> dataList<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token comment">// 使用</span><span class="token class-name">DBEngine</span> dbEngine <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DBEngine</span><span class="token punctuation">(</span><span class="token function">requireContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>dbEngine<span class="token punctuation">.</span><span class="token function">queryAllDatas</span><span class="token punctuation">(</span>dataList <span class="token operator">-></span> <span class="token punctuation">&#123;</span>    <span class="token comment">// 在此处处理和使用 dataList       </span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="使用自定义回调接口">使用自定义回调接口</h3><p>定义一个自定义的回调接口(DataQueryCallback)，并在执行数据库操作的类中使用它。这种方式更传统，通常在 Java 8 之前的代码中更常见。</p><p>特点:</p><p>结构化: 通过定义一个专用的回调接口，使得代码的结构更加清晰。可重用性:如果多个地方需要使用相同的回调逻辑，这种方式可以提高代码的可重用性。适用性更广: 不依赖于 Java 8，可以在更老的 Java 版本中使用。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">querySingleAccountData</span><span class="token punctuation">(</span><span class="token class-name">String</span> username<span class="token punctuation">,</span> <span class="token class-name">Consumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">AccountData</span><span class="token punctuation">></span></span> callback<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        databaseWriteExecutor<span class="token punctuation">.</span><span class="token function">execute</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>            <span class="token class-name">AccountData</span> accountData <span class="token operator">=</span> accountDataDao<span class="token punctuation">.</span><span class="token function">getSingleAccountData</span><span class="token punctuation">(</span>username<span class="token punctuation">)</span><span class="token punctuation">;</span>            callback<span class="token punctuation">.</span><span class="token function">accept</span><span class="token punctuation">(</span>accountData<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token comment">// 使用</span>dbEngine<span class="token punctuation">.</span><span class="token function">querySingleAccountData</span><span class="token punctuation">(</span>username<span class="token punctuation">,</span> accountData <span class="token operator">-></span> <span class="token punctuation">&#123;</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>accountData <span class="token operator">!=</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                <span class="token comment">// 这里处理查询到的 AccountData</span>                <span class="token comment">// 注意：这可能在后台线程上执行，如果需要更新UI，请确保在主线程上运行</span>                <span class="token function">runOnUiThread</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span>                    <span class="token comment">// 更新UI，例如显示账户数据</span>                <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 安卓 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
